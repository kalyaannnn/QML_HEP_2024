{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QML-HEP GSoC 2021 Task 2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMN1ARgOWv0ApcI7EFfJ6fW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ipsit1234/QML-HEP-Evaluation-Test-GSOC-2021/blob/main/QML_HEP_GSoC_2021_Task_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0T-iBh5NoXDH"
      },
      "source": [
        "# Task II: Quantum Generative Adversarial Network (QGAN) Part\n",
        "You will explore how best to apply a quantum generative adversarial network\n",
        "(QGAN) to solve a High Energy Data analysis issue, more specifically, separating\n",
        "the signal events from the background events. You should use the Google Cirq and\n",
        "Tensorflow Quantum (TFQ) libraries for this task.  \n",
        "A set of input samples (simulated with Delphes) is provided in NumPy NPZ format\n",
        "[Download Input](https://drive.google.com/file/d/1r_MZB_crfpij6r3SxPDeU_3JD6t6AxAj/view). In the input file, there are only 100 samples for training and 100\n",
        "samples for testing so it wonâ€™t take much computing resources to accomplish this\n",
        "task. The signal events are labeled with 1 while the background events are labeled\n",
        "with 0.  \n",
        "Be sure to show that you understand how to fine tune your machine learning model\n",
        "to improve the performance. The performance can be evaluated with classification\n",
        "accuracy or Area Under ROC Curve (AUC)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGSPAoeQpmdH"
      },
      "source": [
        "## Downloading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLlyKHbGppEU",
        "outputId": "b9e9583f-429e-42c4-91a5-3a717057f5ac"
      },
      "source": [
        "!gdown --id 1r_MZB_crfpij6r3SxPDeU_3JD6t6AxAj -O events.npz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1r_MZB_crfpij6r3SxPDeU_3JD6t6AxAj\n",
            "To: /content/events.npz\n",
            "\r  0% 0.00/9.14k [00:00<?, ?B/s]\r100% 9.14k/9.14k [00:00<00:00, 8.67MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKzTb64mp_1t"
      },
      "source": [
        "## Setting up the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ss7ffcuyTiB-"
      },
      "source": [
        "!pip install -q tensorflow==2.3.1\n",
        "!pip install -q tensorflow-quantum\n",
        "import tensorflow as tf\n",
        "import tensorflow_quantum as tfq\n",
        "\n",
        "import cirq\n",
        "import sympy\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from cirq.contrib.svg import SVGCircuit"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eW70FBbgqGyV"
      },
      "source": [
        "## Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcPMDux2T6oe"
      },
      "source": [
        "data = np.load('./events.npz', allow_pickle=True)\n",
        "training_input = data['training_input']\n",
        "test_input = data['test_input']"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xozUTJh7T79S",
        "outputId": "ea574bc9-8f90-413f-b30d-4132cd151715"
      },
      "source": [
        "training_input"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array({'0': array([[-0.43079088,  0.86834819, -0.92614721, -0.92662029, -0.56900862],\n",
              "       [ 0.33924198,  0.56155499,  0.93097459, -0.91631726, -0.54463516],\n",
              "       [-0.42888879,  0.87064961, -0.92782179, -0.77533991, -0.58329176],\n",
              "       [-0.43262871,  0.86128919, -0.92240878, -0.88048862, -0.49963115],\n",
              "       [-0.99925345, -0.99949586,  0.07753685, -0.84218034, -0.5149399 ],\n",
              "       [-0.99631106, -0.99775978,  0.0756427 , -0.54117216, -0.66299335],\n",
              "       [-0.42645921,  0.87141204, -0.92908723, -0.52650143, -0.62187526],\n",
              "       [ 0.34317906,  0.57125045,  0.92638556, -0.85113425, -0.40170562],\n",
              "       [-0.99904849, -0.99933931,  0.07737929, -0.81161066, -0.53550246],\n",
              "       [ 0.3371327 ,  0.55874622,  0.92996976, -0.9117092 , -0.50996097],\n",
              "       [ 0.89649306, -0.95523176, -0.66298651, -0.71276678, -0.62698893],\n",
              "       [ 0.34293232,  0.56408047,  0.93448436, -0.88789589, -0.56154273],\n",
              "       [-0.43055876,  0.86615566, -0.92532229, -0.82531102, -0.61433506],\n",
              "       [ 0.33970589,  0.56676702,  0.92567667, -0.91562035, -0.5946945 ],\n",
              "       [-0.99924224, -0.99951208,  0.07752116, -0.8360764 , -0.56981171],\n",
              "       [-0.43099755,  0.86651251, -0.925269  , -0.86698757, -0.5334677 ],\n",
              "       [-0.99937446, -0.99960218,  0.07759084, -0.84990046, -0.57999577],\n",
              "       [-0.99889821, -0.99925173,  0.07726642, -0.78825187, -0.58779546],\n",
              "       [ 0.34950661,  0.58567909,  0.91615208, -0.55392065, -0.71591931],\n",
              "       [-0.9996095 , -0.99972522,  0.07776863, -0.87858433, -0.51991104],\n",
              "       [-0.99941236, -0.99961243,  0.07764686, -0.85816986, -0.53408948],\n",
              "       [-0.99900111, -0.99932936,  0.07736239, -0.80987373, -0.54108498],\n",
              "       [-0.99903613, -0.99944072,  0.0774002 , -0.82528366, -0.58735909],\n",
              "       [-0.99865334, -0.99912034,  0.07716685, -0.77627523, -0.53754642],\n",
              "       [-0.42986358,  0.86654897, -0.925718  , -0.73862815, -0.58674809],\n",
              "       [-0.42826609,  0.8738673 , -0.929425  , -0.83386636, -0.57230481],\n",
              "       [-0.4292864 ,  0.86711   , -0.92616079, -0.67992738, -0.58893727],\n",
              "       [-0.99891246, -0.99935218,  0.07730719, -0.81056136, -0.58817068],\n",
              "       [-0.99943724, -0.99960331,  0.07761935, -0.84897875, -0.57251576],\n",
              "       [-0.43294759,  0.86021519, -0.92189498, -0.87279564, -0.59891923],\n",
              "       [-0.99916459, -0.99946953,  0.07745698, -0.82671955, -0.58793255],\n",
              "       [-0.99341325, -0.99601417,  0.07387947, -0.24695737, -0.73035246],\n",
              "       [-0.99991548, -0.99993058,  0.07794216, -0.90504651, -0.56418312],\n",
              "       [-0.99922291, -0.99953394,  0.07750004, -0.83864938, -0.5907458 ],\n",
              "       [-0.43169009,  0.8646533 , -0.92421266, -0.89228919, -0.52078182],\n",
              "       [-0.99880664, -0.99928854,  0.07727578, -0.80952183, -0.5345482 ],\n",
              "       [-0.42616344,  0.88033088, -0.93295403, -0.82686517, -0.55077716],\n",
              "       [-0.43139955,  0.86308588, -0.92365803, -0.79283965, -0.56396112],\n",
              "       [-0.42911949,  0.86960978, -0.92732509, -0.76203901, -0.59011644],\n",
              "       [-0.43195732,  0.86474019, -0.92418549, -0.92918364, -0.59731155],\n",
              "       [ 0.88467977, -0.95414347, -0.66293153, -0.82036316, -0.59800758],\n",
              "       [ 0.88789478, -0.95460466, -0.66579754, -0.74234352, -0.64920965],\n",
              "       [-0.99902527, -0.99935633,  0.07734482, -0.80286049, -0.61041081],\n",
              "       [ 0.91931151, -0.94505739, -0.67409081, -0.63394305, -0.62119434],\n",
              "       [-0.43082934,  0.86509194, -0.92477223, -0.78937098, -0.60215747],\n",
              "       [ 0.34116539,  0.5652242 ,  0.93031814, -0.91565387, -0.59557556],\n",
              "       [-0.42622734,  0.88083524, -0.93317473, -0.86019392, -0.57646517],\n",
              "       [-0.99827896, -0.99902924,  0.07692876, -0.75539493, -0.62698962],\n",
              "       [-0.99820057, -0.99867978,  0.07680484, -0.70805718, -0.47476004],\n",
              "       [-0.43178015,  0.86266855, -0.92335757, -0.83674549, -0.5820067 ]]), '1': array([[-0.42298067,  0.88630865, -0.93661218, -0.64944313, -0.39193538],\n",
              "       [ 0.90999432, -0.94429141, -0.6746157 , -0.80518637, -0.53296538],\n",
              "       [-0.99909734, -0.99933762,  0.07749262, -0.83351171, -0.393053  ],\n",
              "       [ 0.35152705,  0.5794319 ,  0.91806358, -0.00923369, -0.75412351],\n",
              "       [ 0.34399902,  0.57339474,  0.92616223, -0.91269157, -0.51149302],\n",
              "       [-0.42905645,  0.86662382, -0.92592506, -0.65770698, -0.38264457],\n",
              "       [ 0.88005236, -0.9599969 , -0.66609191, -0.8764421 , -0.58517572],\n",
              "       [-0.99793345, -0.99872791,  0.07670388, -0.71240287, -0.56441582],\n",
              "       [ 0.91213483, -0.91733005, -0.65528741, -0.67448585, -0.51199249],\n",
              "       [ 0.88551226, -0.94868142, -0.66252618, -0.90843762, -0.59915173],\n",
              "       [ 0.89150504, -0.94960047, -0.67068894,  0.15565222, -0.84421124],\n",
              "       [ 0.88909834, -0.94318762, -0.67148287, -0.84064276, -0.60912642],\n",
              "       [ 0.90255888, -0.92328057, -0.67519978, -0.57106936, -0.66341163],\n",
              "       [ 0.8976049 , -0.94774689, -0.66648593, -0.65464116, -0.52345396],\n",
              "       [-0.43030625,  0.86671232, -0.92539398, -0.82285501, -0.16938372],\n",
              "       [-0.42585346,  0.86764478, -0.9275629 , -0.32038269, -0.50555058],\n",
              "       [-0.43160184,  0.86429225, -0.92408762, -0.85714772, -0.54078138],\n",
              "       [ 0.34190452,  0.57019409,  0.9243911 , -0.8128156 , -0.58818964],\n",
              "       [ 0.89132992, -0.9483933 , -0.65619896, -0.88958426, -0.54497086],\n",
              "       [ 0.890744  , -0.95340342, -0.66924037, -0.79894801, -0.56738322],\n",
              "       [ 0.91361551, -0.94972251, -0.67585399, -0.50145697, -0.1911723 ],\n",
              "       [ 0.88935183, -0.94313315, -0.66904492, -0.79169152, -0.54132759],\n",
              "       [ 0.34476283,  0.57803763,  0.92061164, -0.80268017, -0.32524356],\n",
              "       [ 0.88926914, -0.95421273, -0.66842321, -0.68806207, -0.60578623],\n",
              "       [ 0.89505106, -0.9451298 , -0.67363669, -0.69723999, -0.66076491],\n",
              "       [ 0.362334  ,  0.59572753,  0.93213198, -0.72362975, -0.59988639],\n",
              "       [-0.4288074 ,  0.86967951, -0.92739944, -0.74532187, -0.46450815],\n",
              "       [ 0.88718043, -0.95091359, -0.65983102, -0.8940726 , -0.53211548],\n",
              "       [-0.99846294, -0.99900932,  0.07703519, -0.7582664 , -0.54351981],\n",
              "       [ 0.34178597,  0.56585585,  0.92943783, -0.85569345, -0.56113033],\n",
              "       [ 0.88296536, -0.95481506, -0.66270552, -0.78421127, -0.61179041],\n",
              "       [ 0.88426395, -0.95297205, -0.6626738 , -0.65294941, -0.46763447],\n",
              "       [-0.99590417, -0.99728851,  0.07527463, -0.45478035, -0.64937587],\n",
              "       [ 0.34028029,  0.56005082,  0.93145617, -0.76996587, -0.55614608],\n",
              "       [ 0.88938047, -0.95010936, -0.66373879, -0.80909527, -0.45277393],\n",
              "       [-0.99688379, -0.99798169,  0.07591516, -0.57297135, -0.64869519],\n",
              "       [ 0.90560361, -0.9344575 , -0.65804998, -0.7947352 , -0.58626245],\n",
              "       [ 0.3407186 ,  0.5649425 ,  0.92917793, -0.88543543, -0.45824451],\n",
              "       [ 0.34652975,  0.56915775,  0.93002691, -0.5463081 , -0.61838199],\n",
              "       [ 0.34864817,  0.58382871,  0.91784001, -0.62395809, -0.67006945],\n",
              "       [-0.4268417 ,  0.87348543, -0.92980882, -0.61323903, -0.53234345],\n",
              "       [ 0.93378275, -0.85174182, -0.70941954,  0.05491566, -0.27530413],\n",
              "       [ 0.34210266,  0.56613037,  0.92717996, -0.69534422, -0.62937982],\n",
              "       [ 0.3507178 ,  0.57835822,  0.93047705, -0.73815929, -0.53465596],\n",
              "       [ 0.35258416,  0.58849015,  0.92018786, -0.62128929, -0.58711745],\n",
              "       [ 0.92789668, -0.90635417, -0.64723127, -0.61636877, -0.534791  ],\n",
              "       [-0.43018391,  0.86569257, -0.92522163, -0.72360566, -0.50565552],\n",
              "       [ 0.3490436 ,  0.5855566 ,  0.91734464, -0.60993714, -0.58527924],\n",
              "       [ 0.34434515,  0.56629373,  0.93026591, -0.6060978 , -0.64930218],\n",
              "       [ 0.88125135, -0.95437964, -0.66664384, -0.78187561, -0.64345757]])},\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1yRQZJyT-bj",
        "outputId": "cb0b24cc-b1d2-443c-8766-2dc83c0415a5"
      },
      "source": [
        "def prepare_data(training_input, test_input):\n",
        "  x_train_0 = training_input.item()['0']\n",
        "  x_train_1 = training_input.item()['1']\n",
        "  x_test_0 = test_input.item()['0']\n",
        "  x_test_1 = test_input.item()['1']\n",
        "  x_train = np.zeros((len(x_train_0) + len(x_train_1), x_train_0.shape[1]), dtype=np.float32)\n",
        "  x_test = np.zeros((len(x_test_0) + len(x_test_1), x_test_0.shape[1]), dtype=np.float32)\n",
        "  y_train = np.zeros((len(x_train_0) + len(x_train_1),), dtype=np.int32)\n",
        "  y_test = np.zeros((len(x_test_0) + len(x_test_1),), dtype=np.int32)\n",
        "  x_train[:len(x_train_0), :] = x_train_0\n",
        "  x_train[len(x_train_0):, :] = x_train_1\n",
        "  y_train[:len(x_train_0)] = 0\n",
        "  y_train[len(x_train_0):] = 1\n",
        "\n",
        "  x_test[:len(x_test_0), :] = x_test_0\n",
        "  x_test[len(x_test_0):, :] = x_test_1\n",
        "  y_test[:len(x_test_0)] = 0\n",
        "  y_test[len(x_test_0):] = 1\n",
        "\n",
        "  idx1 = np.random.permutation(len(x_train))\n",
        "  idx2 = np.random.permutation(len(x_test))\n",
        "  x_train, y_train = x_train[idx1], y_train[idx1]\n",
        "  x_test, y_test = x_test[idx2], y_test[idx2]\n",
        "\n",
        "  print('Shape of the training set:', x_train.shape)\n",
        "  print('Shape of the test set:', x_test.shape)\n",
        "\n",
        "  return x_train, y_train, x_test, y_test\n",
        "\n",
        "x_train, y_train, x_test, y_test = prepare_data(training_input, test_input)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of the training set: (100, 5)\n",
            "Shape of the test set: (100, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1L9EWgF9HAvh"
      },
      "source": [
        "## Approach\n",
        "We will make use of a Quantum GAN in the following:\n",
        "1. Train a GAN to produce samples that look like they came from quantum circuits.\n",
        "2. Add a classification path to the discriminator and minimize both the minimax loss and  classification loss.\n",
        "3. We will use a random quantum circuit to generate random inputs for the generator. The intution behind this is that the data that was provided are the results (measurements) taken from some quantum experiment. So if we succeed in training a GAN which generates outputs similar to the experimental data, this will help in identifying new or other possible outcomes of the same quantum experiment which have been missed in the dataset provided.\n",
        "4. Simultaneously training the discriminator to classify signal events and background events will help in identifying the signal events generated from the fully trained generator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5B3o_MGRmYk7"
      },
      "source": [
        "## Data Generation\n",
        "As provided in the dataset, each datapoint is 5-dimensional. Hence we will use 5 qubits and pass them through a random quantum circuit and then use these measurements as inputs to the GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOWxrXWHUBEe"
      },
      "source": [
        "def generate_circuit(qubits):\n",
        "  \"\"\"Generate a random circuit on qubits.\"\"\"\n",
        "  random_circuit = cirq.generate_boixo_2018_supremacy_circuits_v2(qubits, cz_depth=2, seed=123242)\n",
        "  return random_circuit\n",
        "\n",
        "def generate_data(circuit, n_samples):\n",
        "  \"\"\"Draw `n_samples` samples from circuit into a tf.Tensor.\"\"\"\n",
        "  return tf.squeeze(tfq.layers.Sample()(circuit, repetitions=n_samples).to_tensor())"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "zIo-t2qLUE2J",
        "outputId": "48879b7d-f8d8-49e8-e1e7-fc4b694f5e3d"
      },
      "source": [
        "# sample data and circuit structure\n",
        "qubits = cirq.GridQubit.rect(1, 5)\n",
        "random_circuit_m = generate_circuit(qubits) + cirq.measure_each(*qubits)\n",
        "SVGCircuit(random_circuit_m)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<cirq.contrib.svg.svg.SVGCircuit at 0x7f07aada2510>"
            ],
            "image/svg+xml": "<svg xmlns=\"http://www.w3.org/2000/svg\" width=\"420.8705078125\" height=\"250.0\"><line x1=\"34.7588671875\" x2=\"390.8705078125\" y1=\"25.0\" y2=\"25.0\" stroke=\"#1967d2\" stroke-width=\"1\" /><line x1=\"34.7588671875\" x2=\"390.8705078125\" y1=\"75.0\" y2=\"75.0\" stroke=\"#1967d2\" stroke-width=\"1\" /><line x1=\"34.7588671875\" x2=\"390.8705078125\" y1=\"125.0\" y2=\"125.0\" stroke=\"#1967d2\" stroke-width=\"1\" /><line x1=\"34.7588671875\" x2=\"390.8705078125\" y1=\"175.0\" y2=\"175.0\" stroke=\"#1967d2\" stroke-width=\"1\" /><line x1=\"34.7588671875\" x2=\"390.8705078125\" y1=\"225.0\" y2=\"225.0\" stroke=\"#1967d2\" stroke-width=\"1\" /><line x1=\"159.517734375\" x2=\"159.517734375\" y1=\"25.0\" y2=\"75.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"225.19412109375\" x2=\"225.19412109375\" y1=\"75.0\" y2=\"125.0\" stroke=\"black\" stroke-width=\"3\" /><rect x=\"10.0\" y=\"5.0\" width=\"49.517734375\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"0\" /><text x=\"34.7588671875\" y=\"25.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">(0, 0): </text><rect x=\"10.0\" y=\"55.0\" width=\"49.517734375\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"0\" /><text x=\"34.7588671875\" y=\"75.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">(0, 1): </text><rect x=\"10.0\" y=\"105.0\" width=\"49.517734375\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"0\" /><text x=\"34.7588671875\" y=\"125.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">(0, 2): </text><rect x=\"10.0\" y=\"155.0\" width=\"49.517734375\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"0\" /><text x=\"34.7588671875\" y=\"175.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">(0, 3): </text><rect x=\"10.0\" y=\"205.0\" width=\"49.517734375\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"0\" /><text x=\"34.7588671875\" y=\"225.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">(0, 4): </text><rect x=\"79.517734375\" y=\"5.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"99.517734375\" y=\"25.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">H</text><rect x=\"79.517734375\" y=\"55.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"99.517734375\" y=\"75.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">H</text><rect x=\"79.517734375\" y=\"105.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"99.517734375\" y=\"125.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">H</text><rect x=\"79.517734375\" y=\"155.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"99.517734375\" y=\"175.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">H</text><rect x=\"79.517734375\" y=\"205.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"99.517734375\" y=\"225.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">H</text><circle cx=\"159.517734375\" cy=\"25.0\" r=\"10.0\" /><circle cx=\"159.517734375\" cy=\"75.0\" r=\"10.0\" /><rect x=\"139.517734375\" y=\"105.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"159.517734375\" y=\"125.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">T</text><rect x=\"139.517734375\" y=\"155.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"159.517734375\" y=\"175.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">T</text><rect x=\"139.517734375\" y=\"205.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"159.517734375\" y=\"225.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">T</text><circle cx=\"225.19412109375\" cy=\"75.0\" r=\"10.0\" /><circle cx=\"225.19412109375\" cy=\"125.0\" r=\"10.0\" /><rect x=\"199.517734375\" y=\"5.0\" width=\"51.352773437500005\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"225.19412109375\" y=\"25.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Y^0.5</text><rect x=\"270.8705078125\" y=\"5.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"290.8705078125\" y=\"25.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">H</text><rect x=\"270.8705078125\" y=\"55.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"290.8705078125\" y=\"75.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">H</text><rect x=\"270.8705078125\" y=\"105.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"290.8705078125\" y=\"125.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">H</text><rect x=\"270.8705078125\" y=\"155.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"290.8705078125\" y=\"175.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">H</text><rect x=\"270.8705078125\" y=\"205.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"290.8705078125\" y=\"225.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">H</text><rect x=\"330.8705078125\" y=\"5.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"350.8705078125\" y=\"25.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">M</text><rect x=\"330.8705078125\" y=\"55.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"350.8705078125\" y=\"75.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">M</text><rect x=\"330.8705078125\" y=\"105.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"350.8705078125\" y=\"125.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">M</text><rect x=\"330.8705078125\" y=\"155.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"350.8705078125\" y=\"175.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">M</text><rect x=\"330.8705078125\" y=\"205.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"350.8705078125\" y=\"225.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">M</text></svg>"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuzoXJwSUGzR",
        "outputId": "81d824ad-bee3-4b2b-a018-df17f7957265"
      },
      "source": [
        "generate_data(random_circuit_m, 10)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10, 5), dtype=int8, numpy=\n",
              "array([[0, 0, 0, 0, 0],\n",
              "       [0, 1, 0, 0, 0],\n",
              "       [0, 1, 1, 0, 0],\n",
              "       [0, 1, 1, 0, 0],\n",
              "       [0, 1, 1, 0, 0],\n",
              "       [1, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0],\n",
              "       [1, 0, 1, 0, 0],\n",
              "       [1, 0, 1, 0, 0]], dtype=int8)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQI6L17LpJNU"
      },
      "source": [
        "We will generate 200 random training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7v4xx3wUKJV",
        "outputId": "663e26be-e696-493c-ad06-d1e37df28157"
      },
      "source": [
        "N_SAMPLES = 200\n",
        "N_QUBITS = 5\n",
        "QUBITS = cirq.GridQubit.rect(1, N_QUBITS)\n",
        "REFERENCE_CIRCUIT = generate_circuit(QUBITS)\n",
        "random_data = generate_data(REFERENCE_CIRCUIT, N_SAMPLES)\n",
        "random_data"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(200, 5), dtype=int8, numpy=\n",
              "array([[0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 1],\n",
              "       [0, 0, 0, 0, 1],\n",
              "       [0, 0, 0, 0, 1],\n",
              "       [0, 0, 0, 0, 1],\n",
              "       [0, 0, 0, 1, 0],\n",
              "       [0, 0, 0, 1, 0],\n",
              "       [0, 0, 0, 1, 1],\n",
              "       [0, 0, 1, 0, 0],\n",
              "       [0, 0, 1, 0, 0],\n",
              "       [0, 0, 1, 0, 0],\n",
              "       [0, 0, 1, 0, 0],\n",
              "       [0, 0, 1, 0, 0],\n",
              "       [0, 0, 1, 0, 0],\n",
              "       [0, 0, 1, 0, 0],\n",
              "       [0, 0, 1, 0, 0],\n",
              "       [0, 0, 1, 0, 0],\n",
              "       [0, 0, 1, 0, 0],\n",
              "       [0, 0, 1, 0, 0],\n",
              "       [0, 0, 1, 0, 0],\n",
              "       [0, 0, 1, 0, 0],\n",
              "       [0, 0, 1, 0, 0],\n",
              "       [0, 0, 1, 0, 0],\n",
              "       [0, 0, 1, 0, 0],\n",
              "       [0, 0, 1, 0, 0],\n",
              "       [0, 0, 1, 0, 0],\n",
              "       [0, 0, 1, 0, 0],\n",
              "       [0, 0, 1, 0, 0],\n",
              "       [0, 0, 1, 0, 1],\n",
              "       [0, 0, 1, 0, 1],\n",
              "       [0, 0, 1, 1, 0],\n",
              "       [0, 0, 1, 1, 0],\n",
              "       [0, 0, 1, 1, 0],\n",
              "       [0, 0, 1, 1, 1],\n",
              "       [0, 1, 0, 0, 0],\n",
              "       [0, 1, 0, 0, 0],\n",
              "       [0, 1, 0, 0, 0],\n",
              "       [0, 1, 0, 0, 0],\n",
              "       [0, 1, 0, 0, 0],\n",
              "       [0, 1, 0, 0, 0],\n",
              "       [0, 1, 0, 0, 0],\n",
              "       [0, 1, 0, 0, 0],\n",
              "       [0, 1, 0, 0, 0],\n",
              "       [0, 1, 0, 0, 0],\n",
              "       [0, 1, 0, 0, 0],\n",
              "       [0, 1, 0, 0, 0],\n",
              "       [0, 1, 0, 0, 1],\n",
              "       [0, 1, 0, 0, 1],\n",
              "       [0, 1, 0, 0, 1],\n",
              "       [0, 1, 0, 0, 1],\n",
              "       [0, 1, 0, 1, 0],\n",
              "       [0, 1, 0, 1, 0],\n",
              "       [0, 1, 0, 1, 0],\n",
              "       [0, 1, 0, 1, 1],\n",
              "       [0, 1, 0, 1, 1],\n",
              "       [0, 1, 1, 0, 0],\n",
              "       [0, 1, 1, 0, 0],\n",
              "       [0, 1, 1, 0, 0],\n",
              "       [0, 1, 1, 0, 0],\n",
              "       [0, 1, 1, 0, 0],\n",
              "       [0, 1, 1, 0, 0],\n",
              "       [0, 1, 1, 0, 0],\n",
              "       [0, 1, 1, 0, 0],\n",
              "       [0, 1, 1, 0, 0],\n",
              "       [0, 1, 1, 0, 0],\n",
              "       [0, 1, 1, 0, 0],\n",
              "       [0, 1, 1, 0, 0],\n",
              "       [0, 1, 1, 0, 0],\n",
              "       [0, 1, 1, 0, 0],\n",
              "       [0, 1, 1, 0, 0],\n",
              "       [0, 1, 1, 0, 0],\n",
              "       [0, 1, 1, 0, 0],\n",
              "       [0, 1, 1, 0, 0],\n",
              "       [0, 1, 1, 0, 0],\n",
              "       [0, 1, 1, 0, 0],\n",
              "       [0, 1, 1, 0, 1],\n",
              "       [0, 1, 1, 0, 1],\n",
              "       [0, 1, 1, 0, 1],\n",
              "       [0, 1, 1, 1, 0],\n",
              "       [0, 1, 1, 1, 0],\n",
              "       [0, 1, 1, 1, 0],\n",
              "       [0, 1, 1, 1, 1],\n",
              "       [1, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 1],\n",
              "       [1, 0, 0, 0, 1],\n",
              "       [1, 0, 0, 0, 1],\n",
              "       [1, 0, 0, 0, 1],\n",
              "       [1, 0, 0, 1, 0],\n",
              "       [1, 0, 0, 1, 0],\n",
              "       [1, 0, 0, 1, 0],\n",
              "       [1, 0, 0, 1, 0],\n",
              "       [1, 0, 0, 1, 0],\n",
              "       [1, 0, 0, 1, 0],\n",
              "       [1, 0, 0, 1, 1],\n",
              "       [1, 0, 1, 0, 0],\n",
              "       [1, 0, 1, 0, 0],\n",
              "       [1, 0, 1, 0, 0],\n",
              "       [1, 0, 1, 0, 0],\n",
              "       [1, 0, 1, 0, 0],\n",
              "       [1, 0, 1, 0, 0],\n",
              "       [1, 0, 1, 0, 0],\n",
              "       [1, 0, 1, 0, 0],\n",
              "       [1, 0, 1, 0, 0],\n",
              "       [1, 0, 1, 0, 0],\n",
              "       [1, 0, 1, 0, 0],\n",
              "       [1, 0, 1, 0, 0],\n",
              "       [1, 0, 1, 0, 0],\n",
              "       [1, 0, 1, 0, 0],\n",
              "       [1, 0, 1, 0, 0],\n",
              "       [1, 0, 1, 0, 0],\n",
              "       [1, 0, 1, 0, 0],\n",
              "       [1, 0, 1, 0, 0],\n",
              "       [1, 0, 1, 0, 0],\n",
              "       [1, 0, 1, 0, 0],\n",
              "       [1, 0, 1, 0, 1],\n",
              "       [1, 0, 1, 0, 1],\n",
              "       [1, 0, 1, 0, 1],\n",
              "       [1, 0, 1, 1, 0],\n",
              "       [1, 0, 1, 1, 0],\n",
              "       [1, 0, 1, 1, 0],\n",
              "       [1, 0, 1, 1, 0],\n",
              "       [1, 0, 1, 1, 0],\n",
              "       [1, 0, 1, 1, 0],\n",
              "       [1, 0, 1, 1, 0],\n",
              "       [1, 0, 1, 1, 0],\n",
              "       [1, 0, 1, 1, 0],\n",
              "       [1, 0, 1, 1, 1],\n",
              "       [1, 0, 1, 1, 1],\n",
              "       [1, 1, 0, 0, 0],\n",
              "       [1, 1, 0, 0, 0],\n",
              "       [1, 1, 0, 0, 0],\n",
              "       [1, 1, 0, 0, 0],\n",
              "       [1, 1, 0, 0, 0],\n",
              "       [1, 1, 0, 0, 0],\n",
              "       [1, 1, 0, 0, 0],\n",
              "       [1, 1, 0, 0, 0],\n",
              "       [1, 1, 0, 0, 0],\n",
              "       [1, 1, 0, 0, 0],\n",
              "       [1, 1, 0, 0, 0],\n",
              "       [1, 1, 0, 0, 0],\n",
              "       [1, 1, 0, 0, 0],\n",
              "       [1, 1, 0, 0, 0],\n",
              "       [1, 1, 0, 0, 1],\n",
              "       [1, 1, 0, 0, 1],\n",
              "       [1, 1, 0, 0, 1],\n",
              "       [1, 1, 0, 0, 1],\n",
              "       [1, 1, 0, 1, 0],\n",
              "       [1, 1, 0, 1, 0],\n",
              "       [1, 1, 0, 1, 0],\n",
              "       [1, 1, 0, 1, 0],\n",
              "       [1, 1, 0, 1, 1],\n",
              "       [1, 1, 1, 0, 0],\n",
              "       [1, 1, 1, 0, 0],\n",
              "       [1, 1, 1, 0, 0],\n",
              "       [1, 1, 1, 0, 0],\n",
              "       [1, 1, 1, 0, 0],\n",
              "       [1, 1, 1, 0, 0],\n",
              "       [1, 1, 1, 0, 0],\n",
              "       [1, 1, 1, 0, 0],\n",
              "       [1, 1, 1, 0, 0],\n",
              "       [1, 1, 1, 0, 0],\n",
              "       [1, 1, 1, 0, 0],\n",
              "       [1, 1, 1, 0, 0],\n",
              "       [1, 1, 1, 0, 0],\n",
              "       [1, 1, 1, 0, 0],\n",
              "       [1, 1, 1, 0, 0],\n",
              "       [1, 1, 1, 0, 1],\n",
              "       [1, 1, 1, 0, 1],\n",
              "       [1, 1, 1, 0, 1],\n",
              "       [1, 1, 1, 0, 1],\n",
              "       [1, 1, 1, 1, 0],\n",
              "       [1, 1, 1, 1, 0],\n",
              "       [1, 1, 1, 1, 0]], dtype=int8)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylaXFXklrncJ"
      },
      "source": [
        "## Building a Model\n",
        "This GAN will be used to produce measurements corresponding to signal/background events."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGqrV2BXUQqa"
      },
      "source": [
        "def make_generator():\n",
        "  \"\"\"Construct generator model.\"\"\"\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Dense(256, use_bias=False, input_shape=(N_QUBITS,), activation='elu'))\n",
        "  model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.3))\n",
        "  model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.3))\n",
        "  model.add(tf.keras.layers.Dense(N_QUBITS, activation=tf.keras.activations.tanh))\n",
        "  return model\n",
        "\n",
        "def make_discriminator():\n",
        "  \"\"\"Construct discriminator model along with a classifier.\"\"\"\n",
        "  inp = tf.keras.Input(shape=(N_QUBITS, ), dtype=tf.float32)\n",
        "  out = tf.keras.layers.Dense(256, use_bias=False, activation='elu')(inp)\n",
        "  out = tf.keras.layers.Dense(128, activation='relu')(out)\n",
        "  out = tf.keras.layers.Dropout(0.4)(out)\n",
        "  out = tf.keras.layers.Dense(64, activation='relu')(out)\n",
        "  out = tf.keras.layers.Dropout(0.3)(out)\n",
        "  classification = tf.keras.layers.Dense(2, activation='softmax')(out)\n",
        "  discrimination = tf.keras.layers.Dense(1, activation='sigmoid')(out)\n",
        "  model = tf.keras.Model(inputs=[inp], outputs=[discrimination, classification])\n",
        "  return model"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUzRHLH62VKv"
      },
      "source": [
        "Let us instantiate our models, define the losses and define the `train_step` function which will be executed in each epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2cRiizKUS-V"
      },
      "source": [
        "generator = make_generator()\n",
        "discriminator = make_discriminator()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qy7NUNAzUUXJ"
      },
      "source": [
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "  \"\"\"Computes the discriminator loss.\"\"\"\n",
        "  real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "  fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "  total_loss = real_loss + fake_loss\n",
        "  return total_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "  \"\"\"Compute the generator loss.\"\"\" \n",
        "  return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUL4KaCEUXV0"
      },
      "source": [
        "BATCH_SIZE = 16\n",
        "bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "# auc = tf.keras.metrics.AUC()\n",
        "@tf.function\n",
        "def train_step(images, labels, noise):\n",
        "  \"\"\"Run train step on provided image batch.\"\"\"\n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "    generated_data = generator(noise, training=True)\n",
        "\n",
        "    real_output, real_preds = discriminator(images, training=True)\n",
        "    fake_output, fake_preds = discriminator(generated_data, training=True)\n",
        "\n",
        "    gen_loss = generator_loss(fake_output)\n",
        "    disc_loss = discriminator_loss(real_output, fake_output)\n",
        "    disc_loss = disc_loss + bce(tf.one_hot(tf.squeeze(labels), depth=2), real_preds)\n",
        "  \n",
        "  gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "  gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "  generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "  discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "  # auc.update_state(tf.one_hot(tf.squeeze(labels), depth=2), real_preds)\n",
        "\n",
        "  return gen_loss, disc_loss"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoygRDKfUY8t"
      },
      "source": [
        "def train(data, labels, noise, epochs):\n",
        "  \"\"\"Launch full training for the given number of epochs.\"\"\"\n",
        "  batched_data = tf.data.Dataset.from_tensor_slices(data).batch(BATCH_SIZE)\n",
        "  batched_labels = tf.data.Dataset.from_tensor_slices(labels).batch(BATCH_SIZE)\n",
        "  batched_noise = tf.data.Dataset.from_tensor_slices(noise).batch(BATCH_SIZE)\n",
        "  AUC = tf.keras.metrics.AUC()\n",
        "  g_losses = []\n",
        "  d_losses = []\n",
        "  # aucs = []\n",
        "  for epoch in range(epochs):\n",
        "    g_epoch_losses = []\n",
        "    d_epoch_losses = []\n",
        "    # aucs_epoch = []\n",
        "    for i, (data_batch, labels_batch, noise_batch) in enumerate(zip(batched_data, batched_labels, batched_noise)):\n",
        "      gl, dl = train_step(data_batch, labels_batch, noise_batch)\n",
        "      g_epoch_losses.append(gl)\n",
        "      d_epoch_losses.append(dl)\n",
        "      # aucs_epoch.append(auc_roc)\n",
        "    \n",
        "    g_losses.append(tf.reduce_mean(g_epoch_losses))\n",
        "    d_losses.append(tf.reduce_mean(d_epoch_losses))\n",
        "    print('Epoch: {}, Generator Loss: {}, Discriminator Loss: {}'.format(epoch, tf.reduce_mean(g_epoch_losses), tf.reduce_mean(d_epoch_losses)))\n",
        "    # aucs.append(tf.reduce_mean(aucs_epoch))\n",
        "  return g_losses, d_losses"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pj9YZiiyUagT",
        "outputId": "26a80375-c630-4740-fc40-3f566798feec"
      },
      "source": [
        "gen_losses, disc_losses = train(x_train, y_train, random_data, 2000)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Generator Loss: 0.47288545966148376, Discriminator Loss: 2.1426262855529785\n",
            "Epoch: 1, Generator Loss: 0.47159355878829956, Discriminator Loss: 2.1388280391693115\n",
            "Epoch: 2, Generator Loss: 0.470787912607193, Discriminator Loss: 2.0825936794281006\n",
            "Epoch: 3, Generator Loss: 0.4702128767967224, Discriminator Loss: 2.075483798980713\n",
            "Epoch: 4, Generator Loss: 0.47020336985588074, Discriminator Loss: 2.0505168437957764\n",
            "Epoch: 5, Generator Loss: 0.4700762927532196, Discriminator Loss: 2.022904872894287\n",
            "Epoch: 6, Generator Loss: 0.46946173906326294, Discriminator Loss: 2.0530457496643066\n",
            "Epoch: 7, Generator Loss: 0.4677629768848419, Discriminator Loss: 2.0332977771759033\n",
            "Epoch: 8, Generator Loss: 0.4693600833415985, Discriminator Loss: 2.048710346221924\n",
            "Epoch: 9, Generator Loss: 0.4706823527812958, Discriminator Loss: 2.019632339477539\n",
            "Epoch: 10, Generator Loss: 0.4711288809776306, Discriminator Loss: 2.0153427124023438\n",
            "Epoch: 11, Generator Loss: 0.47589582204818726, Discriminator Loss: 2.0159380435943604\n",
            "Epoch: 12, Generator Loss: 0.4789740741252899, Discriminator Loss: 2.0038421154022217\n",
            "Epoch: 13, Generator Loss: 0.4811742901802063, Discriminator Loss: 1.968962550163269\n",
            "Epoch: 14, Generator Loss: 0.48366862535476685, Discriminator Loss: 1.984475016593933\n",
            "Epoch: 15, Generator Loss: 0.4791991710662842, Discriminator Loss: 2.003448009490967\n",
            "Epoch: 16, Generator Loss: 0.4744584262371063, Discriminator Loss: 1.9951093196868896\n",
            "Epoch: 17, Generator Loss: 0.4726622998714447, Discriminator Loss: 1.9844080209732056\n",
            "Epoch: 18, Generator Loss: 0.4746064245700836, Discriminator Loss: 1.9852495193481445\n",
            "Epoch: 19, Generator Loss: 0.4767344892024994, Discriminator Loss: 1.9912365674972534\n",
            "Epoch: 20, Generator Loss: 0.47933536767959595, Discriminator Loss: 1.9573673009872437\n",
            "Epoch: 21, Generator Loss: 0.4778843820095062, Discriminator Loss: 1.997721791267395\n",
            "Epoch: 22, Generator Loss: 0.473391056060791, Discriminator Loss: 1.95980966091156\n",
            "Epoch: 23, Generator Loss: 0.4726634919643402, Discriminator Loss: 1.9842766523361206\n",
            "Epoch: 24, Generator Loss: 0.4760975241661072, Discriminator Loss: 1.9720042943954468\n",
            "Epoch: 25, Generator Loss: 0.48795565962791443, Discriminator Loss: 1.9472404718399048\n",
            "Epoch: 26, Generator Loss: 0.4994671046733856, Discriminator Loss: 1.9453903436660767\n",
            "Epoch: 27, Generator Loss: 0.5054343342781067, Discriminator Loss: 1.9152615070343018\n",
            "Epoch: 28, Generator Loss: 0.508525550365448, Discriminator Loss: 1.9260485172271729\n",
            "Epoch: 29, Generator Loss: 0.5018197894096375, Discriminator Loss: 1.9515644311904907\n",
            "Epoch: 30, Generator Loss: 0.48237642645835876, Discriminator Loss: 1.9889060258865356\n",
            "Epoch: 31, Generator Loss: 0.47576358914375305, Discriminator Loss: 1.9954891204833984\n",
            "Epoch: 32, Generator Loss: 0.47649702429771423, Discriminator Loss: 1.9981650114059448\n",
            "Epoch: 33, Generator Loss: 0.47945308685302734, Discriminator Loss: 2.0044641494750977\n",
            "Epoch: 34, Generator Loss: 0.48640182614326477, Discriminator Loss: 1.9839919805526733\n",
            "Epoch: 35, Generator Loss: 0.5043705105781555, Discriminator Loss: 1.964692234992981\n",
            "Epoch: 36, Generator Loss: 0.5061931610107422, Discriminator Loss: 1.9607325792312622\n",
            "Epoch: 37, Generator Loss: 0.5090411305427551, Discriminator Loss: 1.9640862941741943\n",
            "Epoch: 38, Generator Loss: 0.49888092279434204, Discriminator Loss: 2.0025503635406494\n",
            "Epoch: 39, Generator Loss: 0.49890926480293274, Discriminator Loss: 1.9784700870513916\n",
            "Epoch: 40, Generator Loss: 0.4987829327583313, Discriminator Loss: 1.9651585817337036\n",
            "Epoch: 41, Generator Loss: 0.5023303627967834, Discriminator Loss: 1.9435651302337646\n",
            "Epoch: 42, Generator Loss: 0.5030931830406189, Discriminator Loss: 1.970147967338562\n",
            "Epoch: 43, Generator Loss: 0.5103368163108826, Discriminator Loss: 1.948770523071289\n",
            "Epoch: 44, Generator Loss: 0.5141062140464783, Discriminator Loss: 1.9382458925247192\n",
            "Epoch: 45, Generator Loss: 0.5192903280258179, Discriminator Loss: 1.923143982887268\n",
            "Epoch: 46, Generator Loss: 0.5226637125015259, Discriminator Loss: 1.9440218210220337\n",
            "Epoch: 47, Generator Loss: 0.5246140956878662, Discriminator Loss: 1.9339735507965088\n",
            "Epoch: 48, Generator Loss: 0.5278750658035278, Discriminator Loss: 1.9330395460128784\n",
            "Epoch: 49, Generator Loss: 0.5215190052986145, Discriminator Loss: 1.9442846775054932\n",
            "Epoch: 50, Generator Loss: 0.5162323117256165, Discriminator Loss: 1.9721218347549438\n",
            "Epoch: 51, Generator Loss: 0.5048933625221252, Discriminator Loss: 1.9678698778152466\n",
            "Epoch: 52, Generator Loss: 0.509625256061554, Discriminator Loss: 1.9625767469406128\n",
            "Epoch: 53, Generator Loss: 0.5198760628700256, Discriminator Loss: 1.9369432926177979\n",
            "Epoch: 54, Generator Loss: 0.5260513424873352, Discriminator Loss: 1.9441606998443604\n",
            "Epoch: 55, Generator Loss: 0.5376901626586914, Discriminator Loss: 1.936803936958313\n",
            "Epoch: 56, Generator Loss: 0.5560902953147888, Discriminator Loss: 1.902709722518921\n",
            "Epoch: 57, Generator Loss: 0.5605687499046326, Discriminator Loss: 1.8947279453277588\n",
            "Epoch: 58, Generator Loss: 0.5672340393066406, Discriminator Loss: 1.898500680923462\n",
            "Epoch: 59, Generator Loss: 0.5689089894294739, Discriminator Loss: 1.871029019355774\n",
            "Epoch: 60, Generator Loss: 0.5697230696678162, Discriminator Loss: 1.8731046915054321\n",
            "Epoch: 61, Generator Loss: 0.5583624839782715, Discriminator Loss: 1.8996940851211548\n",
            "Epoch: 62, Generator Loss: 0.5345408320426941, Discriminator Loss: 1.9253835678100586\n",
            "Epoch: 63, Generator Loss: 0.5130673050880432, Discriminator Loss: 1.9621416330337524\n",
            "Epoch: 64, Generator Loss: 0.5060693621635437, Discriminator Loss: 1.965281367301941\n",
            "Epoch: 65, Generator Loss: 0.523578941822052, Discriminator Loss: 1.9250215291976929\n",
            "Epoch: 66, Generator Loss: 0.5341764688491821, Discriminator Loss: 1.9169458150863647\n",
            "Epoch: 67, Generator Loss: 0.5437857508659363, Discriminator Loss: 1.8830159902572632\n",
            "Epoch: 68, Generator Loss: 0.5410133004188538, Discriminator Loss: 1.9133960008621216\n",
            "Epoch: 69, Generator Loss: 0.535211443901062, Discriminator Loss: 1.8671048879623413\n",
            "Epoch: 70, Generator Loss: 0.5227625966072083, Discriminator Loss: 1.9147918224334717\n",
            "Epoch: 71, Generator Loss: 0.5165602564811707, Discriminator Loss: 1.9021399021148682\n",
            "Epoch: 72, Generator Loss: 0.5219352841377258, Discriminator Loss: 1.8909399509429932\n",
            "Epoch: 73, Generator Loss: 0.5299494862556458, Discriminator Loss: 1.913786768913269\n",
            "Epoch: 74, Generator Loss: 0.5370683073997498, Discriminator Loss: 1.8993290662765503\n",
            "Epoch: 75, Generator Loss: 0.5543458461761475, Discriminator Loss: 1.886603593826294\n",
            "Epoch: 76, Generator Loss: 0.5559374690055847, Discriminator Loss: 1.8833478689193726\n",
            "Epoch: 77, Generator Loss: 0.5516044497489929, Discriminator Loss: 1.88387930393219\n",
            "Epoch: 78, Generator Loss: 0.5634049773216248, Discriminator Loss: 1.8520065546035767\n",
            "Epoch: 79, Generator Loss: 0.5627610087394714, Discriminator Loss: 1.8465543985366821\n",
            "Epoch: 80, Generator Loss: 0.5510416030883789, Discriminator Loss: 1.8662376403808594\n",
            "Epoch: 81, Generator Loss: 0.5503302216529846, Discriminator Loss: 1.915558099746704\n",
            "Epoch: 82, Generator Loss: 0.5445582866668701, Discriminator Loss: 1.9156755208969116\n",
            "Epoch: 83, Generator Loss: 0.5506776571273804, Discriminator Loss: 1.876555323600769\n",
            "Epoch: 84, Generator Loss: 0.5673772692680359, Discriminator Loss: 1.8943994045257568\n",
            "Epoch: 85, Generator Loss: 0.5826940536499023, Discriminator Loss: 1.8510185480117798\n",
            "Epoch: 86, Generator Loss: 0.5903192758560181, Discriminator Loss: 1.828652262687683\n",
            "Epoch: 87, Generator Loss: 0.5916497111320496, Discriminator Loss: 1.8171485662460327\n",
            "Epoch: 88, Generator Loss: 0.5896720886230469, Discriminator Loss: 1.7961512804031372\n",
            "Epoch: 89, Generator Loss: 0.5867503881454468, Discriminator Loss: 1.8081086874008179\n",
            "Epoch: 90, Generator Loss: 0.561775803565979, Discriminator Loss: 1.8444185256958008\n",
            "Epoch: 91, Generator Loss: 0.552556037902832, Discriminator Loss: 1.8795503377914429\n",
            "Epoch: 92, Generator Loss: 0.544338583946228, Discriminator Loss: 1.8654581308364868\n",
            "Epoch: 93, Generator Loss: 0.5510985255241394, Discriminator Loss: 1.845579981803894\n",
            "Epoch: 94, Generator Loss: 0.5495144724845886, Discriminator Loss: 1.839232087135315\n",
            "Epoch: 95, Generator Loss: 0.5556743741035461, Discriminator Loss: 1.8285958766937256\n",
            "Epoch: 96, Generator Loss: 0.5433610081672668, Discriminator Loss: 1.8459646701812744\n",
            "Epoch: 97, Generator Loss: 0.5369371771812439, Discriminator Loss: 1.8679707050323486\n",
            "Epoch: 98, Generator Loss: 0.5460633635520935, Discriminator Loss: 1.8344632387161255\n",
            "Epoch: 99, Generator Loss: 0.5498455762863159, Discriminator Loss: 1.873538851737976\n",
            "Epoch: 100, Generator Loss: 0.5569822788238525, Discriminator Loss: 1.8296430110931396\n",
            "Epoch: 101, Generator Loss: 0.5649753212928772, Discriminator Loss: 1.8473527431488037\n",
            "Epoch: 102, Generator Loss: 0.5755732655525208, Discriminator Loss: 1.8351280689239502\n",
            "Epoch: 103, Generator Loss: 0.5833231210708618, Discriminator Loss: 1.8038426637649536\n",
            "Epoch: 104, Generator Loss: 0.5890332460403442, Discriminator Loss: 1.8082283735275269\n",
            "Epoch: 105, Generator Loss: 0.5832279920578003, Discriminator Loss: 1.8057630062103271\n",
            "Epoch: 106, Generator Loss: 0.5815386176109314, Discriminator Loss: 1.821812391281128\n",
            "Epoch: 107, Generator Loss: 0.5709091424942017, Discriminator Loss: 1.8217051029205322\n",
            "Epoch: 108, Generator Loss: 0.5474334359169006, Discriminator Loss: 1.8874613046646118\n",
            "Epoch: 109, Generator Loss: 0.5683592557907104, Discriminator Loss: 1.8294686079025269\n",
            "Epoch: 110, Generator Loss: 0.5851566195487976, Discriminator Loss: 1.862564206123352\n",
            "Epoch: 111, Generator Loss: 0.5988563299179077, Discriminator Loss: 1.8206329345703125\n",
            "Epoch: 112, Generator Loss: 0.6115578413009644, Discriminator Loss: 1.773983359336853\n",
            "Epoch: 113, Generator Loss: 0.6199137568473816, Discriminator Loss: 1.8008215427398682\n",
            "Epoch: 114, Generator Loss: 0.6248680949211121, Discriminator Loss: 1.8012686967849731\n",
            "Epoch: 115, Generator Loss: 0.6262296438217163, Discriminator Loss: 1.783456563949585\n",
            "Epoch: 116, Generator Loss: 0.6282876133918762, Discriminator Loss: 1.7769858837127686\n",
            "Epoch: 117, Generator Loss: 0.6285579800605774, Discriminator Loss: 1.7650648355484009\n",
            "Epoch: 118, Generator Loss: 0.6294587254524231, Discriminator Loss: 1.7778254747390747\n",
            "Epoch: 119, Generator Loss: 0.6305748820304871, Discriminator Loss: 1.7778427600860596\n",
            "Epoch: 120, Generator Loss: 0.6291638612747192, Discriminator Loss: 1.775632619857788\n",
            "Epoch: 121, Generator Loss: 0.6280948519706726, Discriminator Loss: 1.7537888288497925\n",
            "Epoch: 122, Generator Loss: 0.6300249099731445, Discriminator Loss: 1.7844229936599731\n",
            "Epoch: 123, Generator Loss: 0.6283208727836609, Discriminator Loss: 1.754599928855896\n",
            "Epoch: 124, Generator Loss: 0.6298123002052307, Discriminator Loss: 1.7848894596099854\n",
            "Epoch: 125, Generator Loss: 0.6309837698936462, Discriminator Loss: 1.7474391460418701\n",
            "Epoch: 126, Generator Loss: 0.6320422887802124, Discriminator Loss: 1.7583450078964233\n",
            "Epoch: 127, Generator Loss: 0.6400904059410095, Discriminator Loss: 1.7694214582443237\n",
            "Epoch: 128, Generator Loss: 0.6324629187583923, Discriminator Loss: 1.7774016857147217\n",
            "Epoch: 129, Generator Loss: 0.6124916076660156, Discriminator Loss: 1.8186702728271484\n",
            "Epoch: 130, Generator Loss: 0.5734853148460388, Discriminator Loss: 1.837594747543335\n",
            "Epoch: 131, Generator Loss: 0.5636682510375977, Discriminator Loss: 1.887058973312378\n",
            "Epoch: 132, Generator Loss: 0.5782691240310669, Discriminator Loss: 1.8639087677001953\n",
            "Epoch: 133, Generator Loss: 0.5855530500411987, Discriminator Loss: 1.8551642894744873\n",
            "Epoch: 134, Generator Loss: 0.5991559624671936, Discriminator Loss: 1.8197938203811646\n",
            "Epoch: 135, Generator Loss: 0.6184874773025513, Discriminator Loss: 1.8181065320968628\n",
            "Epoch: 136, Generator Loss: 0.6258505582809448, Discriminator Loss: 1.8089030981063843\n",
            "Epoch: 137, Generator Loss: 0.6345445513725281, Discriminator Loss: 1.8162752389907837\n",
            "Epoch: 138, Generator Loss: 0.6385933756828308, Discriminator Loss: 1.8260458707809448\n",
            "Epoch: 139, Generator Loss: 0.6473274230957031, Discriminator Loss: 1.7981773614883423\n",
            "Epoch: 140, Generator Loss: 0.6472809910774231, Discriminator Loss: 1.7697031497955322\n",
            "Epoch: 141, Generator Loss: 0.650701642036438, Discriminator Loss: 1.743865728378296\n",
            "Epoch: 142, Generator Loss: 0.6521008610725403, Discriminator Loss: 1.7176873683929443\n",
            "Epoch: 143, Generator Loss: 0.6444106698036194, Discriminator Loss: 1.7693579196929932\n",
            "Epoch: 144, Generator Loss: 0.641905665397644, Discriminator Loss: 1.7705204486846924\n",
            "Epoch: 145, Generator Loss: 0.6374165415763855, Discriminator Loss: 1.8058663606643677\n",
            "Epoch: 146, Generator Loss: 0.6235266923904419, Discriminator Loss: 1.7381870746612549\n",
            "Epoch: 147, Generator Loss: 0.5892828702926636, Discriminator Loss: 1.852325439453125\n",
            "Epoch: 148, Generator Loss: 0.5753353834152222, Discriminator Loss: 1.8449815511703491\n",
            "Epoch: 149, Generator Loss: 0.6026539206504822, Discriminator Loss: 1.7581708431243896\n",
            "Epoch: 150, Generator Loss: 0.6236670613288879, Discriminator Loss: 1.760986089706421\n",
            "Epoch: 151, Generator Loss: 0.6250039339065552, Discriminator Loss: 1.768956184387207\n",
            "Epoch: 152, Generator Loss: 0.6256861090660095, Discriminator Loss: 1.7794551849365234\n",
            "Epoch: 153, Generator Loss: 0.6346012949943542, Discriminator Loss: 1.760243535041809\n",
            "Epoch: 154, Generator Loss: 0.628298819065094, Discriminator Loss: 1.749287724494934\n",
            "Epoch: 155, Generator Loss: 0.6368821859359741, Discriminator Loss: 1.7152551412582397\n",
            "Epoch: 156, Generator Loss: 0.6314118504524231, Discriminator Loss: 1.7351112365722656\n",
            "Epoch: 157, Generator Loss: 0.6265581250190735, Discriminator Loss: 1.736602544784546\n",
            "Epoch: 158, Generator Loss: 0.6100559234619141, Discriminator Loss: 1.7446779012680054\n",
            "Epoch: 159, Generator Loss: 0.5922154188156128, Discriminator Loss: 1.7559831142425537\n",
            "Epoch: 160, Generator Loss: 0.5712250471115112, Discriminator Loss: 1.8211967945098877\n",
            "Epoch: 161, Generator Loss: 0.5644485354423523, Discriminator Loss: 1.8665589094161987\n",
            "Epoch: 162, Generator Loss: 0.6024867296218872, Discriminator Loss: 1.8104267120361328\n",
            "Epoch: 163, Generator Loss: 0.6274945139884949, Discriminator Loss: 1.7998268604278564\n",
            "Epoch: 164, Generator Loss: 0.643936038017273, Discriminator Loss: 1.7096644639968872\n",
            "Epoch: 165, Generator Loss: 0.6404080986976624, Discriminator Loss: 1.716945767402649\n",
            "Epoch: 166, Generator Loss: 0.6532421708106995, Discriminator Loss: 1.7037891149520874\n",
            "Epoch: 167, Generator Loss: 0.6447615027427673, Discriminator Loss: 1.7099530696868896\n",
            "Epoch: 168, Generator Loss: 0.6475247740745544, Discriminator Loss: 1.6623265743255615\n",
            "Epoch: 169, Generator Loss: 0.6348186731338501, Discriminator Loss: 1.6947166919708252\n",
            "Epoch: 170, Generator Loss: 0.6414856314659119, Discriminator Loss: 1.6926909685134888\n",
            "Epoch: 171, Generator Loss: 0.6314753890037537, Discriminator Loss: 1.660185694694519\n",
            "Epoch: 172, Generator Loss: 0.6261246800422668, Discriminator Loss: 1.7072498798370361\n",
            "Epoch: 173, Generator Loss: 0.6210373640060425, Discriminator Loss: 1.675012469291687\n",
            "Epoch: 174, Generator Loss: 0.5940043330192566, Discriminator Loss: 1.7331515550613403\n",
            "Epoch: 175, Generator Loss: 0.5574861168861389, Discriminator Loss: 1.7930545806884766\n",
            "Epoch: 176, Generator Loss: 0.542355477809906, Discriminator Loss: 1.8148338794708252\n",
            "Epoch: 177, Generator Loss: 0.5822253823280334, Discriminator Loss: 1.8181102275848389\n",
            "Epoch: 178, Generator Loss: 0.5935879349708557, Discriminator Loss: 1.8004159927368164\n",
            "Epoch: 179, Generator Loss: 0.6095434427261353, Discriminator Loss: 1.7807486057281494\n",
            "Epoch: 180, Generator Loss: 0.6232584714889526, Discriminator Loss: 1.7791073322296143\n",
            "Epoch: 181, Generator Loss: 0.6337913274765015, Discriminator Loss: 1.7705801725387573\n",
            "Epoch: 182, Generator Loss: 0.6357728242874146, Discriminator Loss: 1.7532390356063843\n",
            "Epoch: 183, Generator Loss: 0.6398280262947083, Discriminator Loss: 1.7407302856445312\n",
            "Epoch: 184, Generator Loss: 0.634765625, Discriminator Loss: 1.7535192966461182\n",
            "Epoch: 185, Generator Loss: 0.63277667760849, Discriminator Loss: 1.7463499307632446\n",
            "Epoch: 186, Generator Loss: 0.6328725218772888, Discriminator Loss: 1.7686328887939453\n",
            "Epoch: 187, Generator Loss: 0.6205809712409973, Discriminator Loss: 1.7650877237319946\n",
            "Epoch: 188, Generator Loss: 0.5941453576087952, Discriminator Loss: 1.8217060565948486\n",
            "Epoch: 189, Generator Loss: 0.6100953817367554, Discriminator Loss: 1.7752925157546997\n",
            "Epoch: 190, Generator Loss: 0.6226822733879089, Discriminator Loss: 1.7728627920150757\n",
            "Epoch: 191, Generator Loss: 0.6314853429794312, Discriminator Loss: 1.7534593343734741\n",
            "Epoch: 192, Generator Loss: 0.637579083442688, Discriminator Loss: 1.7179620265960693\n",
            "Epoch: 193, Generator Loss: 0.6405450105667114, Discriminator Loss: 1.725502848625183\n",
            "Epoch: 194, Generator Loss: 0.6397368311882019, Discriminator Loss: 1.6983562707901\n",
            "Epoch: 195, Generator Loss: 0.6388319730758667, Discriminator Loss: 1.7154982089996338\n",
            "Epoch: 196, Generator Loss: 0.6341049075126648, Discriminator Loss: 1.728189468383789\n",
            "Epoch: 197, Generator Loss: 0.6432934403419495, Discriminator Loss: 1.7179791927337646\n",
            "Epoch: 198, Generator Loss: 0.6484194993972778, Discriminator Loss: 1.6843165159225464\n",
            "Epoch: 199, Generator Loss: 0.6386139988899231, Discriminator Loss: 1.6859838962554932\n",
            "Epoch: 200, Generator Loss: 0.6301169991493225, Discriminator Loss: 1.682997703552246\n",
            "Epoch: 201, Generator Loss: 0.6347073912620544, Discriminator Loss: 1.6391927003860474\n",
            "Epoch: 202, Generator Loss: 0.629494309425354, Discriminator Loss: 1.6566816568374634\n",
            "Epoch: 203, Generator Loss: 0.615505039691925, Discriminator Loss: 1.699209213256836\n",
            "Epoch: 204, Generator Loss: 0.6223717927932739, Discriminator Loss: 1.6770910024642944\n",
            "Epoch: 205, Generator Loss: 0.6162890195846558, Discriminator Loss: 1.6603543758392334\n",
            "Epoch: 206, Generator Loss: 0.6068199872970581, Discriminator Loss: 1.7019388675689697\n",
            "Epoch: 207, Generator Loss: 0.5835185050964355, Discriminator Loss: 1.732129454612732\n",
            "Epoch: 208, Generator Loss: 0.5958709716796875, Discriminator Loss: 1.735878825187683\n",
            "Epoch: 209, Generator Loss: 0.5849490761756897, Discriminator Loss: 1.7640939950942993\n",
            "Epoch: 210, Generator Loss: 0.5637327432632446, Discriminator Loss: 1.8124845027923584\n",
            "Epoch: 211, Generator Loss: 0.577876627445221, Discriminator Loss: 1.8146675825119019\n",
            "Epoch: 212, Generator Loss: 0.5868808031082153, Discriminator Loss: 1.812463402748108\n",
            "Epoch: 213, Generator Loss: 0.605700671672821, Discriminator Loss: 1.7507508993148804\n",
            "Epoch: 214, Generator Loss: 0.6198753714561462, Discriminator Loss: 1.7286328077316284\n",
            "Epoch: 215, Generator Loss: 0.6174247860908508, Discriminator Loss: 1.7417566776275635\n",
            "Epoch: 216, Generator Loss: 0.613113284111023, Discriminator Loss: 1.765594244003296\n",
            "Epoch: 217, Generator Loss: 0.6264870762825012, Discriminator Loss: 1.6827411651611328\n",
            "Epoch: 218, Generator Loss: 0.6183872222900391, Discriminator Loss: 1.7059530019760132\n",
            "Epoch: 219, Generator Loss: 0.6173415184020996, Discriminator Loss: 1.7229164838790894\n",
            "Epoch: 220, Generator Loss: 0.6050001978874207, Discriminator Loss: 1.7366943359375\n",
            "Epoch: 221, Generator Loss: 0.6172733306884766, Discriminator Loss: 1.674242377281189\n",
            "Epoch: 222, Generator Loss: 0.6176896691322327, Discriminator Loss: 1.7133352756500244\n",
            "Epoch: 223, Generator Loss: 0.610809862613678, Discriminator Loss: 1.6950114965438843\n",
            "Epoch: 224, Generator Loss: 0.6104924082756042, Discriminator Loss: 1.75283682346344\n",
            "Epoch: 225, Generator Loss: 0.6131807565689087, Discriminator Loss: 1.7120122909545898\n",
            "Epoch: 226, Generator Loss: 0.6219436526298523, Discriminator Loss: 1.7476533651351929\n",
            "Epoch: 227, Generator Loss: 0.6235187649726868, Discriminator Loss: 1.7707799673080444\n",
            "Epoch: 228, Generator Loss: 0.6321061253547668, Discriminator Loss: 1.6742284297943115\n",
            "Epoch: 229, Generator Loss: 0.6126621961593628, Discriminator Loss: 1.7203556299209595\n",
            "Epoch: 230, Generator Loss: 0.6216983199119568, Discriminator Loss: 1.6999882459640503\n",
            "Epoch: 231, Generator Loss: 0.6149224042892456, Discriminator Loss: 1.741202473640442\n",
            "Epoch: 232, Generator Loss: 0.626215398311615, Discriminator Loss: 1.75324285030365\n",
            "Epoch: 233, Generator Loss: 0.6238604187965393, Discriminator Loss: 1.7004338502883911\n",
            "Epoch: 234, Generator Loss: 0.624783456325531, Discriminator Loss: 1.709208369255066\n",
            "Epoch: 235, Generator Loss: 0.6322792768478394, Discriminator Loss: 1.7244776487350464\n",
            "Epoch: 236, Generator Loss: 0.6291399002075195, Discriminator Loss: 1.703481912612915\n",
            "Epoch: 237, Generator Loss: 0.6249092817306519, Discriminator Loss: 1.7036558389663696\n",
            "Epoch: 238, Generator Loss: 0.6288641691207886, Discriminator Loss: 1.6899945735931396\n",
            "Epoch: 239, Generator Loss: 0.6295877695083618, Discriminator Loss: 1.7107700109481812\n",
            "Epoch: 240, Generator Loss: 0.6331409811973572, Discriminator Loss: 1.693745493888855\n",
            "Epoch: 241, Generator Loss: 0.6244977712631226, Discriminator Loss: 1.7336817979812622\n",
            "Epoch: 242, Generator Loss: 0.6192163825035095, Discriminator Loss: 1.7143076658248901\n",
            "Epoch: 243, Generator Loss: 0.618996262550354, Discriminator Loss: 1.745056390762329\n",
            "Epoch: 244, Generator Loss: 0.633124053478241, Discriminator Loss: 1.6888076066970825\n",
            "Epoch: 245, Generator Loss: 0.6242901682853699, Discriminator Loss: 1.730173110961914\n",
            "Epoch: 246, Generator Loss: 0.6301984190940857, Discriminator Loss: 1.7000253200531006\n",
            "Epoch: 247, Generator Loss: 0.6281982660293579, Discriminator Loss: 1.7466261386871338\n",
            "Epoch: 248, Generator Loss: 0.6365850567817688, Discriminator Loss: 1.7463300228118896\n",
            "Epoch: 249, Generator Loss: 0.6325564980506897, Discriminator Loss: 1.7576972246170044\n",
            "Epoch: 250, Generator Loss: 0.6371110081672668, Discriminator Loss: 1.707656979560852\n",
            "Epoch: 251, Generator Loss: 0.6365411877632141, Discriminator Loss: 1.7142974138259888\n",
            "Epoch: 252, Generator Loss: 0.6340939402580261, Discriminator Loss: 1.695809245109558\n",
            "Epoch: 253, Generator Loss: 0.6294987797737122, Discriminator Loss: 1.746159553527832\n",
            "Epoch: 254, Generator Loss: 0.6255390048027039, Discriminator Loss: 1.676321268081665\n",
            "Epoch: 255, Generator Loss: 0.6201111078262329, Discriminator Loss: 1.729650855064392\n",
            "Epoch: 256, Generator Loss: 0.6224362254142761, Discriminator Loss: 1.6679507493972778\n",
            "Epoch: 257, Generator Loss: 0.6230950951576233, Discriminator Loss: 1.7347079515457153\n",
            "Epoch: 258, Generator Loss: 0.6113643646240234, Discriminator Loss: 1.7098867893218994\n",
            "Epoch: 259, Generator Loss: 0.6205706000328064, Discriminator Loss: 1.7021971940994263\n",
            "Epoch: 260, Generator Loss: 0.6246237754821777, Discriminator Loss: 1.7006195783615112\n",
            "Epoch: 261, Generator Loss: 0.6167707443237305, Discriminator Loss: 1.7234348058700562\n",
            "Epoch: 262, Generator Loss: 0.6311796307563782, Discriminator Loss: 1.7174619436264038\n",
            "Epoch: 263, Generator Loss: 0.6179898381233215, Discriminator Loss: 1.7127645015716553\n",
            "Epoch: 264, Generator Loss: 0.6204423308372498, Discriminator Loss: 1.7028361558914185\n",
            "Epoch: 265, Generator Loss: 0.6133394837379456, Discriminator Loss: 1.6926206350326538\n",
            "Epoch: 266, Generator Loss: 0.6211324334144592, Discriminator Loss: 1.7179358005523682\n",
            "Epoch: 267, Generator Loss: 0.625482976436615, Discriminator Loss: 1.6775175333023071\n",
            "Epoch: 268, Generator Loss: 0.6245230436325073, Discriminator Loss: 1.7046822309494019\n",
            "Epoch: 269, Generator Loss: 0.6272510290145874, Discriminator Loss: 1.722633957862854\n",
            "Epoch: 270, Generator Loss: 0.6352478265762329, Discriminator Loss: 1.7091001272201538\n",
            "Epoch: 271, Generator Loss: 0.63001948595047, Discriminator Loss: 1.687362790107727\n",
            "Epoch: 272, Generator Loss: 0.6199166178703308, Discriminator Loss: 1.7096284627914429\n",
            "Epoch: 273, Generator Loss: 0.6292887926101685, Discriminator Loss: 1.7140878438949585\n",
            "Epoch: 274, Generator Loss: 0.6361770033836365, Discriminator Loss: 1.681007981300354\n",
            "Epoch: 275, Generator Loss: 0.6291784644126892, Discriminator Loss: 1.7483857870101929\n",
            "Epoch: 276, Generator Loss: 0.6315180659294128, Discriminator Loss: 1.7215346097946167\n",
            "Epoch: 277, Generator Loss: 0.6314154863357544, Discriminator Loss: 1.7012547254562378\n",
            "Epoch: 278, Generator Loss: 0.63746577501297, Discriminator Loss: 1.7148911952972412\n",
            "Epoch: 279, Generator Loss: 0.6352987885475159, Discriminator Loss: 1.698099136352539\n",
            "Epoch: 280, Generator Loss: 0.6360382437705994, Discriminator Loss: 1.6983792781829834\n",
            "Epoch: 281, Generator Loss: 0.6355541348457336, Discriminator Loss: 1.6732347011566162\n",
            "Epoch: 282, Generator Loss: 0.6315398812294006, Discriminator Loss: 1.6969462633132935\n",
            "Epoch: 283, Generator Loss: 0.6267959475517273, Discriminator Loss: 1.7253066301345825\n",
            "Epoch: 284, Generator Loss: 0.6349143981933594, Discriminator Loss: 1.6942905187606812\n",
            "Epoch: 285, Generator Loss: 0.6350799202919006, Discriminator Loss: 1.7513391971588135\n",
            "Epoch: 286, Generator Loss: 0.6315780282020569, Discriminator Loss: 1.7159690856933594\n",
            "Epoch: 287, Generator Loss: 0.6353291273117065, Discriminator Loss: 1.6938655376434326\n",
            "Epoch: 288, Generator Loss: 0.6284250617027283, Discriminator Loss: 1.732704997062683\n",
            "Epoch: 289, Generator Loss: 0.627655029296875, Discriminator Loss: 1.728252649307251\n",
            "Epoch: 290, Generator Loss: 0.6375523805618286, Discriminator Loss: 1.6391680240631104\n",
            "Epoch: 291, Generator Loss: 0.6345260739326477, Discriminator Loss: 1.6660269498825073\n",
            "Epoch: 292, Generator Loss: 0.6323086023330688, Discriminator Loss: 1.73190438747406\n",
            "Epoch: 293, Generator Loss: 0.6317147016525269, Discriminator Loss: 1.7209004163742065\n",
            "Epoch: 294, Generator Loss: 0.6281710863113403, Discriminator Loss: 1.6950528621673584\n",
            "Epoch: 295, Generator Loss: 0.6317554116249084, Discriminator Loss: 1.6676512956619263\n",
            "Epoch: 296, Generator Loss: 0.6337504386901855, Discriminator Loss: 1.718414068222046\n",
            "Epoch: 297, Generator Loss: 0.6366174817085266, Discriminator Loss: 1.7014961242675781\n",
            "Epoch: 298, Generator Loss: 0.64112389087677, Discriminator Loss: 1.6978299617767334\n",
            "Epoch: 299, Generator Loss: 0.6359738111495972, Discriminator Loss: 1.6829395294189453\n",
            "Epoch: 300, Generator Loss: 0.6392512917518616, Discriminator Loss: 1.668739914894104\n",
            "Epoch: 301, Generator Loss: 0.6416367292404175, Discriminator Loss: 1.6946498155593872\n",
            "Epoch: 302, Generator Loss: 0.6406598687171936, Discriminator Loss: 1.7617634534835815\n",
            "Epoch: 303, Generator Loss: 0.6414175033569336, Discriminator Loss: 1.720671534538269\n",
            "Epoch: 304, Generator Loss: 0.63744056224823, Discriminator Loss: 1.67775559425354\n",
            "Epoch: 305, Generator Loss: 0.6414657831192017, Discriminator Loss: 1.6948049068450928\n",
            "Epoch: 306, Generator Loss: 0.6470726728439331, Discriminator Loss: 1.6779272556304932\n",
            "Epoch: 307, Generator Loss: 0.6411290764808655, Discriminator Loss: 1.6670846939086914\n",
            "Epoch: 308, Generator Loss: 0.6360321044921875, Discriminator Loss: 1.6787768602371216\n",
            "Epoch: 309, Generator Loss: 0.6412560343742371, Discriminator Loss: 1.701042890548706\n",
            "Epoch: 310, Generator Loss: 0.6406087279319763, Discriminator Loss: 1.6843072175979614\n",
            "Epoch: 311, Generator Loss: 0.6488772630691528, Discriminator Loss: 1.6673251390457153\n",
            "Epoch: 312, Generator Loss: 0.6352800726890564, Discriminator Loss: 1.6697102785110474\n",
            "Epoch: 313, Generator Loss: 0.642504096031189, Discriminator Loss: 1.7025638818740845\n",
            "Epoch: 314, Generator Loss: 0.643029510974884, Discriminator Loss: 1.6820297241210938\n",
            "Epoch: 315, Generator Loss: 0.6405842900276184, Discriminator Loss: 1.7076090574264526\n",
            "Epoch: 316, Generator Loss: 0.6423209309577942, Discriminator Loss: 1.695090413093567\n",
            "Epoch: 317, Generator Loss: 0.6439040899276733, Discriminator Loss: 1.7476117610931396\n",
            "Epoch: 318, Generator Loss: 0.6419994235038757, Discriminator Loss: 1.6418421268463135\n",
            "Epoch: 319, Generator Loss: 0.6411580443382263, Discriminator Loss: 1.6744927167892456\n",
            "Epoch: 320, Generator Loss: 0.6336439847946167, Discriminator Loss: 1.7232341766357422\n",
            "Epoch: 321, Generator Loss: 0.641753613948822, Discriminator Loss: 1.7189486026763916\n",
            "Epoch: 322, Generator Loss: 0.6411749124526978, Discriminator Loss: 1.7273023128509521\n",
            "Epoch: 323, Generator Loss: 0.6538487672805786, Discriminator Loss: 1.6982523202896118\n",
            "Epoch: 324, Generator Loss: 0.6393874287605286, Discriminator Loss: 1.6944007873535156\n",
            "Epoch: 325, Generator Loss: 0.643139660358429, Discriminator Loss: 1.709736943244934\n",
            "Epoch: 326, Generator Loss: 0.6471273303031921, Discriminator Loss: 1.696259617805481\n",
            "Epoch: 327, Generator Loss: 0.6439687013626099, Discriminator Loss: 1.7098512649536133\n",
            "Epoch: 328, Generator Loss: 0.6441207528114319, Discriminator Loss: 1.6927881240844727\n",
            "Epoch: 329, Generator Loss: 0.6538675427436829, Discriminator Loss: 1.6943638324737549\n",
            "Epoch: 330, Generator Loss: 0.6443697810173035, Discriminator Loss: 1.6886612176895142\n",
            "Epoch: 331, Generator Loss: 0.6463592648506165, Discriminator Loss: 1.698427438735962\n",
            "Epoch: 332, Generator Loss: 0.6530896425247192, Discriminator Loss: 1.7214006185531616\n",
            "Epoch: 333, Generator Loss: 0.648661196231842, Discriminator Loss: 1.6833547353744507\n",
            "Epoch: 334, Generator Loss: 0.6480315327644348, Discriminator Loss: 1.677469253540039\n",
            "Epoch: 335, Generator Loss: 0.6461464762687683, Discriminator Loss: 1.6372073888778687\n",
            "Epoch: 336, Generator Loss: 0.6507520079612732, Discriminator Loss: 1.6977587938308716\n",
            "Epoch: 337, Generator Loss: 0.650206983089447, Discriminator Loss: 1.7230720520019531\n",
            "Epoch: 338, Generator Loss: 0.6487578749656677, Discriminator Loss: 1.6764246225357056\n",
            "Epoch: 339, Generator Loss: 0.6530085802078247, Discriminator Loss: 1.696730375289917\n",
            "Epoch: 340, Generator Loss: 0.6482005715370178, Discriminator Loss: 1.6860673427581787\n",
            "Epoch: 341, Generator Loss: 0.651825487613678, Discriminator Loss: 1.6996127367019653\n",
            "Epoch: 342, Generator Loss: 0.6499524712562561, Discriminator Loss: 1.7055333852767944\n",
            "Epoch: 343, Generator Loss: 0.6494059562683105, Discriminator Loss: 1.6800388097763062\n",
            "Epoch: 344, Generator Loss: 0.6409929990768433, Discriminator Loss: 1.6861563920974731\n",
            "Epoch: 345, Generator Loss: 0.6549736857414246, Discriminator Loss: 1.6458942890167236\n",
            "Epoch: 346, Generator Loss: 0.6486549973487854, Discriminator Loss: 1.6705856323242188\n",
            "Epoch: 347, Generator Loss: 0.651098906993866, Discriminator Loss: 1.698678731918335\n",
            "Epoch: 348, Generator Loss: 0.6534900069236755, Discriminator Loss: 1.7280921936035156\n",
            "Epoch: 349, Generator Loss: 0.655157208442688, Discriminator Loss: 1.731066346168518\n",
            "Epoch: 350, Generator Loss: 0.6536402702331543, Discriminator Loss: 1.6886059045791626\n",
            "Epoch: 351, Generator Loss: 0.6563877463340759, Discriminator Loss: 1.6613121032714844\n",
            "Epoch: 352, Generator Loss: 0.6432307362556458, Discriminator Loss: 1.6946407556533813\n",
            "Epoch: 353, Generator Loss: 0.646729052066803, Discriminator Loss: 1.6917955875396729\n",
            "Epoch: 354, Generator Loss: 0.643084704875946, Discriminator Loss: 1.695717692375183\n",
            "Epoch: 355, Generator Loss: 0.6356577277183533, Discriminator Loss: 1.6649482250213623\n",
            "Epoch: 356, Generator Loss: 0.6420130133628845, Discriminator Loss: 1.7270227670669556\n",
            "Epoch: 357, Generator Loss: 0.6449942588806152, Discriminator Loss: 1.7102911472320557\n",
            "Epoch: 358, Generator Loss: 0.6454346776008606, Discriminator Loss: 1.6970735788345337\n",
            "Epoch: 359, Generator Loss: 0.6483719944953918, Discriminator Loss: 1.6393067836761475\n",
            "Epoch: 360, Generator Loss: 0.6413000822067261, Discriminator Loss: 1.714529037475586\n",
            "Epoch: 361, Generator Loss: 0.650255024433136, Discriminator Loss: 1.6897858381271362\n",
            "Epoch: 362, Generator Loss: 0.6560665965080261, Discriminator Loss: 1.6731330156326294\n",
            "Epoch: 363, Generator Loss: 0.6527413129806519, Discriminator Loss: 1.6634390354156494\n",
            "Epoch: 364, Generator Loss: 0.6465504765510559, Discriminator Loss: 1.716728925704956\n",
            "Epoch: 365, Generator Loss: 0.6543140411376953, Discriminator Loss: 1.6930490732192993\n",
            "Epoch: 366, Generator Loss: 0.6534043550491333, Discriminator Loss: 1.6745396852493286\n",
            "Epoch: 367, Generator Loss: 0.6479657888412476, Discriminator Loss: 1.6946688890457153\n",
            "Epoch: 368, Generator Loss: 0.6478835344314575, Discriminator Loss: 1.7000287771224976\n",
            "Epoch: 369, Generator Loss: 0.6524452567100525, Discriminator Loss: 1.7122230529785156\n",
            "Epoch: 370, Generator Loss: 0.6588584184646606, Discriminator Loss: 1.68062424659729\n",
            "Epoch: 371, Generator Loss: 0.6600852608680725, Discriminator Loss: 1.7112023830413818\n",
            "Epoch: 372, Generator Loss: 0.6518851518630981, Discriminator Loss: 1.688549280166626\n",
            "Epoch: 373, Generator Loss: 0.650481104850769, Discriminator Loss: 1.6847975254058838\n",
            "Epoch: 374, Generator Loss: 0.6486272215843201, Discriminator Loss: 1.7053890228271484\n",
            "Epoch: 375, Generator Loss: 0.657839298248291, Discriminator Loss: 1.6730172634124756\n",
            "Epoch: 376, Generator Loss: 0.6502650380134583, Discriminator Loss: 1.666657567024231\n",
            "Epoch: 377, Generator Loss: 0.6393639445304871, Discriminator Loss: 1.7691067457199097\n",
            "Epoch: 378, Generator Loss: 0.649840235710144, Discriminator Loss: 1.7038177251815796\n",
            "Epoch: 379, Generator Loss: 0.6512786149978638, Discriminator Loss: 1.6509445905685425\n",
            "Epoch: 380, Generator Loss: 0.6479384303092957, Discriminator Loss: 1.655727505683899\n",
            "Epoch: 381, Generator Loss: 0.6518164873123169, Discriminator Loss: 1.6746279001235962\n",
            "Epoch: 382, Generator Loss: 0.6529671549797058, Discriminator Loss: 1.676395297050476\n",
            "Epoch: 383, Generator Loss: 0.6492168307304382, Discriminator Loss: 1.6755074262619019\n",
            "Epoch: 384, Generator Loss: 0.6566900014877319, Discriminator Loss: 1.6674937009811401\n",
            "Epoch: 385, Generator Loss: 0.6416869759559631, Discriminator Loss: 1.679625153541565\n",
            "Epoch: 386, Generator Loss: 0.6485247015953064, Discriminator Loss: 1.6929353475570679\n",
            "Epoch: 387, Generator Loss: 0.657448410987854, Discriminator Loss: 1.6521095037460327\n",
            "Epoch: 388, Generator Loss: 0.6494102478027344, Discriminator Loss: 1.6663278341293335\n",
            "Epoch: 389, Generator Loss: 0.6529092788696289, Discriminator Loss: 1.673516869544983\n",
            "Epoch: 390, Generator Loss: 0.652161180973053, Discriminator Loss: 1.696226716041565\n",
            "Epoch: 391, Generator Loss: 0.6512358784675598, Discriminator Loss: 1.6839112043380737\n",
            "Epoch: 392, Generator Loss: 0.6528710126876831, Discriminator Loss: 1.703217625617981\n",
            "Epoch: 393, Generator Loss: 0.6523156762123108, Discriminator Loss: 1.6728049516677856\n",
            "Epoch: 394, Generator Loss: 0.6518306732177734, Discriminator Loss: 1.679080843925476\n",
            "Epoch: 395, Generator Loss: 0.6523564457893372, Discriminator Loss: 1.698641061782837\n",
            "Epoch: 396, Generator Loss: 0.6550713181495667, Discriminator Loss: 1.654215693473816\n",
            "Epoch: 397, Generator Loss: 0.6569225192070007, Discriminator Loss: 1.6904383897781372\n",
            "Epoch: 398, Generator Loss: 0.6514827013015747, Discriminator Loss: 1.681557536125183\n",
            "Epoch: 399, Generator Loss: 0.6569217443466187, Discriminator Loss: 1.6766446828842163\n",
            "Epoch: 400, Generator Loss: 0.655833899974823, Discriminator Loss: 1.7053451538085938\n",
            "Epoch: 401, Generator Loss: 0.6608584523200989, Discriminator Loss: 1.6943312883377075\n",
            "Epoch: 402, Generator Loss: 0.6633474230766296, Discriminator Loss: 1.6925908327102661\n",
            "Epoch: 403, Generator Loss: 0.6640467047691345, Discriminator Loss: 1.7077337503433228\n",
            "Epoch: 404, Generator Loss: 0.662066638469696, Discriminator Loss: 1.6984193325042725\n",
            "Epoch: 405, Generator Loss: 0.656554639339447, Discriminator Loss: 1.6842330694198608\n",
            "Epoch: 406, Generator Loss: 0.6572906374931335, Discriminator Loss: 1.6836738586425781\n",
            "Epoch: 407, Generator Loss: 0.6572604775428772, Discriminator Loss: 1.668912649154663\n",
            "Epoch: 408, Generator Loss: 0.6536831259727478, Discriminator Loss: 1.6939363479614258\n",
            "Epoch: 409, Generator Loss: 0.6552592515945435, Discriminator Loss: 1.6606038808822632\n",
            "Epoch: 410, Generator Loss: 0.6529344916343689, Discriminator Loss: 1.6881322860717773\n",
            "Epoch: 411, Generator Loss: 0.648289680480957, Discriminator Loss: 1.7213157415390015\n",
            "Epoch: 412, Generator Loss: 0.6481661200523376, Discriminator Loss: 1.6586428880691528\n",
            "Epoch: 413, Generator Loss: 0.6449481844902039, Discriminator Loss: 1.6717207431793213\n",
            "Epoch: 414, Generator Loss: 0.6488967537879944, Discriminator Loss: 1.6819826364517212\n",
            "Epoch: 415, Generator Loss: 0.6490365266799927, Discriminator Loss: 1.665506362915039\n",
            "Epoch: 416, Generator Loss: 0.6500602960586548, Discriminator Loss: 1.6866968870162964\n",
            "Epoch: 417, Generator Loss: 0.6575499773025513, Discriminator Loss: 1.6417170763015747\n",
            "Epoch: 418, Generator Loss: 0.6498726010322571, Discriminator Loss: 1.6775449514389038\n",
            "Epoch: 419, Generator Loss: 0.6484846472740173, Discriminator Loss: 1.6591447591781616\n",
            "Epoch: 420, Generator Loss: 0.6550324559211731, Discriminator Loss: 1.6640703678131104\n",
            "Epoch: 421, Generator Loss: 0.6562325358390808, Discriminator Loss: 1.6806488037109375\n",
            "Epoch: 422, Generator Loss: 0.6425971984863281, Discriminator Loss: 1.6959245204925537\n",
            "Epoch: 423, Generator Loss: 0.6473649740219116, Discriminator Loss: 1.6937849521636963\n",
            "Epoch: 424, Generator Loss: 0.6484030485153198, Discriminator Loss: 1.6990060806274414\n",
            "Epoch: 425, Generator Loss: 0.650237500667572, Discriminator Loss: 1.6693354845046997\n",
            "Epoch: 426, Generator Loss: 0.6468190550804138, Discriminator Loss: 1.6998389959335327\n",
            "Epoch: 427, Generator Loss: 0.6389466524124146, Discriminator Loss: 1.66777765750885\n",
            "Epoch: 428, Generator Loss: 0.6454753875732422, Discriminator Loss: 1.6802736520767212\n",
            "Epoch: 429, Generator Loss: 0.642048716545105, Discriminator Loss: 1.7517350912094116\n",
            "Epoch: 430, Generator Loss: 0.6427013278007507, Discriminator Loss: 1.6579419374465942\n",
            "Epoch: 431, Generator Loss: 0.6410039067268372, Discriminator Loss: 1.6682668924331665\n",
            "Epoch: 432, Generator Loss: 0.6349992752075195, Discriminator Loss: 1.7153549194335938\n",
            "Epoch: 433, Generator Loss: 0.6312108039855957, Discriminator Loss: 1.6873199939727783\n",
            "Epoch: 434, Generator Loss: 0.6441245675086975, Discriminator Loss: 1.6712820529937744\n",
            "Epoch: 435, Generator Loss: 0.6489973664283752, Discriminator Loss: 1.7061254978179932\n",
            "Epoch: 436, Generator Loss: 0.6436470150947571, Discriminator Loss: 1.6660109758377075\n",
            "Epoch: 437, Generator Loss: 0.6382003426551819, Discriminator Loss: 1.7276619672775269\n",
            "Epoch: 438, Generator Loss: 0.6365179419517517, Discriminator Loss: 1.7289892435073853\n",
            "Epoch: 439, Generator Loss: 0.6418127417564392, Discriminator Loss: 1.6832600831985474\n",
            "Epoch: 440, Generator Loss: 0.6386955380439758, Discriminator Loss: 1.7099024057388306\n",
            "Epoch: 441, Generator Loss: 0.6392078995704651, Discriminator Loss: 1.7121803760528564\n",
            "Epoch: 442, Generator Loss: 0.6381601691246033, Discriminator Loss: 1.6889714002609253\n",
            "Epoch: 443, Generator Loss: 0.6391387581825256, Discriminator Loss: 1.6843321323394775\n",
            "Epoch: 444, Generator Loss: 0.6372507810592651, Discriminator Loss: 1.6699146032333374\n",
            "Epoch: 445, Generator Loss: 0.6365130543708801, Discriminator Loss: 1.6779212951660156\n",
            "Epoch: 446, Generator Loss: 0.6298976540565491, Discriminator Loss: 1.6823819875717163\n",
            "Epoch: 447, Generator Loss: 0.6354244947433472, Discriminator Loss: 1.6564441919326782\n",
            "Epoch: 448, Generator Loss: 0.6344457864761353, Discriminator Loss: 1.7039587497711182\n",
            "Epoch: 449, Generator Loss: 0.6365236639976501, Discriminator Loss: 1.681031584739685\n",
            "Epoch: 450, Generator Loss: 0.6305633187294006, Discriminator Loss: 1.7422239780426025\n",
            "Epoch: 451, Generator Loss: 0.6382112503051758, Discriminator Loss: 1.7130056619644165\n",
            "Epoch: 452, Generator Loss: 0.6380292177200317, Discriminator Loss: 1.6965081691741943\n",
            "Epoch: 453, Generator Loss: 0.6336358189582825, Discriminator Loss: 1.6863670349121094\n",
            "Epoch: 454, Generator Loss: 0.6311357617378235, Discriminator Loss: 1.699590802192688\n",
            "Epoch: 455, Generator Loss: 0.6267305016517639, Discriminator Loss: 1.7259668111801147\n",
            "Epoch: 456, Generator Loss: 0.6269058585166931, Discriminator Loss: 1.7010345458984375\n",
            "Epoch: 457, Generator Loss: 0.6302587389945984, Discriminator Loss: 1.7359391450881958\n",
            "Epoch: 458, Generator Loss: 0.6227053999900818, Discriminator Loss: 1.7203015089035034\n",
            "Epoch: 459, Generator Loss: 0.6242478489875793, Discriminator Loss: 1.7130452394485474\n",
            "Epoch: 460, Generator Loss: 0.6421509385108948, Discriminator Loss: 1.7371852397918701\n",
            "Epoch: 461, Generator Loss: 0.6391928791999817, Discriminator Loss: 1.7134233713150024\n",
            "Epoch: 462, Generator Loss: 0.6387771964073181, Discriminator Loss: 1.746658205986023\n",
            "Epoch: 463, Generator Loss: 0.6373630166053772, Discriminator Loss: 1.7320626974105835\n",
            "Epoch: 464, Generator Loss: 0.6409958004951477, Discriminator Loss: 1.7239080667495728\n",
            "Epoch: 465, Generator Loss: 0.6380200982093811, Discriminator Loss: 1.7392240762710571\n",
            "Epoch: 466, Generator Loss: 0.6382822394371033, Discriminator Loss: 1.7268637418746948\n",
            "Epoch: 467, Generator Loss: 0.6472324728965759, Discriminator Loss: 1.6907659769058228\n",
            "Epoch: 468, Generator Loss: 0.6422556638717651, Discriminator Loss: 1.740857720375061\n",
            "Epoch: 469, Generator Loss: 0.6468260884284973, Discriminator Loss: 1.7115331888198853\n",
            "Epoch: 470, Generator Loss: 0.6374098062515259, Discriminator Loss: 1.7264858484268188\n",
            "Epoch: 471, Generator Loss: 0.6377214193344116, Discriminator Loss: 1.7260189056396484\n",
            "Epoch: 472, Generator Loss: 0.6398425698280334, Discriminator Loss: 1.72616708278656\n",
            "Epoch: 473, Generator Loss: 0.6440829038619995, Discriminator Loss: 1.7271982431411743\n",
            "Epoch: 474, Generator Loss: 0.644637405872345, Discriminator Loss: 1.6986690759658813\n",
            "Epoch: 475, Generator Loss: 0.6458317041397095, Discriminator Loss: 1.7493088245391846\n",
            "Epoch: 476, Generator Loss: 0.6408771872520447, Discriminator Loss: 1.7010904550552368\n",
            "Epoch: 477, Generator Loss: 0.6440266966819763, Discriminator Loss: 1.6921608448028564\n",
            "Epoch: 478, Generator Loss: 0.6328639388084412, Discriminator Loss: 1.7435247898101807\n",
            "Epoch: 479, Generator Loss: 0.6493088006973267, Discriminator Loss: 1.6666240692138672\n",
            "Epoch: 480, Generator Loss: 0.6469473242759705, Discriminator Loss: 1.741018533706665\n",
            "Epoch: 481, Generator Loss: 0.6483659148216248, Discriminator Loss: 1.7095192670822144\n",
            "Epoch: 482, Generator Loss: 0.6544705629348755, Discriminator Loss: 1.7004305124282837\n",
            "Epoch: 483, Generator Loss: 0.639735996723175, Discriminator Loss: 1.6825374364852905\n",
            "Epoch: 484, Generator Loss: 0.6387744545936584, Discriminator Loss: 1.7525376081466675\n",
            "Epoch: 485, Generator Loss: 0.6404043436050415, Discriminator Loss: 1.7331864833831787\n",
            "Epoch: 486, Generator Loss: 0.6376029253005981, Discriminator Loss: 1.7226918935775757\n",
            "Epoch: 487, Generator Loss: 0.6497175097465515, Discriminator Loss: 1.718881607055664\n",
            "Epoch: 488, Generator Loss: 0.6383171677589417, Discriminator Loss: 1.7296276092529297\n",
            "Epoch: 489, Generator Loss: 0.6386598348617554, Discriminator Loss: 1.7561150789260864\n",
            "Epoch: 490, Generator Loss: 0.6334605813026428, Discriminator Loss: 1.7275303602218628\n",
            "Epoch: 491, Generator Loss: 0.6460512280464172, Discriminator Loss: 1.7396091222763062\n",
            "Epoch: 492, Generator Loss: 0.6466561555862427, Discriminator Loss: 1.7021329402923584\n",
            "Epoch: 493, Generator Loss: 0.6545848846435547, Discriminator Loss: 1.675658106803894\n",
            "Epoch: 494, Generator Loss: 0.6409793496131897, Discriminator Loss: 1.7288497686386108\n",
            "Epoch: 495, Generator Loss: 0.6479927897453308, Discriminator Loss: 1.7237962484359741\n",
            "Epoch: 496, Generator Loss: 0.6459988355636597, Discriminator Loss: 1.7035281658172607\n",
            "Epoch: 497, Generator Loss: 0.6468120217323303, Discriminator Loss: 1.732664942741394\n",
            "Epoch: 498, Generator Loss: 0.646313488483429, Discriminator Loss: 1.7399671077728271\n",
            "Epoch: 499, Generator Loss: 0.6446083188056946, Discriminator Loss: 1.7300862073898315\n",
            "Epoch: 500, Generator Loss: 0.6465867757797241, Discriminator Loss: 1.7159841060638428\n",
            "Epoch: 501, Generator Loss: 0.6460285186767578, Discriminator Loss: 1.6874383687973022\n",
            "Epoch: 502, Generator Loss: 0.6456257104873657, Discriminator Loss: 1.7023175954818726\n",
            "Epoch: 503, Generator Loss: 0.643633246421814, Discriminator Loss: 1.682584524154663\n",
            "Epoch: 504, Generator Loss: 0.6315022110939026, Discriminator Loss: 1.712424397468567\n",
            "Epoch: 505, Generator Loss: 0.6404158473014832, Discriminator Loss: 1.7217235565185547\n",
            "Epoch: 506, Generator Loss: 0.6407049894332886, Discriminator Loss: 1.7395509481430054\n",
            "Epoch: 507, Generator Loss: 0.651261031627655, Discriminator Loss: 1.7207889556884766\n",
            "Epoch: 508, Generator Loss: 0.6371370553970337, Discriminator Loss: 1.7288981676101685\n",
            "Epoch: 509, Generator Loss: 0.6368877291679382, Discriminator Loss: 1.733722448348999\n",
            "Epoch: 510, Generator Loss: 0.6415016055107117, Discriminator Loss: 1.699857473373413\n",
            "Epoch: 511, Generator Loss: 0.6460474133491516, Discriminator Loss: 1.7265273332595825\n",
            "Epoch: 512, Generator Loss: 0.6464323401451111, Discriminator Loss: 1.691421627998352\n",
            "Epoch: 513, Generator Loss: 0.6431714296340942, Discriminator Loss: 1.7233145236968994\n",
            "Epoch: 514, Generator Loss: 0.6478354334831238, Discriminator Loss: 1.696202278137207\n",
            "Epoch: 515, Generator Loss: 0.6436597108840942, Discriminator Loss: 1.689874529838562\n",
            "Epoch: 516, Generator Loss: 0.6445439457893372, Discriminator Loss: 1.7291213274002075\n",
            "Epoch: 517, Generator Loss: 0.6450526118278503, Discriminator Loss: 1.6791390180587769\n",
            "Epoch: 518, Generator Loss: 0.6417635679244995, Discriminator Loss: 1.7001069784164429\n",
            "Epoch: 519, Generator Loss: 0.6416174173355103, Discriminator Loss: 1.6857212781906128\n",
            "Epoch: 520, Generator Loss: 0.6346318125724792, Discriminator Loss: 1.733219861984253\n",
            "Epoch: 521, Generator Loss: 0.6401599645614624, Discriminator Loss: 1.7015329599380493\n",
            "Epoch: 522, Generator Loss: 0.6417166590690613, Discriminator Loss: 1.7246837615966797\n",
            "Epoch: 523, Generator Loss: 0.6437986493110657, Discriminator Loss: 1.7250064611434937\n",
            "Epoch: 524, Generator Loss: 0.6435602903366089, Discriminator Loss: 1.714203119277954\n",
            "Epoch: 525, Generator Loss: 0.6505936980247498, Discriminator Loss: 1.6878184080123901\n",
            "Epoch: 526, Generator Loss: 0.6451969742774963, Discriminator Loss: 1.706188440322876\n",
            "Epoch: 527, Generator Loss: 0.6391844153404236, Discriminator Loss: 1.71918785572052\n",
            "Epoch: 528, Generator Loss: 0.6497907638549805, Discriminator Loss: 1.69139564037323\n",
            "Epoch: 529, Generator Loss: 0.6435853242874146, Discriminator Loss: 1.7027615308761597\n",
            "Epoch: 530, Generator Loss: 0.6478977203369141, Discriminator Loss: 1.6680095195770264\n",
            "Epoch: 531, Generator Loss: 0.6422362923622131, Discriminator Loss: 1.6901931762695312\n",
            "Epoch: 532, Generator Loss: 0.6395028233528137, Discriminator Loss: 1.7183085680007935\n",
            "Epoch: 533, Generator Loss: 0.6390719413757324, Discriminator Loss: 1.752616286277771\n",
            "Epoch: 534, Generator Loss: 0.6461448073387146, Discriminator Loss: 1.7244123220443726\n",
            "Epoch: 535, Generator Loss: 0.6382750272750854, Discriminator Loss: 1.7053464651107788\n",
            "Epoch: 536, Generator Loss: 0.6468350291252136, Discriminator Loss: 1.7276287078857422\n",
            "Epoch: 537, Generator Loss: 0.6451752781867981, Discriminator Loss: 1.733986735343933\n",
            "Epoch: 538, Generator Loss: 0.6455855369567871, Discriminator Loss: 1.7040364742279053\n",
            "Epoch: 539, Generator Loss: 0.6458898782730103, Discriminator Loss: 1.7507258653640747\n",
            "Epoch: 540, Generator Loss: 0.6437700390815735, Discriminator Loss: 1.6975432634353638\n",
            "Epoch: 541, Generator Loss: 0.6432673335075378, Discriminator Loss: 1.682424783706665\n",
            "Epoch: 542, Generator Loss: 0.6449616551399231, Discriminator Loss: 1.7289115190505981\n",
            "Epoch: 543, Generator Loss: 0.6366567611694336, Discriminator Loss: 1.708222508430481\n",
            "Epoch: 544, Generator Loss: 0.6322803497314453, Discriminator Loss: 1.7114955186843872\n",
            "Epoch: 545, Generator Loss: 0.6455966830253601, Discriminator Loss: 1.6807993650436401\n",
            "Epoch: 546, Generator Loss: 0.6406574845314026, Discriminator Loss: 1.7394542694091797\n",
            "Epoch: 547, Generator Loss: 0.6416850686073303, Discriminator Loss: 1.724012017250061\n",
            "Epoch: 548, Generator Loss: 0.6401681303977966, Discriminator Loss: 1.704384446144104\n",
            "Epoch: 549, Generator Loss: 0.6320453882217407, Discriminator Loss: 1.7224096059799194\n",
            "Epoch: 550, Generator Loss: 0.6362686157226562, Discriminator Loss: 1.7556642293930054\n",
            "Epoch: 551, Generator Loss: 0.6494402885437012, Discriminator Loss: 1.6904728412628174\n",
            "Epoch: 552, Generator Loss: 0.6489729285240173, Discriminator Loss: 1.7021461725234985\n",
            "Epoch: 553, Generator Loss: 0.649836540222168, Discriminator Loss: 1.715810775756836\n",
            "Epoch: 554, Generator Loss: 0.6393277049064636, Discriminator Loss: 1.7054554224014282\n",
            "Epoch: 555, Generator Loss: 0.6521444320678711, Discriminator Loss: 1.72475266456604\n",
            "Epoch: 556, Generator Loss: 0.64891517162323, Discriminator Loss: 1.6891615390777588\n",
            "Epoch: 557, Generator Loss: 0.6475425362586975, Discriminator Loss: 1.7075157165527344\n",
            "Epoch: 558, Generator Loss: 0.6434658169746399, Discriminator Loss: 1.7023178339004517\n",
            "Epoch: 559, Generator Loss: 0.6396093368530273, Discriminator Loss: 1.6593021154403687\n",
            "Epoch: 560, Generator Loss: 0.6456161737442017, Discriminator Loss: 1.7162034511566162\n",
            "Epoch: 561, Generator Loss: 0.6336517930030823, Discriminator Loss: 1.7600843906402588\n",
            "Epoch: 562, Generator Loss: 0.6502593159675598, Discriminator Loss: 1.6701890230178833\n",
            "Epoch: 563, Generator Loss: 0.6462160348892212, Discriminator Loss: 1.7266788482666016\n",
            "Epoch: 564, Generator Loss: 0.6390146613121033, Discriminator Loss: 1.7181769609451294\n",
            "Epoch: 565, Generator Loss: 0.6494898796081543, Discriminator Loss: 1.7173501253128052\n",
            "Epoch: 566, Generator Loss: 0.6480706334114075, Discriminator Loss: 1.7200766801834106\n",
            "Epoch: 567, Generator Loss: 0.6385363340377808, Discriminator Loss: 1.7352887392044067\n",
            "Epoch: 568, Generator Loss: 0.645427405834198, Discriminator Loss: 1.6773624420166016\n",
            "Epoch: 569, Generator Loss: 0.6406273245811462, Discriminator Loss: 1.7098504304885864\n",
            "Epoch: 570, Generator Loss: 0.6411681771278381, Discriminator Loss: 1.689942717552185\n",
            "Epoch: 571, Generator Loss: 0.6484667658805847, Discriminator Loss: 1.7147146463394165\n",
            "Epoch: 572, Generator Loss: 0.6379316449165344, Discriminator Loss: 1.71744704246521\n",
            "Epoch: 573, Generator Loss: 0.6381335854530334, Discriminator Loss: 1.698133111000061\n",
            "Epoch: 574, Generator Loss: 0.6452977061271667, Discriminator Loss: 1.715394139289856\n",
            "Epoch: 575, Generator Loss: 0.6459864377975464, Discriminator Loss: 1.7314311265945435\n",
            "Epoch: 576, Generator Loss: 0.6459803581237793, Discriminator Loss: 1.7258292436599731\n",
            "Epoch: 577, Generator Loss: 0.6444963216781616, Discriminator Loss: 1.7474660873413086\n",
            "Epoch: 578, Generator Loss: 0.6510491967201233, Discriminator Loss: 1.68548583984375\n",
            "Epoch: 579, Generator Loss: 0.6476432085037231, Discriminator Loss: 1.6777387857437134\n",
            "Epoch: 580, Generator Loss: 0.6536946296691895, Discriminator Loss: 1.691680908203125\n",
            "Epoch: 581, Generator Loss: 0.6467289328575134, Discriminator Loss: 1.7328191995620728\n",
            "Epoch: 582, Generator Loss: 0.6532516479492188, Discriminator Loss: 1.7276585102081299\n",
            "Epoch: 583, Generator Loss: 0.6447929739952087, Discriminator Loss: 1.677324891090393\n",
            "Epoch: 584, Generator Loss: 0.6445941925048828, Discriminator Loss: 1.7300300598144531\n",
            "Epoch: 585, Generator Loss: 0.6438347101211548, Discriminator Loss: 1.6921042203903198\n",
            "Epoch: 586, Generator Loss: 0.6491560935974121, Discriminator Loss: 1.7227003574371338\n",
            "Epoch: 587, Generator Loss: 0.6552411317825317, Discriminator Loss: 1.68121337890625\n",
            "Epoch: 588, Generator Loss: 0.6446952819824219, Discriminator Loss: 1.7126566171646118\n",
            "Epoch: 589, Generator Loss: 0.6544064283370972, Discriminator Loss: 1.7300485372543335\n",
            "Epoch: 590, Generator Loss: 0.6547861695289612, Discriminator Loss: 1.688804268836975\n",
            "Epoch: 591, Generator Loss: 0.6581984162330627, Discriminator Loss: 1.7197335958480835\n",
            "Epoch: 592, Generator Loss: 0.6561910510063171, Discriminator Loss: 1.7396231889724731\n",
            "Epoch: 593, Generator Loss: 0.6609682440757751, Discriminator Loss: 1.7209975719451904\n",
            "Epoch: 594, Generator Loss: 0.6471766233444214, Discriminator Loss: 1.6934856176376343\n",
            "Epoch: 595, Generator Loss: 0.6462430953979492, Discriminator Loss: 1.6919153928756714\n",
            "Epoch: 596, Generator Loss: 0.6408125758171082, Discriminator Loss: 1.7283484935760498\n",
            "Epoch: 597, Generator Loss: 0.6493585705757141, Discriminator Loss: 1.7023932933807373\n",
            "Epoch: 598, Generator Loss: 0.650325357913971, Discriminator Loss: 1.7073715925216675\n",
            "Epoch: 599, Generator Loss: 0.6464393734931946, Discriminator Loss: 1.7107115983963013\n",
            "Epoch: 600, Generator Loss: 0.640667736530304, Discriminator Loss: 1.7643671035766602\n",
            "Epoch: 601, Generator Loss: 0.6360573768615723, Discriminator Loss: 1.7052233219146729\n",
            "Epoch: 602, Generator Loss: 0.6408621668815613, Discriminator Loss: 1.7274101972579956\n",
            "Epoch: 603, Generator Loss: 0.6508602499961853, Discriminator Loss: 1.700880765914917\n",
            "Epoch: 604, Generator Loss: 0.6443660855293274, Discriminator Loss: 1.6816424131393433\n",
            "Epoch: 605, Generator Loss: 0.6543483734130859, Discriminator Loss: 1.717677354812622\n",
            "Epoch: 606, Generator Loss: 0.644430935382843, Discriminator Loss: 1.7002851963043213\n",
            "Epoch: 607, Generator Loss: 0.6484087705612183, Discriminator Loss: 1.6780575513839722\n",
            "Epoch: 608, Generator Loss: 0.6374434232711792, Discriminator Loss: 1.6895710229873657\n",
            "Epoch: 609, Generator Loss: 0.6419127583503723, Discriminator Loss: 1.7350033521652222\n",
            "Epoch: 610, Generator Loss: 0.6450770497322083, Discriminator Loss: 1.6905438899993896\n",
            "Epoch: 611, Generator Loss: 0.6390124559402466, Discriminator Loss: 1.723897099494934\n",
            "Epoch: 612, Generator Loss: 0.6385976672172546, Discriminator Loss: 1.7208298444747925\n",
            "Epoch: 613, Generator Loss: 0.6532154679298401, Discriminator Loss: 1.721074104309082\n",
            "Epoch: 614, Generator Loss: 0.6480951309204102, Discriminator Loss: 1.685648798942566\n",
            "Epoch: 615, Generator Loss: 0.6473840475082397, Discriminator Loss: 1.7613219022750854\n",
            "Epoch: 616, Generator Loss: 0.6498456597328186, Discriminator Loss: 1.698244333267212\n",
            "Epoch: 617, Generator Loss: 0.6521903276443481, Discriminator Loss: 1.7075685262680054\n",
            "Epoch: 618, Generator Loss: 0.647883415222168, Discriminator Loss: 1.7007532119750977\n",
            "Epoch: 619, Generator Loss: 0.6451229453086853, Discriminator Loss: 1.7299894094467163\n",
            "Epoch: 620, Generator Loss: 0.6522132754325867, Discriminator Loss: 1.7024046182632446\n",
            "Epoch: 621, Generator Loss: 0.6495028734207153, Discriminator Loss: 1.7064999341964722\n",
            "Epoch: 622, Generator Loss: 0.6487794518470764, Discriminator Loss: 1.7128114700317383\n",
            "Epoch: 623, Generator Loss: 0.6468440294265747, Discriminator Loss: 1.7018928527832031\n",
            "Epoch: 624, Generator Loss: 0.6501750946044922, Discriminator Loss: 1.704114317893982\n",
            "Epoch: 625, Generator Loss: 0.6519048810005188, Discriminator Loss: 1.7134772539138794\n",
            "Epoch: 626, Generator Loss: 0.6496487855911255, Discriminator Loss: 1.7133853435516357\n",
            "Epoch: 627, Generator Loss: 0.6488069295883179, Discriminator Loss: 1.7192658185958862\n",
            "Epoch: 628, Generator Loss: 0.6520361304283142, Discriminator Loss: 1.7396137714385986\n",
            "Epoch: 629, Generator Loss: 0.6389988660812378, Discriminator Loss: 1.7110227346420288\n",
            "Epoch: 630, Generator Loss: 0.6508556008338928, Discriminator Loss: 1.6788095235824585\n",
            "Epoch: 631, Generator Loss: 0.6489812135696411, Discriminator Loss: 1.7069934606552124\n",
            "Epoch: 632, Generator Loss: 0.6446418762207031, Discriminator Loss: 1.6941732168197632\n",
            "Epoch: 633, Generator Loss: 0.6439553499221802, Discriminator Loss: 1.703308343887329\n",
            "Epoch: 634, Generator Loss: 0.645875096321106, Discriminator Loss: 1.7379515171051025\n",
            "Epoch: 635, Generator Loss: 0.6495761871337891, Discriminator Loss: 1.7349752187728882\n",
            "Epoch: 636, Generator Loss: 0.6406208276748657, Discriminator Loss: 1.7139689922332764\n",
            "Epoch: 637, Generator Loss: 0.6434067487716675, Discriminator Loss: 1.723318338394165\n",
            "Epoch: 638, Generator Loss: 0.6582432985305786, Discriminator Loss: 1.7031503915786743\n",
            "Epoch: 639, Generator Loss: 0.6505247950553894, Discriminator Loss: 1.724421501159668\n",
            "Epoch: 640, Generator Loss: 0.6423639059066772, Discriminator Loss: 1.708424687385559\n",
            "Epoch: 641, Generator Loss: 0.6457765698432922, Discriminator Loss: 1.6687899827957153\n",
            "Epoch: 642, Generator Loss: 0.6474562287330627, Discriminator Loss: 1.7423897981643677\n",
            "Epoch: 643, Generator Loss: 0.6507746577262878, Discriminator Loss: 1.7254942655563354\n",
            "Epoch: 644, Generator Loss: 0.652487576007843, Discriminator Loss: 1.6841703653335571\n",
            "Epoch: 645, Generator Loss: 0.642878532409668, Discriminator Loss: 1.709596037864685\n",
            "Epoch: 646, Generator Loss: 0.6481768488883972, Discriminator Loss: 1.6813005208969116\n",
            "Epoch: 647, Generator Loss: 0.644088864326477, Discriminator Loss: 1.7251845598220825\n",
            "Epoch: 648, Generator Loss: 0.6482861638069153, Discriminator Loss: 1.7014007568359375\n",
            "Epoch: 649, Generator Loss: 0.6480976939201355, Discriminator Loss: 1.7090208530426025\n",
            "Epoch: 650, Generator Loss: 0.641251266002655, Discriminator Loss: 1.7292425632476807\n",
            "Epoch: 651, Generator Loss: 0.6420422792434692, Discriminator Loss: 1.7293959856033325\n",
            "Epoch: 652, Generator Loss: 0.6498850584030151, Discriminator Loss: 1.6765506267547607\n",
            "Epoch: 653, Generator Loss: 0.6470620036125183, Discriminator Loss: 1.6961252689361572\n",
            "Epoch: 654, Generator Loss: 0.6525403261184692, Discriminator Loss: 1.7000200748443604\n",
            "Epoch: 655, Generator Loss: 0.6552196741104126, Discriminator Loss: 1.7249294519424438\n",
            "Epoch: 656, Generator Loss: 0.6552778482437134, Discriminator Loss: 1.7124086618423462\n",
            "Epoch: 657, Generator Loss: 0.6496023535728455, Discriminator Loss: 1.7230451107025146\n",
            "Epoch: 658, Generator Loss: 0.6598016023635864, Discriminator Loss: 1.6910802125930786\n",
            "Epoch: 659, Generator Loss: 0.6551877856254578, Discriminator Loss: 1.727120041847229\n",
            "Epoch: 660, Generator Loss: 0.6588956117630005, Discriminator Loss: 1.6999547481536865\n",
            "Epoch: 661, Generator Loss: 0.6610201597213745, Discriminator Loss: 1.6816712617874146\n",
            "Epoch: 662, Generator Loss: 0.6516863703727722, Discriminator Loss: 1.7263306379318237\n",
            "Epoch: 663, Generator Loss: 0.6554455161094666, Discriminator Loss: 1.694117784500122\n",
            "Epoch: 664, Generator Loss: 0.6591094732284546, Discriminator Loss: 1.7212132215499878\n",
            "Epoch: 665, Generator Loss: 0.6475030779838562, Discriminator Loss: 1.688982605934143\n",
            "Epoch: 666, Generator Loss: 0.6522754430770874, Discriminator Loss: 1.6969629526138306\n",
            "Epoch: 667, Generator Loss: 0.6547427177429199, Discriminator Loss: 1.7026126384735107\n",
            "Epoch: 668, Generator Loss: 0.6445921659469604, Discriminator Loss: 1.7033116817474365\n",
            "Epoch: 669, Generator Loss: 0.6509907841682434, Discriminator Loss: 1.6917507648468018\n",
            "Epoch: 670, Generator Loss: 0.6414352059364319, Discriminator Loss: 1.6912163496017456\n",
            "Epoch: 671, Generator Loss: 0.6466999053955078, Discriminator Loss: 1.716010332107544\n",
            "Epoch: 672, Generator Loss: 0.6409691572189331, Discriminator Loss: 1.6857430934906006\n",
            "Epoch: 673, Generator Loss: 0.6476035714149475, Discriminator Loss: 1.6896809339523315\n",
            "Epoch: 674, Generator Loss: 0.6481903195381165, Discriminator Loss: 1.7004072666168213\n",
            "Epoch: 675, Generator Loss: 0.6451383829116821, Discriminator Loss: 1.6611257791519165\n",
            "Epoch: 676, Generator Loss: 0.6327113509178162, Discriminator Loss: 1.7317324876785278\n",
            "Epoch: 677, Generator Loss: 0.6377248167991638, Discriminator Loss: 1.7006465196609497\n",
            "Epoch: 678, Generator Loss: 0.6433922052383423, Discriminator Loss: 1.7071669101715088\n",
            "Epoch: 679, Generator Loss: 0.6497190594673157, Discriminator Loss: 1.6828477382659912\n",
            "Epoch: 680, Generator Loss: 0.6439818143844604, Discriminator Loss: 1.7026736736297607\n",
            "Epoch: 681, Generator Loss: 0.6469658613204956, Discriminator Loss: 1.6991424560546875\n",
            "Epoch: 682, Generator Loss: 0.6398240923881531, Discriminator Loss: 1.6997405290603638\n",
            "Epoch: 683, Generator Loss: 0.6391592621803284, Discriminator Loss: 1.7318267822265625\n",
            "Epoch: 684, Generator Loss: 0.6408663392066956, Discriminator Loss: 1.673862338066101\n",
            "Epoch: 685, Generator Loss: 0.6500198245048523, Discriminator Loss: 1.685040831565857\n",
            "Epoch: 686, Generator Loss: 0.6358771920204163, Discriminator Loss: 1.7216780185699463\n",
            "Epoch: 687, Generator Loss: 0.6395267844200134, Discriminator Loss: 1.7305868864059448\n",
            "Epoch: 688, Generator Loss: 0.6420048475265503, Discriminator Loss: 1.699788212776184\n",
            "Epoch: 689, Generator Loss: 0.6447049379348755, Discriminator Loss: 1.7149924039840698\n",
            "Epoch: 690, Generator Loss: 0.6535834074020386, Discriminator Loss: 1.7112070322036743\n",
            "Epoch: 691, Generator Loss: 0.6467687487602234, Discriminator Loss: 1.691772699356079\n",
            "Epoch: 692, Generator Loss: 0.636663019657135, Discriminator Loss: 1.6497966051101685\n",
            "Epoch: 693, Generator Loss: 0.6400681734085083, Discriminator Loss: 1.6793047189712524\n",
            "Epoch: 694, Generator Loss: 0.643056333065033, Discriminator Loss: 1.6821733713150024\n",
            "Epoch: 695, Generator Loss: 0.64157634973526, Discriminator Loss: 1.6699150800704956\n",
            "Epoch: 696, Generator Loss: 0.6401907205581665, Discriminator Loss: 1.6892337799072266\n",
            "Epoch: 697, Generator Loss: 0.6330589652061462, Discriminator Loss: 1.677409052848816\n",
            "Epoch: 698, Generator Loss: 0.6385116577148438, Discriminator Loss: 1.7031491994857788\n",
            "Epoch: 699, Generator Loss: 0.6364205479621887, Discriminator Loss: 1.733725905418396\n",
            "Epoch: 700, Generator Loss: 0.6317831873893738, Discriminator Loss: 1.7031773328781128\n",
            "Epoch: 701, Generator Loss: 0.6478490233421326, Discriminator Loss: 1.7116345167160034\n",
            "Epoch: 702, Generator Loss: 0.6404679417610168, Discriminator Loss: 1.6890389919281006\n",
            "Epoch: 703, Generator Loss: 0.6385825872421265, Discriminator Loss: 1.7102320194244385\n",
            "Epoch: 704, Generator Loss: 0.6532966494560242, Discriminator Loss: 1.7021243572235107\n",
            "Epoch: 705, Generator Loss: 0.6487284898757935, Discriminator Loss: 1.6546480655670166\n",
            "Epoch: 706, Generator Loss: 0.6519132256507874, Discriminator Loss: 1.6760019063949585\n",
            "Epoch: 707, Generator Loss: 0.6471331715583801, Discriminator Loss: 1.684066891670227\n",
            "Epoch: 708, Generator Loss: 0.6470133066177368, Discriminator Loss: 1.6751196384429932\n",
            "Epoch: 709, Generator Loss: 0.6434577107429504, Discriminator Loss: 1.6570206880569458\n",
            "Epoch: 710, Generator Loss: 0.6412639617919922, Discriminator Loss: 1.6884324550628662\n",
            "Epoch: 711, Generator Loss: 0.6441124677658081, Discriminator Loss: 1.7134150266647339\n",
            "Epoch: 712, Generator Loss: 0.6473022699356079, Discriminator Loss: 1.6906318664550781\n",
            "Epoch: 713, Generator Loss: 0.6435247659683228, Discriminator Loss: 1.6807698011398315\n",
            "Epoch: 714, Generator Loss: 0.6436974406242371, Discriminator Loss: 1.7165021896362305\n",
            "Epoch: 715, Generator Loss: 0.6473265886306763, Discriminator Loss: 1.7013685703277588\n",
            "Epoch: 716, Generator Loss: 0.6397422552108765, Discriminator Loss: 1.7117912769317627\n",
            "Epoch: 717, Generator Loss: 0.6453207731246948, Discriminator Loss: 1.6846321821212769\n",
            "Epoch: 718, Generator Loss: 0.6398537755012512, Discriminator Loss: 1.7061866521835327\n",
            "Epoch: 719, Generator Loss: 0.6508920788764954, Discriminator Loss: 1.6802626848220825\n",
            "Epoch: 720, Generator Loss: 0.6444445848464966, Discriminator Loss: 1.7013481855392456\n",
            "Epoch: 721, Generator Loss: 0.6505058407783508, Discriminator Loss: 1.716732144355774\n",
            "Epoch: 722, Generator Loss: 0.6473464965820312, Discriminator Loss: 1.7014559507369995\n",
            "Epoch: 723, Generator Loss: 0.6448990702629089, Discriminator Loss: 1.6986925601959229\n",
            "Epoch: 724, Generator Loss: 0.6504625678062439, Discriminator Loss: 1.6873323917388916\n",
            "Epoch: 725, Generator Loss: 0.6497464179992676, Discriminator Loss: 1.6949042081832886\n",
            "Epoch: 726, Generator Loss: 0.6522478461265564, Discriminator Loss: 1.6927471160888672\n",
            "Epoch: 727, Generator Loss: 0.6479756236076355, Discriminator Loss: 1.6814329624176025\n",
            "Epoch: 728, Generator Loss: 0.6464875340461731, Discriminator Loss: 1.6981178522109985\n",
            "Epoch: 729, Generator Loss: 0.6415838599205017, Discriminator Loss: 1.6870149374008179\n",
            "Epoch: 730, Generator Loss: 0.6426458954811096, Discriminator Loss: 1.6779733896255493\n",
            "Epoch: 731, Generator Loss: 0.64557945728302, Discriminator Loss: 1.6843950748443604\n",
            "Epoch: 732, Generator Loss: 0.6513968706130981, Discriminator Loss: 1.6686490774154663\n",
            "Epoch: 733, Generator Loss: 0.6459425091743469, Discriminator Loss: 1.7219676971435547\n",
            "Epoch: 734, Generator Loss: 0.6419215798377991, Discriminator Loss: 1.6957749128341675\n",
            "Epoch: 735, Generator Loss: 0.6463278532028198, Discriminator Loss: 1.6745575666427612\n",
            "Epoch: 736, Generator Loss: 0.6488265991210938, Discriminator Loss: 1.7147865295410156\n",
            "Epoch: 737, Generator Loss: 0.6525413393974304, Discriminator Loss: 1.6956892013549805\n",
            "Epoch: 738, Generator Loss: 0.6470385193824768, Discriminator Loss: 1.7312358617782593\n",
            "Epoch: 739, Generator Loss: 0.6473317742347717, Discriminator Loss: 1.6918449401855469\n",
            "Epoch: 740, Generator Loss: 0.6487342119216919, Discriminator Loss: 1.6814501285552979\n",
            "Epoch: 741, Generator Loss: 0.6435590982437134, Discriminator Loss: 1.6876095533370972\n",
            "Epoch: 742, Generator Loss: 0.6395849585533142, Discriminator Loss: 1.699951410293579\n",
            "Epoch: 743, Generator Loss: 0.644643247127533, Discriminator Loss: 1.6984503269195557\n",
            "Epoch: 744, Generator Loss: 0.6471146941184998, Discriminator Loss: 1.712123990058899\n",
            "Epoch: 745, Generator Loss: 0.6470648050308228, Discriminator Loss: 1.671948790550232\n",
            "Epoch: 746, Generator Loss: 0.6399591565132141, Discriminator Loss: 1.68153977394104\n",
            "Epoch: 747, Generator Loss: 0.6464433073997498, Discriminator Loss: 1.7010051012039185\n",
            "Epoch: 748, Generator Loss: 0.6498083472251892, Discriminator Loss: 1.701948881149292\n",
            "Epoch: 749, Generator Loss: 0.6479542851448059, Discriminator Loss: 1.6838973760604858\n",
            "Epoch: 750, Generator Loss: 0.64307701587677, Discriminator Loss: 1.6997483968734741\n",
            "Epoch: 751, Generator Loss: 0.6515929102897644, Discriminator Loss: 1.6954702138900757\n",
            "Epoch: 752, Generator Loss: 0.6460015177726746, Discriminator Loss: 1.706880807876587\n",
            "Epoch: 753, Generator Loss: 0.6415852904319763, Discriminator Loss: 1.68752920627594\n",
            "Epoch: 754, Generator Loss: 0.6494743227958679, Discriminator Loss: 1.6795198917388916\n",
            "Epoch: 755, Generator Loss: 0.6482347846031189, Discriminator Loss: 1.7042187452316284\n",
            "Epoch: 756, Generator Loss: 0.6443434953689575, Discriminator Loss: 1.6941226720809937\n",
            "Epoch: 757, Generator Loss: 0.6454507112503052, Discriminator Loss: 1.6864975690841675\n",
            "Epoch: 758, Generator Loss: 0.643109917640686, Discriminator Loss: 1.6671899557113647\n",
            "Epoch: 759, Generator Loss: 0.6529294848442078, Discriminator Loss: 1.6861461400985718\n",
            "Epoch: 760, Generator Loss: 0.6438174247741699, Discriminator Loss: 1.70931875705719\n",
            "Epoch: 761, Generator Loss: 0.6582310795783997, Discriminator Loss: 1.654891014099121\n",
            "Epoch: 762, Generator Loss: 0.6489982604980469, Discriminator Loss: 1.6764832735061646\n",
            "Epoch: 763, Generator Loss: 0.6463857293128967, Discriminator Loss: 1.6900039911270142\n",
            "Epoch: 764, Generator Loss: 0.6573041677474976, Discriminator Loss: 1.6679399013519287\n",
            "Epoch: 765, Generator Loss: 0.6482601761817932, Discriminator Loss: 1.6725728511810303\n",
            "Epoch: 766, Generator Loss: 0.6458998918533325, Discriminator Loss: 1.6830204725265503\n",
            "Epoch: 767, Generator Loss: 0.6491058468818665, Discriminator Loss: 1.676790475845337\n",
            "Epoch: 768, Generator Loss: 0.6470144987106323, Discriminator Loss: 1.6808502674102783\n",
            "Epoch: 769, Generator Loss: 0.642812192440033, Discriminator Loss: 1.6906501054763794\n",
            "Epoch: 770, Generator Loss: 0.6365771889686584, Discriminator Loss: 1.6672942638397217\n",
            "Epoch: 771, Generator Loss: 0.6453655362129211, Discriminator Loss: 1.6843831539154053\n",
            "Epoch: 772, Generator Loss: 0.6340989470481873, Discriminator Loss: 1.6859967708587646\n",
            "Epoch: 773, Generator Loss: 0.6429591178894043, Discriminator Loss: 1.722176194190979\n",
            "Epoch: 774, Generator Loss: 0.6477354764938354, Discriminator Loss: 1.6444982290267944\n",
            "Epoch: 775, Generator Loss: 0.6292028427124023, Discriminator Loss: 1.6534775495529175\n",
            "Epoch: 776, Generator Loss: 0.6249591708183289, Discriminator Loss: 1.715687870979309\n",
            "Epoch: 777, Generator Loss: 0.6299771070480347, Discriminator Loss: 1.6845495700836182\n",
            "Epoch: 778, Generator Loss: 0.6275840401649475, Discriminator Loss: 1.7489240169525146\n",
            "Epoch: 779, Generator Loss: 0.6371291875839233, Discriminator Loss: 1.7346246242523193\n",
            "Epoch: 780, Generator Loss: 0.6447266340255737, Discriminator Loss: 1.6902333498001099\n",
            "Epoch: 781, Generator Loss: 0.6300956010818481, Discriminator Loss: 1.667650580406189\n",
            "Epoch: 782, Generator Loss: 0.6396809220314026, Discriminator Loss: 1.658516764640808\n",
            "Epoch: 783, Generator Loss: 0.6314967274665833, Discriminator Loss: 1.71120285987854\n",
            "Epoch: 784, Generator Loss: 0.6397515535354614, Discriminator Loss: 1.6867932081222534\n",
            "Epoch: 785, Generator Loss: 0.642111599445343, Discriminator Loss: 1.714389443397522\n",
            "Epoch: 786, Generator Loss: 0.6434993743896484, Discriminator Loss: 1.6609569787979126\n",
            "Epoch: 787, Generator Loss: 0.6469904780387878, Discriminator Loss: 1.738085389137268\n",
            "Epoch: 788, Generator Loss: 0.6392697691917419, Discriminator Loss: 1.7076529264450073\n",
            "Epoch: 789, Generator Loss: 0.6459762454032898, Discriminator Loss: 1.6899471282958984\n",
            "Epoch: 790, Generator Loss: 0.6465340852737427, Discriminator Loss: 1.688905119895935\n",
            "Epoch: 791, Generator Loss: 0.6471802592277527, Discriminator Loss: 1.6884241104125977\n",
            "Epoch: 792, Generator Loss: 0.6433486938476562, Discriminator Loss: 1.6890718936920166\n",
            "Epoch: 793, Generator Loss: 0.6495346426963806, Discriminator Loss: 1.6874850988388062\n",
            "Epoch: 794, Generator Loss: 0.6457193493843079, Discriminator Loss: 1.701111912727356\n",
            "Epoch: 795, Generator Loss: 0.646174967288971, Discriminator Loss: 1.6698459386825562\n",
            "Epoch: 796, Generator Loss: 0.644953727722168, Discriminator Loss: 1.7134424448013306\n",
            "Epoch: 797, Generator Loss: 0.6456112861633301, Discriminator Loss: 1.6563189029693604\n",
            "Epoch: 798, Generator Loss: 0.6462131142616272, Discriminator Loss: 1.6736215353012085\n",
            "Epoch: 799, Generator Loss: 0.6474617719650269, Discriminator Loss: 1.6953423023223877\n",
            "Epoch: 800, Generator Loss: 0.6444851160049438, Discriminator Loss: 1.708457112312317\n",
            "Epoch: 801, Generator Loss: 0.6504328846931458, Discriminator Loss: 1.6993917226791382\n",
            "Epoch: 802, Generator Loss: 0.6523750424385071, Discriminator Loss: 1.6880384683609009\n",
            "Epoch: 803, Generator Loss: 0.6532325744628906, Discriminator Loss: 1.6493464708328247\n",
            "Epoch: 804, Generator Loss: 0.6428362727165222, Discriminator Loss: 1.6698048114776611\n",
            "Epoch: 805, Generator Loss: 0.6448794007301331, Discriminator Loss: 1.6639974117279053\n",
            "Epoch: 806, Generator Loss: 0.6373741030693054, Discriminator Loss: 1.698905348777771\n",
            "Epoch: 807, Generator Loss: 0.645259439945221, Discriminator Loss: 1.6668585538864136\n",
            "Epoch: 808, Generator Loss: 0.6438730955123901, Discriminator Loss: 1.6846847534179688\n",
            "Epoch: 809, Generator Loss: 0.6420916318893433, Discriminator Loss: 1.6988307237625122\n",
            "Epoch: 810, Generator Loss: 0.6509504914283752, Discriminator Loss: 1.658785104751587\n",
            "Epoch: 811, Generator Loss: 0.6368170976638794, Discriminator Loss: 1.6991512775421143\n",
            "Epoch: 812, Generator Loss: 0.6453040838241577, Discriminator Loss: 1.651410698890686\n",
            "Epoch: 813, Generator Loss: 0.6459422707557678, Discriminator Loss: 1.7235463857650757\n",
            "Epoch: 814, Generator Loss: 0.6482740044593811, Discriminator Loss: 1.6971060037612915\n",
            "Epoch: 815, Generator Loss: 0.6415413618087769, Discriminator Loss: 1.7134684324264526\n",
            "Epoch: 816, Generator Loss: 0.6521839499473572, Discriminator Loss: 1.700764536857605\n",
            "Epoch: 817, Generator Loss: 0.6512317657470703, Discriminator Loss: 1.682900071144104\n",
            "Epoch: 818, Generator Loss: 0.6461367011070251, Discriminator Loss: 1.6639320850372314\n",
            "Epoch: 819, Generator Loss: 0.647760808467865, Discriminator Loss: 1.6698148250579834\n",
            "Epoch: 820, Generator Loss: 0.6385040283203125, Discriminator Loss: 1.689350962638855\n",
            "Epoch: 821, Generator Loss: 0.6383013129234314, Discriminator Loss: 1.7015478610992432\n",
            "Epoch: 822, Generator Loss: 0.6478620767593384, Discriminator Loss: 1.6936986446380615\n",
            "Epoch: 823, Generator Loss: 0.6455589532852173, Discriminator Loss: 1.6838206052780151\n",
            "Epoch: 824, Generator Loss: 0.6472979187965393, Discriminator Loss: 1.6719515323638916\n",
            "Epoch: 825, Generator Loss: 0.6509844660758972, Discriminator Loss: 1.6869994401931763\n",
            "Epoch: 826, Generator Loss: 0.641825795173645, Discriminator Loss: 1.6944301128387451\n",
            "Epoch: 827, Generator Loss: 0.6347590088844299, Discriminator Loss: 1.689614176750183\n",
            "Epoch: 828, Generator Loss: 0.6492579579353333, Discriminator Loss: 1.6682952642440796\n",
            "Epoch: 829, Generator Loss: 0.6344814300537109, Discriminator Loss: 1.6861628293991089\n",
            "Epoch: 830, Generator Loss: 0.6350279450416565, Discriminator Loss: 1.699157953262329\n",
            "Epoch: 831, Generator Loss: 0.6338053941726685, Discriminator Loss: 1.69450843334198\n",
            "Epoch: 832, Generator Loss: 0.6286574602127075, Discriminator Loss: 1.6767548322677612\n",
            "Epoch: 833, Generator Loss: 0.6234298348426819, Discriminator Loss: 1.665231466293335\n",
            "Epoch: 834, Generator Loss: 0.6288501024246216, Discriminator Loss: 1.7117830514907837\n",
            "Epoch: 835, Generator Loss: 0.6353852152824402, Discriminator Loss: 1.6733992099761963\n",
            "Epoch: 836, Generator Loss: 0.6249300837516785, Discriminator Loss: 1.6993669271469116\n",
            "Epoch: 837, Generator Loss: 0.6322340965270996, Discriminator Loss: 1.6541074514389038\n",
            "Epoch: 838, Generator Loss: 0.6432577967643738, Discriminator Loss: 1.732157826423645\n",
            "Epoch: 839, Generator Loss: 0.6342883706092834, Discriminator Loss: 1.6698881387710571\n",
            "Epoch: 840, Generator Loss: 0.6335920691490173, Discriminator Loss: 1.6787515878677368\n",
            "Epoch: 841, Generator Loss: 0.6468864679336548, Discriminator Loss: 1.6998026371002197\n",
            "Epoch: 842, Generator Loss: 0.6474236249923706, Discriminator Loss: 1.701973795890808\n",
            "Epoch: 843, Generator Loss: 0.646966278553009, Discriminator Loss: 1.6718199253082275\n",
            "Epoch: 844, Generator Loss: 0.6464241147041321, Discriminator Loss: 1.6771689653396606\n",
            "Epoch: 845, Generator Loss: 0.6436119079589844, Discriminator Loss: 1.6836856603622437\n",
            "Epoch: 846, Generator Loss: 0.6447038650512695, Discriminator Loss: 1.6657428741455078\n",
            "Epoch: 847, Generator Loss: 0.643575131893158, Discriminator Loss: 1.6953004598617554\n",
            "Epoch: 848, Generator Loss: 0.6439548134803772, Discriminator Loss: 1.6941919326782227\n",
            "Epoch: 849, Generator Loss: 0.6444023251533508, Discriminator Loss: 1.6473432779312134\n",
            "Epoch: 850, Generator Loss: 0.6435033679008484, Discriminator Loss: 1.6676075458526611\n",
            "Epoch: 851, Generator Loss: 0.6410353779792786, Discriminator Loss: 1.6864688396453857\n",
            "Epoch: 852, Generator Loss: 0.6380628943443298, Discriminator Loss: 1.7044686079025269\n",
            "Epoch: 853, Generator Loss: 0.6416370272636414, Discriminator Loss: 1.6941779851913452\n",
            "Epoch: 854, Generator Loss: 0.637104868888855, Discriminator Loss: 1.692208170890808\n",
            "Epoch: 855, Generator Loss: 0.6373717188835144, Discriminator Loss: 1.6706877946853638\n",
            "Epoch: 856, Generator Loss: 0.648043692111969, Discriminator Loss: 1.6834051609039307\n",
            "Epoch: 857, Generator Loss: 0.6430210471153259, Discriminator Loss: 1.6871057748794556\n",
            "Epoch: 858, Generator Loss: 0.6490249633789062, Discriminator Loss: 1.6574829816818237\n",
            "Epoch: 859, Generator Loss: 0.6438630819320679, Discriminator Loss: 1.6763256788253784\n",
            "Epoch: 860, Generator Loss: 0.6436120867729187, Discriminator Loss: 1.6939095258712769\n",
            "Epoch: 861, Generator Loss: 0.6371508836746216, Discriminator Loss: 1.6697616577148438\n",
            "Epoch: 862, Generator Loss: 0.6344031691551208, Discriminator Loss: 1.7212742567062378\n",
            "Epoch: 863, Generator Loss: 0.6486796140670776, Discriminator Loss: 1.6936308145523071\n",
            "Epoch: 864, Generator Loss: 0.6404510140419006, Discriminator Loss: 1.6690387725830078\n",
            "Epoch: 865, Generator Loss: 0.6404638886451721, Discriminator Loss: 1.6841017007827759\n",
            "Epoch: 866, Generator Loss: 0.6466824412345886, Discriminator Loss: 1.6654150485992432\n",
            "Epoch: 867, Generator Loss: 0.6479066014289856, Discriminator Loss: 1.7022093534469604\n",
            "Epoch: 868, Generator Loss: 0.6469464898109436, Discriminator Loss: 1.7041388750076294\n",
            "Epoch: 869, Generator Loss: 0.643588125705719, Discriminator Loss: 1.6739662885665894\n",
            "Epoch: 870, Generator Loss: 0.6456783413887024, Discriminator Loss: 1.6774803400039673\n",
            "Epoch: 871, Generator Loss: 0.6498838067054749, Discriminator Loss: 1.6766918897628784\n",
            "Epoch: 872, Generator Loss: 0.6481409072875977, Discriminator Loss: 1.6888803243637085\n",
            "Epoch: 873, Generator Loss: 0.6471696496009827, Discriminator Loss: 1.6902731657028198\n",
            "Epoch: 874, Generator Loss: 0.653039276599884, Discriminator Loss: 1.6859287023544312\n",
            "Epoch: 875, Generator Loss: 0.647546112537384, Discriminator Loss: 1.6752945184707642\n",
            "Epoch: 876, Generator Loss: 0.6455156207084656, Discriminator Loss: 1.6606072187423706\n",
            "Epoch: 877, Generator Loss: 0.6521846055984497, Discriminator Loss: 1.689477562904358\n",
            "Epoch: 878, Generator Loss: 0.6519935727119446, Discriminator Loss: 1.6982775926589966\n",
            "Epoch: 879, Generator Loss: 0.6572600603103638, Discriminator Loss: 1.647428274154663\n",
            "Epoch: 880, Generator Loss: 0.6559563279151917, Discriminator Loss: 1.6672943830490112\n",
            "Epoch: 881, Generator Loss: 0.6527286171913147, Discriminator Loss: 1.6454914808273315\n",
            "Epoch: 882, Generator Loss: 0.639464795589447, Discriminator Loss: 1.651898980140686\n",
            "Epoch: 883, Generator Loss: 0.6389814019203186, Discriminator Loss: 1.6733735799789429\n",
            "Epoch: 884, Generator Loss: 0.6412025690078735, Discriminator Loss: 1.6632921695709229\n",
            "Epoch: 885, Generator Loss: 0.6392243504524231, Discriminator Loss: 1.7342363595962524\n",
            "Epoch: 886, Generator Loss: 0.6374303102493286, Discriminator Loss: 1.690898060798645\n",
            "Epoch: 887, Generator Loss: 0.6308087110519409, Discriminator Loss: 1.6720257997512817\n",
            "Epoch: 888, Generator Loss: 0.6332067847251892, Discriminator Loss: 1.6905430555343628\n",
            "Epoch: 889, Generator Loss: 0.6326988935470581, Discriminator Loss: 1.657012701034546\n",
            "Epoch: 890, Generator Loss: 0.6267148852348328, Discriminator Loss: 1.6859959363937378\n",
            "Epoch: 891, Generator Loss: 0.623721718788147, Discriminator Loss: 1.6886694431304932\n",
            "Epoch: 892, Generator Loss: 0.6213251352310181, Discriminator Loss: 1.7302616834640503\n",
            "Epoch: 893, Generator Loss: 0.635186493396759, Discriminator Loss: 1.7061434984207153\n",
            "Epoch: 894, Generator Loss: 0.6342921257019043, Discriminator Loss: 1.6842916011810303\n",
            "Epoch: 895, Generator Loss: 0.6303486824035645, Discriminator Loss: 1.6999636888504028\n",
            "Epoch: 896, Generator Loss: 0.6277429461479187, Discriminator Loss: 1.6979912519454956\n",
            "Epoch: 897, Generator Loss: 0.6398482322692871, Discriminator Loss: 1.6644891500473022\n",
            "Epoch: 898, Generator Loss: 0.6475699543952942, Discriminator Loss: 1.6782225370407104\n",
            "Epoch: 899, Generator Loss: 0.6417468190193176, Discriminator Loss: 1.6812816858291626\n",
            "Epoch: 900, Generator Loss: 0.6446961164474487, Discriminator Loss: 1.6953086853027344\n",
            "Epoch: 901, Generator Loss: 0.6461649537086487, Discriminator Loss: 1.6835520267486572\n",
            "Epoch: 902, Generator Loss: 0.6407870054244995, Discriminator Loss: 1.6949354410171509\n",
            "Epoch: 903, Generator Loss: 0.6385378241539001, Discriminator Loss: 1.6903635263442993\n",
            "Epoch: 904, Generator Loss: 0.6442316174507141, Discriminator Loss: 1.6634455919265747\n",
            "Epoch: 905, Generator Loss: 0.6462773680686951, Discriminator Loss: 1.663248062133789\n",
            "Epoch: 906, Generator Loss: 0.6412237882614136, Discriminator Loss: 1.7047537565231323\n",
            "Epoch: 907, Generator Loss: 0.6442143321037292, Discriminator Loss: 1.6794270277023315\n",
            "Epoch: 908, Generator Loss: 0.6510756611824036, Discriminator Loss: 1.7028783559799194\n",
            "Epoch: 909, Generator Loss: 0.6463631391525269, Discriminator Loss: 1.7337805032730103\n",
            "Epoch: 910, Generator Loss: 0.6431803703308105, Discriminator Loss: 1.6838675737380981\n",
            "Epoch: 911, Generator Loss: 0.6543909907341003, Discriminator Loss: 1.6219362020492554\n",
            "Epoch: 912, Generator Loss: 0.6420890688896179, Discriminator Loss: 1.6907455921173096\n",
            "Epoch: 913, Generator Loss: 0.6412531733512878, Discriminator Loss: 1.6703691482543945\n",
            "Epoch: 914, Generator Loss: 0.6454876661300659, Discriminator Loss: 1.6610437631607056\n",
            "Epoch: 915, Generator Loss: 0.6446781158447266, Discriminator Loss: 1.6957942247390747\n",
            "Epoch: 916, Generator Loss: 0.641388475894928, Discriminator Loss: 1.6840860843658447\n",
            "Epoch: 917, Generator Loss: 0.6512919664382935, Discriminator Loss: 1.7019147872924805\n",
            "Epoch: 918, Generator Loss: 0.6423724293708801, Discriminator Loss: 1.6586627960205078\n",
            "Epoch: 919, Generator Loss: 0.6410511136054993, Discriminator Loss: 1.7042564153671265\n",
            "Epoch: 920, Generator Loss: 0.6445048451423645, Discriminator Loss: 1.665042519569397\n",
            "Epoch: 921, Generator Loss: 0.6416832804679871, Discriminator Loss: 1.66871178150177\n",
            "Epoch: 922, Generator Loss: 0.6474618911743164, Discriminator Loss: 1.6780444383621216\n",
            "Epoch: 923, Generator Loss: 0.6473535895347595, Discriminator Loss: 1.6927995681762695\n",
            "Epoch: 924, Generator Loss: 0.6501808166503906, Discriminator Loss: 1.6669687032699585\n",
            "Epoch: 925, Generator Loss: 0.6476714015007019, Discriminator Loss: 1.6908963918685913\n",
            "Epoch: 926, Generator Loss: 0.648737907409668, Discriminator Loss: 1.6695107221603394\n",
            "Epoch: 927, Generator Loss: 0.6412931084632874, Discriminator Loss: 1.7100414037704468\n",
            "Epoch: 928, Generator Loss: 0.6543822884559631, Discriminator Loss: 1.6781543493270874\n",
            "Epoch: 929, Generator Loss: 0.6556010842323303, Discriminator Loss: 1.6547144651412964\n",
            "Epoch: 930, Generator Loss: 0.6472338438034058, Discriminator Loss: 1.6981847286224365\n",
            "Epoch: 931, Generator Loss: 0.6545968651771545, Discriminator Loss: 1.647918462753296\n",
            "Epoch: 932, Generator Loss: 0.6487293243408203, Discriminator Loss: 1.6687551736831665\n",
            "Epoch: 933, Generator Loss: 0.6517857909202576, Discriminator Loss: 1.687915563583374\n",
            "Epoch: 934, Generator Loss: 0.6536468863487244, Discriminator Loss: 1.6828334331512451\n",
            "Epoch: 935, Generator Loss: 0.6577009558677673, Discriminator Loss: 1.654845118522644\n",
            "Epoch: 936, Generator Loss: 0.652551531791687, Discriminator Loss: 1.6769635677337646\n",
            "Epoch: 937, Generator Loss: 0.6545099020004272, Discriminator Loss: 1.6609342098236084\n",
            "Epoch: 938, Generator Loss: 0.6504921913146973, Discriminator Loss: 1.6926213502883911\n",
            "Epoch: 939, Generator Loss: 0.649067223072052, Discriminator Loss: 1.672222375869751\n",
            "Epoch: 940, Generator Loss: 0.648470401763916, Discriminator Loss: 1.641292929649353\n",
            "Epoch: 941, Generator Loss: 0.6478667259216309, Discriminator Loss: 1.672516942024231\n",
            "Epoch: 942, Generator Loss: 0.6499871015548706, Discriminator Loss: 1.6764332056045532\n",
            "Epoch: 943, Generator Loss: 0.6433542966842651, Discriminator Loss: 1.6407592296600342\n",
            "Epoch: 944, Generator Loss: 0.6346005201339722, Discriminator Loss: 1.6843923330307007\n",
            "Epoch: 945, Generator Loss: 0.6402190327644348, Discriminator Loss: 1.6881657838821411\n",
            "Epoch: 946, Generator Loss: 0.6361384391784668, Discriminator Loss: 1.660211443901062\n",
            "Epoch: 947, Generator Loss: 0.6307412385940552, Discriminator Loss: 1.6766997575759888\n",
            "Epoch: 948, Generator Loss: 0.6394586563110352, Discriminator Loss: 1.640747308731079\n",
            "Epoch: 949, Generator Loss: 0.6276842951774597, Discriminator Loss: 1.6923192739486694\n",
            "Epoch: 950, Generator Loss: 0.6381983757019043, Discriminator Loss: 1.6670491695404053\n",
            "Epoch: 951, Generator Loss: 0.6308623552322388, Discriminator Loss: 1.6977864503860474\n",
            "Epoch: 952, Generator Loss: 0.6386958360671997, Discriminator Loss: 1.6770102977752686\n",
            "Epoch: 953, Generator Loss: 0.6379613876342773, Discriminator Loss: 1.6999932527542114\n",
            "Epoch: 954, Generator Loss: 0.6332514882087708, Discriminator Loss: 1.6762632131576538\n",
            "Epoch: 955, Generator Loss: 0.6324372291564941, Discriminator Loss: 1.7024307250976562\n",
            "Epoch: 956, Generator Loss: 0.6302662491798401, Discriminator Loss: 1.6605632305145264\n",
            "Epoch: 957, Generator Loss: 0.6169535517692566, Discriminator Loss: 1.6973581314086914\n",
            "Epoch: 958, Generator Loss: 0.6266957521438599, Discriminator Loss: 1.7076683044433594\n",
            "Epoch: 959, Generator Loss: 0.6308096051216125, Discriminator Loss: 1.6312190294265747\n",
            "Epoch: 960, Generator Loss: 0.6222925186157227, Discriminator Loss: 1.6966724395751953\n",
            "Epoch: 961, Generator Loss: 0.6432718634605408, Discriminator Loss: 1.6772640943527222\n",
            "Epoch: 962, Generator Loss: 0.645840585231781, Discriminator Loss: 1.679280400276184\n",
            "Epoch: 963, Generator Loss: 0.6496676206588745, Discriminator Loss: 1.6930577754974365\n",
            "Epoch: 964, Generator Loss: 0.644074022769928, Discriminator Loss: 1.6629143953323364\n",
            "Epoch: 965, Generator Loss: 0.640508234500885, Discriminator Loss: 1.7152215242385864\n",
            "Epoch: 966, Generator Loss: 0.642883837223053, Discriminator Loss: 1.6762198209762573\n",
            "Epoch: 967, Generator Loss: 0.6453239321708679, Discriminator Loss: 1.6659414768218994\n",
            "Epoch: 968, Generator Loss: 0.6519098877906799, Discriminator Loss: 1.6596927642822266\n",
            "Epoch: 969, Generator Loss: 0.6489405035972595, Discriminator Loss: 1.7079856395721436\n",
            "Epoch: 970, Generator Loss: 0.6518526077270508, Discriminator Loss: 1.6738616228103638\n",
            "Epoch: 971, Generator Loss: 0.6478977203369141, Discriminator Loss: 1.6705843210220337\n",
            "Epoch: 972, Generator Loss: 0.6499918103218079, Discriminator Loss: 1.6667543649673462\n",
            "Epoch: 973, Generator Loss: 0.6534369587898254, Discriminator Loss: 1.690002679824829\n",
            "Epoch: 974, Generator Loss: 0.6465455889701843, Discriminator Loss: 1.6816914081573486\n",
            "Epoch: 975, Generator Loss: 0.6541441679000854, Discriminator Loss: 1.6981830596923828\n",
            "Epoch: 976, Generator Loss: 0.65855473279953, Discriminator Loss: 1.6607091426849365\n",
            "Epoch: 977, Generator Loss: 0.649769127368927, Discriminator Loss: 1.7022501230239868\n",
            "Epoch: 978, Generator Loss: 0.6493767499923706, Discriminator Loss: 1.681581735610962\n",
            "Epoch: 979, Generator Loss: 0.6476168632507324, Discriminator Loss: 1.6732395887374878\n",
            "Epoch: 980, Generator Loss: 0.6498528718948364, Discriminator Loss: 1.6637760400772095\n",
            "Epoch: 981, Generator Loss: 0.6434953808784485, Discriminator Loss: 1.7017279863357544\n",
            "Epoch: 982, Generator Loss: 0.6432884335517883, Discriminator Loss: 1.6733802556991577\n",
            "Epoch: 983, Generator Loss: 0.651581346988678, Discriminator Loss: 1.670411229133606\n",
            "Epoch: 984, Generator Loss: 0.6481914520263672, Discriminator Loss: 1.677817702293396\n",
            "Epoch: 985, Generator Loss: 0.6487250924110413, Discriminator Loss: 1.7028416395187378\n",
            "Epoch: 986, Generator Loss: 0.6470733284950256, Discriminator Loss: 1.7148908376693726\n",
            "Epoch: 987, Generator Loss: 0.6525593996047974, Discriminator Loss: 1.6931438446044922\n",
            "Epoch: 988, Generator Loss: 0.6482371687889099, Discriminator Loss: 1.6736136674880981\n",
            "Epoch: 989, Generator Loss: 0.6463660597801208, Discriminator Loss: 1.6736193895339966\n",
            "Epoch: 990, Generator Loss: 0.6433032155036926, Discriminator Loss: 1.6605441570281982\n",
            "Epoch: 991, Generator Loss: 0.6489080786705017, Discriminator Loss: 1.6890615224838257\n",
            "Epoch: 992, Generator Loss: 0.6557827591896057, Discriminator Loss: 1.6264772415161133\n",
            "Epoch: 993, Generator Loss: 0.6466866135597229, Discriminator Loss: 1.6473532915115356\n",
            "Epoch: 994, Generator Loss: 0.6439887285232544, Discriminator Loss: 1.6728202104568481\n",
            "Epoch: 995, Generator Loss: 0.6452072262763977, Discriminator Loss: 1.6788474321365356\n",
            "Epoch: 996, Generator Loss: 0.6500336527824402, Discriminator Loss: 1.664977788925171\n",
            "Epoch: 997, Generator Loss: 0.6534667611122131, Discriminator Loss: 1.659953236579895\n",
            "Epoch: 998, Generator Loss: 0.6533231735229492, Discriminator Loss: 1.6543101072311401\n",
            "Epoch: 999, Generator Loss: 0.6523329615592957, Discriminator Loss: 1.654296636581421\n",
            "Epoch: 1000, Generator Loss: 0.6545578837394714, Discriminator Loss: 1.7095508575439453\n",
            "Epoch: 1001, Generator Loss: 0.6497458219528198, Discriminator Loss: 1.6904691457748413\n",
            "Epoch: 1002, Generator Loss: 0.650953471660614, Discriminator Loss: 1.6595782041549683\n",
            "Epoch: 1003, Generator Loss: 0.6458336710929871, Discriminator Loss: 1.7060775756835938\n",
            "Epoch: 1004, Generator Loss: 0.6400992274284363, Discriminator Loss: 1.680001139640808\n",
            "Epoch: 1005, Generator Loss: 0.64464271068573, Discriminator Loss: 1.6767663955688477\n",
            "Epoch: 1006, Generator Loss: 0.6469523310661316, Discriminator Loss: 1.6745768785476685\n",
            "Epoch: 1007, Generator Loss: 0.6544378399848938, Discriminator Loss: 1.6709190607070923\n",
            "Epoch: 1008, Generator Loss: 0.6440057158470154, Discriminator Loss: 1.6830092668533325\n",
            "Epoch: 1009, Generator Loss: 0.647577166557312, Discriminator Loss: 1.6747910976409912\n",
            "Epoch: 1010, Generator Loss: 0.653142511844635, Discriminator Loss: 1.6775071620941162\n",
            "Epoch: 1011, Generator Loss: 0.6564585566520691, Discriminator Loss: 1.65201735496521\n",
            "Epoch: 1012, Generator Loss: 0.6501021385192871, Discriminator Loss: 1.642349123954773\n",
            "Epoch: 1013, Generator Loss: 0.6466683149337769, Discriminator Loss: 1.6583834886550903\n",
            "Epoch: 1014, Generator Loss: 0.6559926271438599, Discriminator Loss: 1.6642245054244995\n",
            "Epoch: 1015, Generator Loss: 0.6504499316215515, Discriminator Loss: 1.6621639728546143\n",
            "Epoch: 1016, Generator Loss: 0.6472968459129333, Discriminator Loss: 1.689492106437683\n",
            "Epoch: 1017, Generator Loss: 0.6458834409713745, Discriminator Loss: 1.6840795278549194\n",
            "Epoch: 1018, Generator Loss: 0.6397076845169067, Discriminator Loss: 1.6834638118743896\n",
            "Epoch: 1019, Generator Loss: 0.6449151635169983, Discriminator Loss: 1.6779592037200928\n",
            "Epoch: 1020, Generator Loss: 0.6381171941757202, Discriminator Loss: 1.6874809265136719\n",
            "Epoch: 1021, Generator Loss: 0.6462339162826538, Discriminator Loss: 1.6918843984603882\n",
            "Epoch: 1022, Generator Loss: 0.6513643264770508, Discriminator Loss: 1.6519521474838257\n",
            "Epoch: 1023, Generator Loss: 0.6450982689857483, Discriminator Loss: 1.6609361171722412\n",
            "Epoch: 1024, Generator Loss: 0.6421942114830017, Discriminator Loss: 1.699310302734375\n",
            "Epoch: 1025, Generator Loss: 0.6420653462409973, Discriminator Loss: 1.6866520643234253\n",
            "Epoch: 1026, Generator Loss: 0.6413701772689819, Discriminator Loss: 1.6706316471099854\n",
            "Epoch: 1027, Generator Loss: 0.6494686007499695, Discriminator Loss: 1.6924301385879517\n",
            "Epoch: 1028, Generator Loss: 0.6410335898399353, Discriminator Loss: 1.6978062391281128\n",
            "Epoch: 1029, Generator Loss: 0.643086314201355, Discriminator Loss: 1.6981031894683838\n",
            "Epoch: 1030, Generator Loss: 0.6417050957679749, Discriminator Loss: 1.7032287120819092\n",
            "Epoch: 1031, Generator Loss: 0.6486301422119141, Discriminator Loss: 1.6808805465698242\n",
            "Epoch: 1032, Generator Loss: 0.6498674750328064, Discriminator Loss: 1.6256779432296753\n",
            "Epoch: 1033, Generator Loss: 0.6544308066368103, Discriminator Loss: 1.6783477067947388\n",
            "Epoch: 1034, Generator Loss: 0.6557580232620239, Discriminator Loss: 1.677991271018982\n",
            "Epoch: 1035, Generator Loss: 0.6516487002372742, Discriminator Loss: 1.6736118793487549\n",
            "Epoch: 1036, Generator Loss: 0.6417420506477356, Discriminator Loss: 1.6875171661376953\n",
            "Epoch: 1037, Generator Loss: 0.6517005562782288, Discriminator Loss: 1.679491639137268\n",
            "Epoch: 1038, Generator Loss: 0.6396104097366333, Discriminator Loss: 1.70478093624115\n",
            "Epoch: 1039, Generator Loss: 0.6575397849082947, Discriminator Loss: 1.703431487083435\n",
            "Epoch: 1040, Generator Loss: 0.6542254090309143, Discriminator Loss: 1.6966451406478882\n",
            "Epoch: 1041, Generator Loss: 0.6493098139762878, Discriminator Loss: 1.6668449640274048\n",
            "Epoch: 1042, Generator Loss: 0.6538373827934265, Discriminator Loss: 1.6854370832443237\n",
            "Epoch: 1043, Generator Loss: 0.6596593856811523, Discriminator Loss: 1.68960702419281\n",
            "Epoch: 1044, Generator Loss: 0.6567384004592896, Discriminator Loss: 1.673371434211731\n",
            "Epoch: 1045, Generator Loss: 0.6612784266471863, Discriminator Loss: 1.6388171911239624\n",
            "Epoch: 1046, Generator Loss: 0.6604262590408325, Discriminator Loss: 1.6596848964691162\n",
            "Epoch: 1047, Generator Loss: 0.6608721613883972, Discriminator Loss: 1.63776433467865\n",
            "Epoch: 1048, Generator Loss: 0.6553564071655273, Discriminator Loss: 1.666810393333435\n",
            "Epoch: 1049, Generator Loss: 0.6596790552139282, Discriminator Loss: 1.6629515886306763\n",
            "Epoch: 1050, Generator Loss: 0.6543067097663879, Discriminator Loss: 1.679915189743042\n",
            "Epoch: 1051, Generator Loss: 0.6595374345779419, Discriminator Loss: 1.6411610841751099\n",
            "Epoch: 1052, Generator Loss: 0.6456162333488464, Discriminator Loss: 1.6617794036865234\n",
            "Epoch: 1053, Generator Loss: 0.649627685546875, Discriminator Loss: 1.6748054027557373\n",
            "Epoch: 1054, Generator Loss: 0.6560127139091492, Discriminator Loss: 1.6866226196289062\n",
            "Epoch: 1055, Generator Loss: 0.651210367679596, Discriminator Loss: 1.6582897901535034\n",
            "Epoch: 1056, Generator Loss: 0.6525540351867676, Discriminator Loss: 1.7126176357269287\n",
            "Epoch: 1057, Generator Loss: 0.6584601402282715, Discriminator Loss: 1.7025349140167236\n",
            "Epoch: 1058, Generator Loss: 0.6519074440002441, Discriminator Loss: 1.6438398361206055\n",
            "Epoch: 1059, Generator Loss: 0.658030092716217, Discriminator Loss: 1.6537972688674927\n",
            "Epoch: 1060, Generator Loss: 0.6552554368972778, Discriminator Loss: 1.6838738918304443\n",
            "Epoch: 1061, Generator Loss: 0.653412401676178, Discriminator Loss: 1.6951009035110474\n",
            "Epoch: 1062, Generator Loss: 0.663392961025238, Discriminator Loss: 1.6950147151947021\n",
            "Epoch: 1063, Generator Loss: 0.6605915427207947, Discriminator Loss: 1.6563581228256226\n",
            "Epoch: 1064, Generator Loss: 0.6536003351211548, Discriminator Loss: 1.670927882194519\n",
            "Epoch: 1065, Generator Loss: 0.6617608666419983, Discriminator Loss: 1.6946213245391846\n",
            "Epoch: 1066, Generator Loss: 0.6633986234664917, Discriminator Loss: 1.6756203174591064\n",
            "Epoch: 1067, Generator Loss: 0.6540139317512512, Discriminator Loss: 1.6853199005126953\n",
            "Epoch: 1068, Generator Loss: 0.6534045934677124, Discriminator Loss: 1.6684468984603882\n",
            "Epoch: 1069, Generator Loss: 0.657817542552948, Discriminator Loss: 1.6768425703048706\n",
            "Epoch: 1070, Generator Loss: 0.6580556035041809, Discriminator Loss: 1.6692792177200317\n",
            "Epoch: 1071, Generator Loss: 0.6557177901268005, Discriminator Loss: 1.6701767444610596\n",
            "Epoch: 1072, Generator Loss: 0.6535516977310181, Discriminator Loss: 1.6437863111495972\n",
            "Epoch: 1073, Generator Loss: 0.6596497297286987, Discriminator Loss: 1.6531459093093872\n",
            "Epoch: 1074, Generator Loss: 0.6528570055961609, Discriminator Loss: 1.7048672437667847\n",
            "Epoch: 1075, Generator Loss: 0.6555608510971069, Discriminator Loss: 1.685302495956421\n",
            "Epoch: 1076, Generator Loss: 0.6572316288948059, Discriminator Loss: 1.6684659719467163\n",
            "Epoch: 1077, Generator Loss: 0.6580737233161926, Discriminator Loss: 1.656374216079712\n",
            "Epoch: 1078, Generator Loss: 0.6534423828125, Discriminator Loss: 1.6599832773208618\n",
            "Epoch: 1079, Generator Loss: 0.6485797762870789, Discriminator Loss: 1.65749192237854\n",
            "Epoch: 1080, Generator Loss: 0.6490947604179382, Discriminator Loss: 1.6746360063552856\n",
            "Epoch: 1081, Generator Loss: 0.6518474221229553, Discriminator Loss: 1.6597055196762085\n",
            "Epoch: 1082, Generator Loss: 0.6486597657203674, Discriminator Loss: 1.6124275922775269\n",
            "Epoch: 1083, Generator Loss: 0.650050699710846, Discriminator Loss: 1.6555671691894531\n",
            "Epoch: 1084, Generator Loss: 0.6465370059013367, Discriminator Loss: 1.6436645984649658\n",
            "Epoch: 1085, Generator Loss: 0.645898163318634, Discriminator Loss: 1.6792100667953491\n",
            "Epoch: 1086, Generator Loss: 0.6509347558021545, Discriminator Loss: 1.6682502031326294\n",
            "Epoch: 1087, Generator Loss: 0.6523917317390442, Discriminator Loss: 1.6558376550674438\n",
            "Epoch: 1088, Generator Loss: 0.6546353101730347, Discriminator Loss: 1.6906274557113647\n",
            "Epoch: 1089, Generator Loss: 0.6522423624992371, Discriminator Loss: 1.6540333032608032\n",
            "Epoch: 1090, Generator Loss: 0.6391345858573914, Discriminator Loss: 1.6799956560134888\n",
            "Epoch: 1091, Generator Loss: 0.6562709212303162, Discriminator Loss: 1.682117223739624\n",
            "Epoch: 1092, Generator Loss: 0.6481834053993225, Discriminator Loss: 1.6360406875610352\n",
            "Epoch: 1093, Generator Loss: 0.642146646976471, Discriminator Loss: 1.6503450870513916\n",
            "Epoch: 1094, Generator Loss: 0.647358775138855, Discriminator Loss: 1.6672338247299194\n",
            "Epoch: 1095, Generator Loss: 0.6507096290588379, Discriminator Loss: 1.6704330444335938\n",
            "Epoch: 1096, Generator Loss: 0.6444815397262573, Discriminator Loss: 1.6645309925079346\n",
            "Epoch: 1097, Generator Loss: 0.6414772868156433, Discriminator Loss: 1.6722300052642822\n",
            "Epoch: 1098, Generator Loss: 0.6435736417770386, Discriminator Loss: 1.6665223836898804\n",
            "Epoch: 1099, Generator Loss: 0.6440314650535583, Discriminator Loss: 1.6776268482208252\n",
            "Epoch: 1100, Generator Loss: 0.6401309370994568, Discriminator Loss: 1.6745531558990479\n",
            "Epoch: 1101, Generator Loss: 0.6374984383583069, Discriminator Loss: 1.6788520812988281\n",
            "Epoch: 1102, Generator Loss: 0.6467112898826599, Discriminator Loss: 1.6662005186080933\n",
            "Epoch: 1103, Generator Loss: 0.6332675814628601, Discriminator Loss: 1.7037584781646729\n",
            "Epoch: 1104, Generator Loss: 0.6375776529312134, Discriminator Loss: 1.6291592121124268\n",
            "Epoch: 1105, Generator Loss: 0.6426326036453247, Discriminator Loss: 1.6530648469924927\n",
            "Epoch: 1106, Generator Loss: 0.6453632712364197, Discriminator Loss: 1.6917628049850464\n",
            "Epoch: 1107, Generator Loss: 0.6443458795547485, Discriminator Loss: 1.6347973346710205\n",
            "Epoch: 1108, Generator Loss: 0.6465269923210144, Discriminator Loss: 1.650148630142212\n",
            "Epoch: 1109, Generator Loss: 0.6454489827156067, Discriminator Loss: 1.6628717184066772\n",
            "Epoch: 1110, Generator Loss: 0.6387604475021362, Discriminator Loss: 1.6956311464309692\n",
            "Epoch: 1111, Generator Loss: 0.6410619020462036, Discriminator Loss: 1.6733248233795166\n",
            "Epoch: 1112, Generator Loss: 0.6399866342544556, Discriminator Loss: 1.669708490371704\n",
            "Epoch: 1113, Generator Loss: 0.6381648182868958, Discriminator Loss: 1.6743978261947632\n",
            "Epoch: 1114, Generator Loss: 0.6439035534858704, Discriminator Loss: 1.6506794691085815\n",
            "Epoch: 1115, Generator Loss: 0.6479878425598145, Discriminator Loss: 1.6630688905715942\n",
            "Epoch: 1116, Generator Loss: 0.6391757130622864, Discriminator Loss: 1.6430206298828125\n",
            "Epoch: 1117, Generator Loss: 0.6374538540840149, Discriminator Loss: 1.6823607683181763\n",
            "Epoch: 1118, Generator Loss: 0.6510518789291382, Discriminator Loss: 1.6814641952514648\n",
            "Epoch: 1119, Generator Loss: 0.6490689516067505, Discriminator Loss: 1.7116843461990356\n",
            "Epoch: 1120, Generator Loss: 0.6573408842086792, Discriminator Loss: 1.6356258392333984\n",
            "Epoch: 1121, Generator Loss: 0.6525334119796753, Discriminator Loss: 1.6447101831436157\n",
            "Epoch: 1122, Generator Loss: 0.654327929019928, Discriminator Loss: 1.67890202999115\n",
            "Epoch: 1123, Generator Loss: 0.6561023592948914, Discriminator Loss: 1.6463215351104736\n",
            "Epoch: 1124, Generator Loss: 0.6558276414871216, Discriminator Loss: 1.6595121622085571\n",
            "Epoch: 1125, Generator Loss: 0.6517801284790039, Discriminator Loss: 1.6866587400436401\n",
            "Epoch: 1126, Generator Loss: 0.6523953676223755, Discriminator Loss: 1.7253564596176147\n",
            "Epoch: 1127, Generator Loss: 0.6556571125984192, Discriminator Loss: 1.6603329181671143\n",
            "Epoch: 1128, Generator Loss: 0.6580336689949036, Discriminator Loss: 1.6477347612380981\n",
            "Epoch: 1129, Generator Loss: 0.6492061614990234, Discriminator Loss: 1.698004126548767\n",
            "Epoch: 1130, Generator Loss: 0.6553652882575989, Discriminator Loss: 1.6364322900772095\n",
            "Epoch: 1131, Generator Loss: 0.655481219291687, Discriminator Loss: 1.6653510332107544\n",
            "Epoch: 1132, Generator Loss: 0.6522723436355591, Discriminator Loss: 1.6799472570419312\n",
            "Epoch: 1133, Generator Loss: 0.6514487862586975, Discriminator Loss: 1.672151803970337\n",
            "Epoch: 1134, Generator Loss: 0.6415582895278931, Discriminator Loss: 1.6848933696746826\n",
            "Epoch: 1135, Generator Loss: 0.6509315371513367, Discriminator Loss: 1.667061686515808\n",
            "Epoch: 1136, Generator Loss: 0.6556640267372131, Discriminator Loss: 1.6776809692382812\n",
            "Epoch: 1137, Generator Loss: 0.6495510935783386, Discriminator Loss: 1.6474812030792236\n",
            "Epoch: 1138, Generator Loss: 0.6538923978805542, Discriminator Loss: 1.7020193338394165\n",
            "Epoch: 1139, Generator Loss: 0.6513382792472839, Discriminator Loss: 1.6687861680984497\n",
            "Epoch: 1140, Generator Loss: 0.6505554914474487, Discriminator Loss: 1.673471212387085\n",
            "Epoch: 1141, Generator Loss: 0.6535704731941223, Discriminator Loss: 1.6769574880599976\n",
            "Epoch: 1142, Generator Loss: 0.6571369171142578, Discriminator Loss: 1.6974124908447266\n",
            "Epoch: 1143, Generator Loss: 0.6521667838096619, Discriminator Loss: 1.6580873727798462\n",
            "Epoch: 1144, Generator Loss: 0.6569467186927795, Discriminator Loss: 1.7094935178756714\n",
            "Epoch: 1145, Generator Loss: 0.6512962579727173, Discriminator Loss: 1.655438780784607\n",
            "Epoch: 1146, Generator Loss: 0.6493626832962036, Discriminator Loss: 1.6713361740112305\n",
            "Epoch: 1147, Generator Loss: 0.6452003121376038, Discriminator Loss: 1.6291719675064087\n",
            "Epoch: 1148, Generator Loss: 0.6376761198043823, Discriminator Loss: 1.6745296716690063\n",
            "Epoch: 1149, Generator Loss: 0.6553465127944946, Discriminator Loss: 1.6107842922210693\n",
            "Epoch: 1150, Generator Loss: 0.6477042436599731, Discriminator Loss: 1.635528326034546\n",
            "Epoch: 1151, Generator Loss: 0.6392406821250916, Discriminator Loss: 1.7099491357803345\n",
            "Epoch: 1152, Generator Loss: 0.6380015015602112, Discriminator Loss: 1.6370304822921753\n",
            "Epoch: 1153, Generator Loss: 0.6406666040420532, Discriminator Loss: 1.6190201044082642\n",
            "Epoch: 1154, Generator Loss: 0.6413539052009583, Discriminator Loss: 1.6611436605453491\n",
            "Epoch: 1155, Generator Loss: 0.6367269158363342, Discriminator Loss: 1.6914044618606567\n",
            "Epoch: 1156, Generator Loss: 0.6393346786499023, Discriminator Loss: 1.6802583932876587\n",
            "Epoch: 1157, Generator Loss: 0.6393870711326599, Discriminator Loss: 1.67672598361969\n",
            "Epoch: 1158, Generator Loss: 0.6383585333824158, Discriminator Loss: 1.674614667892456\n",
            "Epoch: 1159, Generator Loss: 0.644036591053009, Discriminator Loss: 1.6571682691574097\n",
            "Epoch: 1160, Generator Loss: 0.6431364417076111, Discriminator Loss: 1.657144546508789\n",
            "Epoch: 1161, Generator Loss: 0.6448215842247009, Discriminator Loss: 1.6429437398910522\n",
            "Epoch: 1162, Generator Loss: 0.6419533491134644, Discriminator Loss: 1.6056791543960571\n",
            "Epoch: 1163, Generator Loss: 0.6416446566581726, Discriminator Loss: 1.5818977355957031\n",
            "Epoch: 1164, Generator Loss: 0.6548519134521484, Discriminator Loss: 1.5999356508255005\n",
            "Epoch: 1165, Generator Loss: 0.6420032382011414, Discriminator Loss: 1.6220964193344116\n",
            "Epoch: 1166, Generator Loss: 0.6505847573280334, Discriminator Loss: 1.5674089193344116\n",
            "Epoch: 1167, Generator Loss: 0.6427062153816223, Discriminator Loss: 1.5828253030776978\n",
            "Epoch: 1168, Generator Loss: 0.6407782435417175, Discriminator Loss: 1.589030146598816\n",
            "Epoch: 1169, Generator Loss: 0.6410986185073853, Discriminator Loss: 1.5948489904403687\n",
            "Epoch: 1170, Generator Loss: 0.6456122994422913, Discriminator Loss: 1.5888527631759644\n",
            "Epoch: 1171, Generator Loss: 0.6491497159004211, Discriminator Loss: 1.6090869903564453\n",
            "Epoch: 1172, Generator Loss: 0.6435423493385315, Discriminator Loss: 1.6331459283828735\n",
            "Epoch: 1173, Generator Loss: 0.6303786635398865, Discriminator Loss: 1.6258840560913086\n",
            "Epoch: 1174, Generator Loss: 0.582170307636261, Discriminator Loss: 1.6716331243515015\n",
            "Epoch: 1175, Generator Loss: 0.5800583958625793, Discriminator Loss: 1.7005958557128906\n",
            "Epoch: 1176, Generator Loss: 0.5804582238197327, Discriminator Loss: 1.6898419857025146\n",
            "Epoch: 1177, Generator Loss: 0.5912343263626099, Discriminator Loss: 1.7561770677566528\n",
            "Epoch: 1178, Generator Loss: 0.6226202845573425, Discriminator Loss: 1.6695175170898438\n",
            "Epoch: 1179, Generator Loss: 0.6348945498466492, Discriminator Loss: 1.7125791311264038\n",
            "Epoch: 1180, Generator Loss: 0.6374363303184509, Discriminator Loss: 1.6726343631744385\n",
            "Epoch: 1181, Generator Loss: 0.6320720314979553, Discriminator Loss: 1.7130171060562134\n",
            "Epoch: 1182, Generator Loss: 0.64141446352005, Discriminator Loss: 1.7185968160629272\n",
            "Epoch: 1183, Generator Loss: 0.6411744356155396, Discriminator Loss: 1.7126057147979736\n",
            "Epoch: 1184, Generator Loss: 0.640991747379303, Discriminator Loss: 1.664078712463379\n",
            "Epoch: 1185, Generator Loss: 0.6468594670295715, Discriminator Loss: 1.6371678113937378\n",
            "Epoch: 1186, Generator Loss: 0.6457719206809998, Discriminator Loss: 1.6901301145553589\n",
            "Epoch: 1187, Generator Loss: 0.6518090963363647, Discriminator Loss: 1.6737916469573975\n",
            "Epoch: 1188, Generator Loss: 0.644311249256134, Discriminator Loss: 1.6805058717727661\n",
            "Epoch: 1189, Generator Loss: 0.6551898121833801, Discriminator Loss: 1.6754744052886963\n",
            "Epoch: 1190, Generator Loss: 0.6498600244522095, Discriminator Loss: 1.7017217874526978\n",
            "Epoch: 1191, Generator Loss: 0.6587594151496887, Discriminator Loss: 1.663667917251587\n",
            "Epoch: 1192, Generator Loss: 0.6525000333786011, Discriminator Loss: 1.664498209953308\n",
            "Epoch: 1193, Generator Loss: 0.6517258882522583, Discriminator Loss: 1.7034164667129517\n",
            "Epoch: 1194, Generator Loss: 0.6524985432624817, Discriminator Loss: 1.6786613464355469\n",
            "Epoch: 1195, Generator Loss: 0.6542423963546753, Discriminator Loss: 1.6772912740707397\n",
            "Epoch: 1196, Generator Loss: 0.6533303260803223, Discriminator Loss: 1.6728452444076538\n",
            "Epoch: 1197, Generator Loss: 0.663564145565033, Discriminator Loss: 1.6453417539596558\n",
            "Epoch: 1198, Generator Loss: 0.6367782354354858, Discriminator Loss: 1.6648823022842407\n",
            "Epoch: 1199, Generator Loss: 0.6455888152122498, Discriminator Loss: 1.6442896127700806\n",
            "Epoch: 1200, Generator Loss: 0.6387848258018494, Discriminator Loss: 1.7004314661026\n",
            "Epoch: 1201, Generator Loss: 0.6335766911506653, Discriminator Loss: 1.6814690828323364\n",
            "Epoch: 1202, Generator Loss: 0.6341056227684021, Discriminator Loss: 1.6849850416183472\n",
            "Epoch: 1203, Generator Loss: 0.6436320543289185, Discriminator Loss: 1.6574395895004272\n",
            "Epoch: 1204, Generator Loss: 0.6505817770957947, Discriminator Loss: 1.6722317934036255\n",
            "Epoch: 1205, Generator Loss: 0.6442270874977112, Discriminator Loss: 1.6693357229232788\n",
            "Epoch: 1206, Generator Loss: 0.6483955383300781, Discriminator Loss: 1.6528500318527222\n",
            "Epoch: 1207, Generator Loss: 0.6564576029777527, Discriminator Loss: 1.6724779605865479\n",
            "Epoch: 1208, Generator Loss: 0.6538344621658325, Discriminator Loss: 1.6752369403839111\n",
            "Epoch: 1209, Generator Loss: 0.6575826406478882, Discriminator Loss: 1.668224573135376\n",
            "Epoch: 1210, Generator Loss: 0.6576094627380371, Discriminator Loss: 1.6922645568847656\n",
            "Epoch: 1211, Generator Loss: 0.655213475227356, Discriminator Loss: 1.6593043804168701\n",
            "Epoch: 1212, Generator Loss: 0.6517152190208435, Discriminator Loss: 1.656184196472168\n",
            "Epoch: 1213, Generator Loss: 0.6567950248718262, Discriminator Loss: 1.6412822008132935\n",
            "Epoch: 1214, Generator Loss: 0.6616367101669312, Discriminator Loss: 1.6649812459945679\n",
            "Epoch: 1215, Generator Loss: 0.6530846357345581, Discriminator Loss: 1.663331151008606\n",
            "Epoch: 1216, Generator Loss: 0.6568703651428223, Discriminator Loss: 1.6737139225006104\n",
            "Epoch: 1217, Generator Loss: 0.6583381295204163, Discriminator Loss: 1.6306931972503662\n",
            "Epoch: 1218, Generator Loss: 0.6529985070228577, Discriminator Loss: 1.661277174949646\n",
            "Epoch: 1219, Generator Loss: 0.6612951159477234, Discriminator Loss: 1.681122064590454\n",
            "Epoch: 1220, Generator Loss: 0.6581515073776245, Discriminator Loss: 1.6311800479888916\n",
            "Epoch: 1221, Generator Loss: 0.6594228744506836, Discriminator Loss: 1.6595962047576904\n",
            "Epoch: 1222, Generator Loss: 0.6568355560302734, Discriminator Loss: 1.6690502166748047\n",
            "Epoch: 1223, Generator Loss: 0.6604880094528198, Discriminator Loss: 1.6635663509368896\n",
            "Epoch: 1224, Generator Loss: 0.6593460440635681, Discriminator Loss: 1.651484489440918\n",
            "Epoch: 1225, Generator Loss: 0.6665200591087341, Discriminator Loss: 1.6712507009506226\n",
            "Epoch: 1226, Generator Loss: 0.6578783392906189, Discriminator Loss: 1.6542160511016846\n",
            "Epoch: 1227, Generator Loss: 0.6584072113037109, Discriminator Loss: 1.6402149200439453\n",
            "Epoch: 1228, Generator Loss: 0.662287175655365, Discriminator Loss: 1.6472290754318237\n",
            "Epoch: 1229, Generator Loss: 0.655992865562439, Discriminator Loss: 1.7074915170669556\n",
            "Epoch: 1230, Generator Loss: 0.6606817841529846, Discriminator Loss: 1.6523689031600952\n",
            "Epoch: 1231, Generator Loss: 0.664672315120697, Discriminator Loss: 1.6423028707504272\n",
            "Epoch: 1232, Generator Loss: 0.661228597164154, Discriminator Loss: 1.6342144012451172\n",
            "Epoch: 1233, Generator Loss: 0.6663321256637573, Discriminator Loss: 1.6511577367782593\n",
            "Epoch: 1234, Generator Loss: 0.6553473472595215, Discriminator Loss: 1.6815918684005737\n",
            "Epoch: 1235, Generator Loss: 0.6639971137046814, Discriminator Loss: 1.645577073097229\n",
            "Epoch: 1236, Generator Loss: 0.6659244894981384, Discriminator Loss: 1.6479674577713013\n",
            "Epoch: 1237, Generator Loss: 0.6597194075584412, Discriminator Loss: 1.670525312423706\n",
            "Epoch: 1238, Generator Loss: 0.6630571484565735, Discriminator Loss: 1.642651915550232\n",
            "Epoch: 1239, Generator Loss: 0.6595485806465149, Discriminator Loss: 1.665457010269165\n",
            "Epoch: 1240, Generator Loss: 0.6574819684028625, Discriminator Loss: 1.6333731412887573\n",
            "Epoch: 1241, Generator Loss: 0.6577057242393494, Discriminator Loss: 1.6614841222763062\n",
            "Epoch: 1242, Generator Loss: 0.6495617628097534, Discriminator Loss: 1.6683508157730103\n",
            "Epoch: 1243, Generator Loss: 0.6570148468017578, Discriminator Loss: 1.6318538188934326\n",
            "Epoch: 1244, Generator Loss: 0.6516122221946716, Discriminator Loss: 1.6126329898834229\n",
            "Epoch: 1245, Generator Loss: 0.6460831761360168, Discriminator Loss: 1.6535875797271729\n",
            "Epoch: 1246, Generator Loss: 0.6564846038818359, Discriminator Loss: 1.6633306741714478\n",
            "Epoch: 1247, Generator Loss: 0.6472182869911194, Discriminator Loss: 1.6884450912475586\n",
            "Epoch: 1248, Generator Loss: 0.6570234298706055, Discriminator Loss: 1.646406888961792\n",
            "Epoch: 1249, Generator Loss: 0.6463174819946289, Discriminator Loss: 1.6338547468185425\n",
            "Epoch: 1250, Generator Loss: 0.6512469053268433, Discriminator Loss: 1.6754158735275269\n",
            "Epoch: 1251, Generator Loss: 0.6571413278579712, Discriminator Loss: 1.6420619487762451\n",
            "Epoch: 1252, Generator Loss: 0.649053156375885, Discriminator Loss: 1.6814134120941162\n",
            "Epoch: 1253, Generator Loss: 0.6495621800422668, Discriminator Loss: 1.6456615924835205\n",
            "Epoch: 1254, Generator Loss: 0.6529827117919922, Discriminator Loss: 1.6723383665084839\n",
            "Epoch: 1255, Generator Loss: 0.6461490392684937, Discriminator Loss: 1.6615219116210938\n",
            "Epoch: 1256, Generator Loss: 0.648934543132782, Discriminator Loss: 1.6550557613372803\n",
            "Epoch: 1257, Generator Loss: 0.6488696336746216, Discriminator Loss: 1.6581262350082397\n",
            "Epoch: 1258, Generator Loss: 0.6476975679397583, Discriminator Loss: 1.6355394124984741\n",
            "Epoch: 1259, Generator Loss: 0.6555215716362, Discriminator Loss: 1.6714153289794922\n",
            "Epoch: 1260, Generator Loss: 0.6525635123252869, Discriminator Loss: 1.6109732389450073\n",
            "Epoch: 1261, Generator Loss: 0.6444579362869263, Discriminator Loss: 1.6559669971466064\n",
            "Epoch: 1262, Generator Loss: 0.643211841583252, Discriminator Loss: 1.6661745309829712\n",
            "Epoch: 1263, Generator Loss: 0.6604682207107544, Discriminator Loss: 1.647653341293335\n",
            "Epoch: 1264, Generator Loss: 0.6488139033317566, Discriminator Loss: 1.6573295593261719\n",
            "Epoch: 1265, Generator Loss: 0.6554189920425415, Discriminator Loss: 1.6520349979400635\n",
            "Epoch: 1266, Generator Loss: 0.6481828689575195, Discriminator Loss: 1.6995155811309814\n",
            "Epoch: 1267, Generator Loss: 0.6568138003349304, Discriminator Loss: 1.6576728820800781\n",
            "Epoch: 1268, Generator Loss: 0.6551809310913086, Discriminator Loss: 1.693595290184021\n",
            "Epoch: 1269, Generator Loss: 0.6609169244766235, Discriminator Loss: 1.6618362665176392\n",
            "Epoch: 1270, Generator Loss: 0.6569326519966125, Discriminator Loss: 1.6806684732437134\n",
            "Epoch: 1271, Generator Loss: 0.6550309062004089, Discriminator Loss: 1.686145544052124\n",
            "Epoch: 1272, Generator Loss: 0.6517230868339539, Discriminator Loss: 1.6503115892410278\n",
            "Epoch: 1273, Generator Loss: 0.6569312214851379, Discriminator Loss: 1.62273108959198\n",
            "Epoch: 1274, Generator Loss: 0.6563076972961426, Discriminator Loss: 1.6275283098220825\n",
            "Epoch: 1275, Generator Loss: 0.6507123112678528, Discriminator Loss: 1.6577852964401245\n",
            "Epoch: 1276, Generator Loss: 0.6625983119010925, Discriminator Loss: 1.6506175994873047\n",
            "Epoch: 1277, Generator Loss: 0.6604320406913757, Discriminator Loss: 1.651310682296753\n",
            "Epoch: 1278, Generator Loss: 0.6537402868270874, Discriminator Loss: 1.6805360317230225\n",
            "Epoch: 1279, Generator Loss: 0.656126856803894, Discriminator Loss: 1.6517858505249023\n",
            "Epoch: 1280, Generator Loss: 0.6634419560432434, Discriminator Loss: 1.6701668500900269\n",
            "Epoch: 1281, Generator Loss: 0.6572081446647644, Discriminator Loss: 1.6593761444091797\n",
            "Epoch: 1282, Generator Loss: 0.6618607640266418, Discriminator Loss: 1.645729899406433\n",
            "Epoch: 1283, Generator Loss: 0.66155606508255, Discriminator Loss: 1.6788848638534546\n",
            "Epoch: 1284, Generator Loss: 0.6526760458946228, Discriminator Loss: 1.6467612981796265\n",
            "Epoch: 1285, Generator Loss: 0.6514220833778381, Discriminator Loss: 1.667316198348999\n",
            "Epoch: 1286, Generator Loss: 0.6488319635391235, Discriminator Loss: 1.6492310762405396\n",
            "Epoch: 1287, Generator Loss: 0.6490567326545715, Discriminator Loss: 1.6519107818603516\n",
            "Epoch: 1288, Generator Loss: 0.6603417992591858, Discriminator Loss: 1.6611175537109375\n",
            "Epoch: 1289, Generator Loss: 0.659924328327179, Discriminator Loss: 1.669698715209961\n",
            "Epoch: 1290, Generator Loss: 0.6579888463020325, Discriminator Loss: 1.6461057662963867\n",
            "Epoch: 1291, Generator Loss: 0.6631989479064941, Discriminator Loss: 1.619891881942749\n",
            "Epoch: 1292, Generator Loss: 0.6549245119094849, Discriminator Loss: 1.6333200931549072\n",
            "Epoch: 1293, Generator Loss: 0.6525278687477112, Discriminator Loss: 1.658944845199585\n",
            "Epoch: 1294, Generator Loss: 0.6480426788330078, Discriminator Loss: 1.6828699111938477\n",
            "Epoch: 1295, Generator Loss: 0.6523095369338989, Discriminator Loss: 1.6259708404541016\n",
            "Epoch: 1296, Generator Loss: 0.6517998576164246, Discriminator Loss: 1.6356550455093384\n",
            "Epoch: 1297, Generator Loss: 0.6494055986404419, Discriminator Loss: 1.639508605003357\n",
            "Epoch: 1298, Generator Loss: 0.6532209515571594, Discriminator Loss: 1.622077226638794\n",
            "Epoch: 1299, Generator Loss: 0.6499229669570923, Discriminator Loss: 1.6078447103500366\n",
            "Epoch: 1300, Generator Loss: 0.6418628096580505, Discriminator Loss: 1.5690386295318604\n",
            "Epoch: 1301, Generator Loss: 0.6426067352294922, Discriminator Loss: 1.6144112348556519\n",
            "Epoch: 1302, Generator Loss: 0.6285651326179504, Discriminator Loss: 1.5892590284347534\n",
            "Epoch: 1303, Generator Loss: 0.6249789595603943, Discriminator Loss: 1.6109968423843384\n",
            "Epoch: 1304, Generator Loss: 0.5947089791297913, Discriminator Loss: 1.6679933071136475\n",
            "Epoch: 1305, Generator Loss: 0.5850836634635925, Discriminator Loss: 1.6738673448562622\n",
            "Epoch: 1306, Generator Loss: 0.600967288017273, Discriminator Loss: 1.6951391696929932\n",
            "Epoch: 1307, Generator Loss: 0.6168094277381897, Discriminator Loss: 1.645403265953064\n",
            "Epoch: 1308, Generator Loss: 0.6246881484985352, Discriminator Loss: 1.689113974571228\n",
            "Epoch: 1309, Generator Loss: 0.6312369704246521, Discriminator Loss: 1.6348329782485962\n",
            "Epoch: 1310, Generator Loss: 0.6363827586174011, Discriminator Loss: 1.6716439723968506\n",
            "Epoch: 1311, Generator Loss: 0.6330011487007141, Discriminator Loss: 1.6741737127304077\n",
            "Epoch: 1312, Generator Loss: 0.6424239277839661, Discriminator Loss: 1.6702117919921875\n",
            "Epoch: 1313, Generator Loss: 0.6409842371940613, Discriminator Loss: 1.6909923553466797\n",
            "Epoch: 1314, Generator Loss: 0.6508497595787048, Discriminator Loss: 1.6407569646835327\n",
            "Epoch: 1315, Generator Loss: 0.6398538947105408, Discriminator Loss: 1.664854884147644\n",
            "Epoch: 1316, Generator Loss: 0.6451045870780945, Discriminator Loss: 1.6511999368667603\n",
            "Epoch: 1317, Generator Loss: 0.6491714119911194, Discriminator Loss: 1.6139259338378906\n",
            "Epoch: 1318, Generator Loss: 0.6404816508293152, Discriminator Loss: 1.6457159519195557\n",
            "Epoch: 1319, Generator Loss: 0.6459512114524841, Discriminator Loss: 1.6475096940994263\n",
            "Epoch: 1320, Generator Loss: 0.6477831602096558, Discriminator Loss: 1.6583430767059326\n",
            "Epoch: 1321, Generator Loss: 0.6496142148971558, Discriminator Loss: 1.6780344247817993\n",
            "Epoch: 1322, Generator Loss: 0.6514231562614441, Discriminator Loss: 1.668905258178711\n",
            "Epoch: 1323, Generator Loss: 0.6539020538330078, Discriminator Loss: 1.6433804035186768\n",
            "Epoch: 1324, Generator Loss: 0.6497960686683655, Discriminator Loss: 1.6795635223388672\n",
            "Epoch: 1325, Generator Loss: 0.6558197736740112, Discriminator Loss: 1.6470773220062256\n",
            "Epoch: 1326, Generator Loss: 0.6530619859695435, Discriminator Loss: 1.6548281908035278\n",
            "Epoch: 1327, Generator Loss: 0.6527928113937378, Discriminator Loss: 1.6337203979492188\n",
            "Epoch: 1328, Generator Loss: 0.650340735912323, Discriminator Loss: 1.6813390254974365\n",
            "Epoch: 1329, Generator Loss: 0.6589441299438477, Discriminator Loss: 1.6539212465286255\n",
            "Epoch: 1330, Generator Loss: 0.6559054255485535, Discriminator Loss: 1.6442512273788452\n",
            "Epoch: 1331, Generator Loss: 0.6485267281532288, Discriminator Loss: 1.6683921813964844\n",
            "Epoch: 1332, Generator Loss: 0.6525558233261108, Discriminator Loss: 1.6730924844741821\n",
            "Epoch: 1333, Generator Loss: 0.6586003303527832, Discriminator Loss: 1.649617314338684\n",
            "Epoch: 1334, Generator Loss: 0.6543957591056824, Discriminator Loss: 1.6667085886001587\n",
            "Epoch: 1335, Generator Loss: 0.6522388458251953, Discriminator Loss: 1.674619436264038\n",
            "Epoch: 1336, Generator Loss: 0.6568306088447571, Discriminator Loss: 1.6267865896224976\n",
            "Epoch: 1337, Generator Loss: 0.65485680103302, Discriminator Loss: 1.684126853942871\n",
            "Epoch: 1338, Generator Loss: 0.6575249433517456, Discriminator Loss: 1.5911023616790771\n",
            "Epoch: 1339, Generator Loss: 0.6550391316413879, Discriminator Loss: 1.6823457479476929\n",
            "Epoch: 1340, Generator Loss: 0.6554521322250366, Discriminator Loss: 1.6761424541473389\n",
            "Epoch: 1341, Generator Loss: 0.6577993631362915, Discriminator Loss: 1.6640088558197021\n",
            "Epoch: 1342, Generator Loss: 0.6514478921890259, Discriminator Loss: 1.6120929718017578\n",
            "Epoch: 1343, Generator Loss: 0.6558846235275269, Discriminator Loss: 1.6789289712905884\n",
            "Epoch: 1344, Generator Loss: 0.660625159740448, Discriminator Loss: 1.6808301210403442\n",
            "Epoch: 1345, Generator Loss: 0.6547390818595886, Discriminator Loss: 1.6274152994155884\n",
            "Epoch: 1346, Generator Loss: 0.6576013565063477, Discriminator Loss: 1.6730738878250122\n",
            "Epoch: 1347, Generator Loss: 0.6549287438392639, Discriminator Loss: 1.6392295360565186\n",
            "Epoch: 1348, Generator Loss: 0.6562065482139587, Discriminator Loss: 1.6636428833007812\n",
            "Epoch: 1349, Generator Loss: 0.6492525935173035, Discriminator Loss: 1.6764018535614014\n",
            "Epoch: 1350, Generator Loss: 0.6484156847000122, Discriminator Loss: 1.6637922525405884\n",
            "Epoch: 1351, Generator Loss: 0.6601997017860413, Discriminator Loss: 1.6507748365402222\n",
            "Epoch: 1352, Generator Loss: 0.6525670289993286, Discriminator Loss: 1.6379822492599487\n",
            "Epoch: 1353, Generator Loss: 0.6520934700965881, Discriminator Loss: 1.6746572256088257\n",
            "Epoch: 1354, Generator Loss: 0.6608067750930786, Discriminator Loss: 1.6695268154144287\n",
            "Epoch: 1355, Generator Loss: 0.6557994484901428, Discriminator Loss: 1.68948233127594\n",
            "Epoch: 1356, Generator Loss: 0.6560629606246948, Discriminator Loss: 1.7012053728103638\n",
            "Epoch: 1357, Generator Loss: 0.6504091620445251, Discriminator Loss: 1.6159236431121826\n",
            "Epoch: 1358, Generator Loss: 0.653484046459198, Discriminator Loss: 1.6490404605865479\n",
            "Epoch: 1359, Generator Loss: 0.6490179896354675, Discriminator Loss: 1.667814016342163\n",
            "Epoch: 1360, Generator Loss: 0.6563626527786255, Discriminator Loss: 1.6430461406707764\n",
            "Epoch: 1361, Generator Loss: 0.6595810055732727, Discriminator Loss: 1.6645545959472656\n",
            "Epoch: 1362, Generator Loss: 0.6518763303756714, Discriminator Loss: 1.638771891593933\n",
            "Epoch: 1363, Generator Loss: 0.6547175049781799, Discriminator Loss: 1.6517945528030396\n",
            "Epoch: 1364, Generator Loss: 0.6567093729972839, Discriminator Loss: 1.6385873556137085\n",
            "Epoch: 1365, Generator Loss: 0.6515759825706482, Discriminator Loss: 1.6978590488433838\n",
            "Epoch: 1366, Generator Loss: 0.6544405817985535, Discriminator Loss: 1.673156976699829\n",
            "Epoch: 1367, Generator Loss: 0.6521363854408264, Discriminator Loss: 1.6472216844558716\n",
            "Epoch: 1368, Generator Loss: 0.6503686904907227, Discriminator Loss: 1.6593793630599976\n",
            "Epoch: 1369, Generator Loss: 0.6518344283103943, Discriminator Loss: 1.6513633728027344\n",
            "Epoch: 1370, Generator Loss: 0.6489633917808533, Discriminator Loss: 1.6366578340530396\n",
            "Epoch: 1371, Generator Loss: 0.6501855850219727, Discriminator Loss: 1.6459428071975708\n",
            "Epoch: 1372, Generator Loss: 0.6447911858558655, Discriminator Loss: 1.7110360860824585\n",
            "Epoch: 1373, Generator Loss: 0.6529685854911804, Discriminator Loss: 1.6396534442901611\n",
            "Epoch: 1374, Generator Loss: 0.6560923457145691, Discriminator Loss: 1.6347157955169678\n",
            "Epoch: 1375, Generator Loss: 0.6490367650985718, Discriminator Loss: 1.658794641494751\n",
            "Epoch: 1376, Generator Loss: 0.6506088376045227, Discriminator Loss: 1.6556072235107422\n",
            "Epoch: 1377, Generator Loss: 0.6489596962928772, Discriminator Loss: 1.661228895187378\n",
            "Epoch: 1378, Generator Loss: 0.6527214646339417, Discriminator Loss: 1.667049765586853\n",
            "Epoch: 1379, Generator Loss: 0.6529183983802795, Discriminator Loss: 1.685502052307129\n",
            "Epoch: 1380, Generator Loss: 0.6568640470504761, Discriminator Loss: 1.6518309116363525\n",
            "Epoch: 1381, Generator Loss: 0.6518393754959106, Discriminator Loss: 1.6204591989517212\n",
            "Epoch: 1382, Generator Loss: 0.6591614484786987, Discriminator Loss: 1.6384990215301514\n",
            "Epoch: 1383, Generator Loss: 0.6598365902900696, Discriminator Loss: 1.680704116821289\n",
            "Epoch: 1384, Generator Loss: 0.6496005654335022, Discriminator Loss: 1.6709786653518677\n",
            "Epoch: 1385, Generator Loss: 0.6513761878013611, Discriminator Loss: 1.6586331129074097\n",
            "Epoch: 1386, Generator Loss: 0.6581485867500305, Discriminator Loss: 1.6656635999679565\n",
            "Epoch: 1387, Generator Loss: 0.6555308103561401, Discriminator Loss: 1.6737765073776245\n",
            "Epoch: 1388, Generator Loss: 0.6552044153213501, Discriminator Loss: 1.6267225742340088\n",
            "Epoch: 1389, Generator Loss: 0.6576852202415466, Discriminator Loss: 1.6396116018295288\n",
            "Epoch: 1390, Generator Loss: 0.6568583846092224, Discriminator Loss: 1.7009421586990356\n",
            "Epoch: 1391, Generator Loss: 0.6564041972160339, Discriminator Loss: 1.6158987283706665\n",
            "Epoch: 1392, Generator Loss: 0.6506990194320679, Discriminator Loss: 1.6517212390899658\n",
            "Epoch: 1393, Generator Loss: 0.6549952626228333, Discriminator Loss: 1.650553584098816\n",
            "Epoch: 1394, Generator Loss: 0.659880518913269, Discriminator Loss: 1.6630779504776\n",
            "Epoch: 1395, Generator Loss: 0.6580024361610413, Discriminator Loss: 1.6677281856536865\n",
            "Epoch: 1396, Generator Loss: 0.6548729538917542, Discriminator Loss: 1.645447015762329\n",
            "Epoch: 1397, Generator Loss: 0.6544448137283325, Discriminator Loss: 1.6757452487945557\n",
            "Epoch: 1398, Generator Loss: 0.6600199937820435, Discriminator Loss: 1.635045051574707\n",
            "Epoch: 1399, Generator Loss: 0.6544896364212036, Discriminator Loss: 1.6702972650527954\n",
            "Epoch: 1400, Generator Loss: 0.6603055596351624, Discriminator Loss: 1.6506913900375366\n",
            "Epoch: 1401, Generator Loss: 0.6553611159324646, Discriminator Loss: 1.6341562271118164\n",
            "Epoch: 1402, Generator Loss: 0.6566607356071472, Discriminator Loss: 1.6581367254257202\n",
            "Epoch: 1403, Generator Loss: 0.6620674729347229, Discriminator Loss: 1.6969013214111328\n",
            "Epoch: 1404, Generator Loss: 0.6609893441200256, Discriminator Loss: 1.6285349130630493\n",
            "Epoch: 1405, Generator Loss: 0.6584230661392212, Discriminator Loss: 1.6492846012115479\n",
            "Epoch: 1406, Generator Loss: 0.656855583190918, Discriminator Loss: 1.6808531284332275\n",
            "Epoch: 1407, Generator Loss: 0.6590844392776489, Discriminator Loss: 1.6096456050872803\n",
            "Epoch: 1408, Generator Loss: 0.6562138199806213, Discriminator Loss: 1.6509798765182495\n",
            "Epoch: 1409, Generator Loss: 0.6604600548744202, Discriminator Loss: 1.6339671611785889\n",
            "Epoch: 1410, Generator Loss: 0.6618840098381042, Discriminator Loss: 1.648142695426941\n",
            "Epoch: 1411, Generator Loss: 0.6653160452842712, Discriminator Loss: 1.666109323501587\n",
            "Epoch: 1412, Generator Loss: 0.6562235951423645, Discriminator Loss: 1.6907094717025757\n",
            "Epoch: 1413, Generator Loss: 0.6614949107170105, Discriminator Loss: 1.639661431312561\n",
            "Epoch: 1414, Generator Loss: 0.6565523743629456, Discriminator Loss: 1.6257809400558472\n",
            "Epoch: 1415, Generator Loss: 0.6568604111671448, Discriminator Loss: 1.6851071119308472\n",
            "Epoch: 1416, Generator Loss: 0.6569623947143555, Discriminator Loss: 1.6646875143051147\n",
            "Epoch: 1417, Generator Loss: 0.6543565988540649, Discriminator Loss: 1.6506017446517944\n",
            "Epoch: 1418, Generator Loss: 0.6563649773597717, Discriminator Loss: 1.6141937971115112\n",
            "Epoch: 1419, Generator Loss: 0.6596975326538086, Discriminator Loss: 1.6386464834213257\n",
            "Epoch: 1420, Generator Loss: 0.6657269597053528, Discriminator Loss: 1.610144853591919\n",
            "Epoch: 1421, Generator Loss: 0.6587496399879456, Discriminator Loss: 1.6303255558013916\n",
            "Epoch: 1422, Generator Loss: 0.6635017395019531, Discriminator Loss: 1.6312607526779175\n",
            "Epoch: 1423, Generator Loss: 0.6579347252845764, Discriminator Loss: 1.650673747062683\n",
            "Epoch: 1424, Generator Loss: 0.6510380506515503, Discriminator Loss: 1.669442892074585\n",
            "Epoch: 1425, Generator Loss: 0.655156135559082, Discriminator Loss: 1.6463392972946167\n",
            "Epoch: 1426, Generator Loss: 0.6607983708381653, Discriminator Loss: 1.6719008684158325\n",
            "Epoch: 1427, Generator Loss: 0.6585878133773804, Discriminator Loss: 1.6453914642333984\n",
            "Epoch: 1428, Generator Loss: 0.6616678237915039, Discriminator Loss: 1.6267188787460327\n",
            "Epoch: 1429, Generator Loss: 0.6540742516517639, Discriminator Loss: 1.6494191884994507\n",
            "Epoch: 1430, Generator Loss: 0.6572757363319397, Discriminator Loss: 1.6896820068359375\n",
            "Epoch: 1431, Generator Loss: 0.6591415405273438, Discriminator Loss: 1.6513326168060303\n",
            "Epoch: 1432, Generator Loss: 0.6634434461593628, Discriminator Loss: 1.6597716808319092\n",
            "Epoch: 1433, Generator Loss: 0.661585807800293, Discriminator Loss: 1.628661036491394\n",
            "Epoch: 1434, Generator Loss: 0.6579685211181641, Discriminator Loss: 1.6663296222686768\n",
            "Epoch: 1435, Generator Loss: 0.6586350202560425, Discriminator Loss: 1.6048848628997803\n",
            "Epoch: 1436, Generator Loss: 0.6545435190200806, Discriminator Loss: 1.652557134628296\n",
            "Epoch: 1437, Generator Loss: 0.6585139036178589, Discriminator Loss: 1.653820276260376\n",
            "Epoch: 1438, Generator Loss: 0.6620820164680481, Discriminator Loss: 1.6498386859893799\n",
            "Epoch: 1439, Generator Loss: 0.6592152714729309, Discriminator Loss: 1.6556146144866943\n",
            "Epoch: 1440, Generator Loss: 0.6562591791152954, Discriminator Loss: 1.6591466665267944\n",
            "Epoch: 1441, Generator Loss: 0.6631302237510681, Discriminator Loss: 1.6013959646224976\n",
            "Epoch: 1442, Generator Loss: 0.6576060652732849, Discriminator Loss: 1.6491153240203857\n",
            "Epoch: 1443, Generator Loss: 0.6621440052986145, Discriminator Loss: 1.64711594581604\n",
            "Epoch: 1444, Generator Loss: 0.6596289873123169, Discriminator Loss: 1.6247752904891968\n",
            "Epoch: 1445, Generator Loss: 0.6640765070915222, Discriminator Loss: 1.6575137376785278\n",
            "Epoch: 1446, Generator Loss: 0.6527663469314575, Discriminator Loss: 1.66153883934021\n",
            "Epoch: 1447, Generator Loss: 0.6572738289833069, Discriminator Loss: 1.6726351976394653\n",
            "Epoch: 1448, Generator Loss: 0.6574092507362366, Discriminator Loss: 1.6645002365112305\n",
            "Epoch: 1449, Generator Loss: 0.6582712531089783, Discriminator Loss: 1.6454588174819946\n",
            "Epoch: 1450, Generator Loss: 0.6603453755378723, Discriminator Loss: 1.6618454456329346\n",
            "Epoch: 1451, Generator Loss: 0.6586240530014038, Discriminator Loss: 1.6688897609710693\n",
            "Epoch: 1452, Generator Loss: 0.6498944163322449, Discriminator Loss: 1.6512340307235718\n",
            "Epoch: 1453, Generator Loss: 0.6559500098228455, Discriminator Loss: 1.6273186206817627\n",
            "Epoch: 1454, Generator Loss: 0.6528449654579163, Discriminator Loss: 1.6383827924728394\n",
            "Epoch: 1455, Generator Loss: 0.6519349217414856, Discriminator Loss: 1.6604292392730713\n",
            "Epoch: 1456, Generator Loss: 0.6546324491500854, Discriminator Loss: 1.63625168800354\n",
            "Epoch: 1457, Generator Loss: 0.6488752365112305, Discriminator Loss: 1.6884313821792603\n",
            "Epoch: 1458, Generator Loss: 0.6507828831672668, Discriminator Loss: 1.6609452962875366\n",
            "Epoch: 1459, Generator Loss: 0.6583625078201294, Discriminator Loss: 1.6385400295257568\n",
            "Epoch: 1460, Generator Loss: 0.6511878371238708, Discriminator Loss: 1.613842248916626\n",
            "Epoch: 1461, Generator Loss: 0.6481748223304749, Discriminator Loss: 1.61484694480896\n",
            "Epoch: 1462, Generator Loss: 0.6499820351600647, Discriminator Loss: 1.644868016242981\n",
            "Epoch: 1463, Generator Loss: 0.6462215185165405, Discriminator Loss: 1.6486090421676636\n",
            "Epoch: 1464, Generator Loss: 0.6483065485954285, Discriminator Loss: 1.6479692459106445\n",
            "Epoch: 1465, Generator Loss: 0.6490894556045532, Discriminator Loss: 1.6677674055099487\n",
            "Epoch: 1466, Generator Loss: 0.6543803811073303, Discriminator Loss: 1.6583195924758911\n",
            "Epoch: 1467, Generator Loss: 0.6574227213859558, Discriminator Loss: 1.6439917087554932\n",
            "Epoch: 1468, Generator Loss: 0.6533251404762268, Discriminator Loss: 1.6354949474334717\n",
            "Epoch: 1469, Generator Loss: 0.6468241810798645, Discriminator Loss: 1.6264654397964478\n",
            "Epoch: 1470, Generator Loss: 0.6425334215164185, Discriminator Loss: 1.6567834615707397\n",
            "Epoch: 1471, Generator Loss: 0.6468720436096191, Discriminator Loss: 1.6812056303024292\n",
            "Epoch: 1472, Generator Loss: 0.6509811282157898, Discriminator Loss: 1.6513183116912842\n",
            "Epoch: 1473, Generator Loss: 0.6564846038818359, Discriminator Loss: 1.6513375043869019\n",
            "Epoch: 1474, Generator Loss: 0.6536237001419067, Discriminator Loss: 1.682418704032898\n",
            "Epoch: 1475, Generator Loss: 0.6590045690536499, Discriminator Loss: 1.6584688425064087\n",
            "Epoch: 1476, Generator Loss: 0.6613169312477112, Discriminator Loss: 1.6551769971847534\n",
            "Epoch: 1477, Generator Loss: 0.6633310914039612, Discriminator Loss: 1.637795090675354\n",
            "Epoch: 1478, Generator Loss: 0.6600578427314758, Discriminator Loss: 1.63441002368927\n",
            "Epoch: 1479, Generator Loss: 0.6542024612426758, Discriminator Loss: 1.674103856086731\n",
            "Epoch: 1480, Generator Loss: 0.6532198786735535, Discriminator Loss: 1.6344457864761353\n",
            "Epoch: 1481, Generator Loss: 0.6563940644264221, Discriminator Loss: 1.7030116319656372\n",
            "Epoch: 1482, Generator Loss: 0.6615994572639465, Discriminator Loss: 1.670164704322815\n",
            "Epoch: 1483, Generator Loss: 0.6580435633659363, Discriminator Loss: 1.6221288442611694\n",
            "Epoch: 1484, Generator Loss: 0.6597716212272644, Discriminator Loss: 1.6702147722244263\n",
            "Epoch: 1485, Generator Loss: 0.6623496413230896, Discriminator Loss: 1.6070078611373901\n",
            "Epoch: 1486, Generator Loss: 0.6602875590324402, Discriminator Loss: 1.6423860788345337\n",
            "Epoch: 1487, Generator Loss: 0.6556998491287231, Discriminator Loss: 1.6367653608322144\n",
            "Epoch: 1488, Generator Loss: 0.6632151007652283, Discriminator Loss: 1.6285607814788818\n",
            "Epoch: 1489, Generator Loss: 0.6618161797523499, Discriminator Loss: 1.6322563886642456\n",
            "Epoch: 1490, Generator Loss: 0.6637691855430603, Discriminator Loss: 1.6080896854400635\n",
            "Epoch: 1491, Generator Loss: 0.6605356335639954, Discriminator Loss: 1.6315311193466187\n",
            "Epoch: 1492, Generator Loss: 0.6557715535163879, Discriminator Loss: 1.6262400150299072\n",
            "Epoch: 1493, Generator Loss: 0.6591891646385193, Discriminator Loss: 1.6622053384780884\n",
            "Epoch: 1494, Generator Loss: 0.6513634920120239, Discriminator Loss: 1.6412559747695923\n",
            "Epoch: 1495, Generator Loss: 0.6554900407791138, Discriminator Loss: 1.6564377546310425\n",
            "Epoch: 1496, Generator Loss: 0.6457972526550293, Discriminator Loss: 1.6123942136764526\n",
            "Epoch: 1497, Generator Loss: 0.6547765731811523, Discriminator Loss: 1.6115500926971436\n",
            "Epoch: 1498, Generator Loss: 0.6446139216423035, Discriminator Loss: 1.6456836462020874\n",
            "Epoch: 1499, Generator Loss: 0.6432395577430725, Discriminator Loss: 1.6217248439788818\n",
            "Epoch: 1500, Generator Loss: 0.6430150270462036, Discriminator Loss: 1.6513696908950806\n",
            "Epoch: 1501, Generator Loss: 0.6425524353981018, Discriminator Loss: 1.6144014596939087\n",
            "Epoch: 1502, Generator Loss: 0.639177680015564, Discriminator Loss: 1.5908173322677612\n",
            "Epoch: 1503, Generator Loss: 0.629531979560852, Discriminator Loss: 1.6631834506988525\n",
            "Epoch: 1504, Generator Loss: 0.6248326897621155, Discriminator Loss: 1.6568048000335693\n",
            "Epoch: 1505, Generator Loss: 0.6462618708610535, Discriminator Loss: 1.6712895631790161\n",
            "Epoch: 1506, Generator Loss: 0.6499453783035278, Discriminator Loss: 1.6243343353271484\n",
            "Epoch: 1507, Generator Loss: 0.6502984166145325, Discriminator Loss: 1.6207648515701294\n",
            "Epoch: 1508, Generator Loss: 0.6465982794761658, Discriminator Loss: 1.6329339742660522\n",
            "Epoch: 1509, Generator Loss: 0.6551907658576965, Discriminator Loss: 1.6792831420898438\n",
            "Epoch: 1510, Generator Loss: 0.6498937606811523, Discriminator Loss: 1.6307498216629028\n",
            "Epoch: 1511, Generator Loss: 0.6509402394294739, Discriminator Loss: 1.62574303150177\n",
            "Epoch: 1512, Generator Loss: 0.6554293632507324, Discriminator Loss: 1.6558105945587158\n",
            "Epoch: 1513, Generator Loss: 0.6560513377189636, Discriminator Loss: 1.6418384313583374\n",
            "Epoch: 1514, Generator Loss: 0.6533184051513672, Discriminator Loss: 1.626468300819397\n",
            "Epoch: 1515, Generator Loss: 0.6568331718444824, Discriminator Loss: 1.6205729246139526\n",
            "Epoch: 1516, Generator Loss: 0.656605064868927, Discriminator Loss: 1.6122194528579712\n",
            "Epoch: 1517, Generator Loss: 0.6515697836875916, Discriminator Loss: 1.6324684619903564\n",
            "Epoch: 1518, Generator Loss: 0.6570647358894348, Discriminator Loss: 1.625020980834961\n",
            "Epoch: 1519, Generator Loss: 0.6526316404342651, Discriminator Loss: 1.6759655475616455\n",
            "Epoch: 1520, Generator Loss: 0.6491765379905701, Discriminator Loss: 1.66908860206604\n",
            "Epoch: 1521, Generator Loss: 0.655202329158783, Discriminator Loss: 1.6579797267913818\n",
            "Epoch: 1522, Generator Loss: 0.646727979183197, Discriminator Loss: 1.710609793663025\n",
            "Epoch: 1523, Generator Loss: 0.6539167165756226, Discriminator Loss: 1.6386330127716064\n",
            "Epoch: 1524, Generator Loss: 0.6540970206260681, Discriminator Loss: 1.648244023323059\n",
            "Epoch: 1525, Generator Loss: 0.6580397486686707, Discriminator Loss: 1.6265666484832764\n",
            "Epoch: 1526, Generator Loss: 0.6526103615760803, Discriminator Loss: 1.6231087446212769\n",
            "Epoch: 1527, Generator Loss: 0.6546220779418945, Discriminator Loss: 1.6682406663894653\n",
            "Epoch: 1528, Generator Loss: 0.6488274335861206, Discriminator Loss: 1.6366199254989624\n",
            "Epoch: 1529, Generator Loss: 0.6526719927787781, Discriminator Loss: 1.6288906335830688\n",
            "Epoch: 1530, Generator Loss: 0.6514195203781128, Discriminator Loss: 1.6144895553588867\n",
            "Epoch: 1531, Generator Loss: 0.6485229134559631, Discriminator Loss: 1.6265182495117188\n",
            "Epoch: 1532, Generator Loss: 0.6517993211746216, Discriminator Loss: 1.6343315839767456\n",
            "Epoch: 1533, Generator Loss: 0.6546005606651306, Discriminator Loss: 1.635905385017395\n",
            "Epoch: 1534, Generator Loss: 0.6559202075004578, Discriminator Loss: 1.6727389097213745\n",
            "Epoch: 1535, Generator Loss: 0.646848738193512, Discriminator Loss: 1.6296888589859009\n",
            "Epoch: 1536, Generator Loss: 0.6527727842330933, Discriminator Loss: 1.6377522945404053\n",
            "Epoch: 1537, Generator Loss: 0.6557617783546448, Discriminator Loss: 1.6380418539047241\n",
            "Epoch: 1538, Generator Loss: 0.6564304232597351, Discriminator Loss: 1.6052619218826294\n",
            "Epoch: 1539, Generator Loss: 0.6530006527900696, Discriminator Loss: 1.629730463027954\n",
            "Epoch: 1540, Generator Loss: 0.6532015800476074, Discriminator Loss: 1.648751974105835\n",
            "Epoch: 1541, Generator Loss: 0.6439740061759949, Discriminator Loss: 1.6691255569458008\n",
            "Epoch: 1542, Generator Loss: 0.6584257483482361, Discriminator Loss: 1.6598739624023438\n",
            "Epoch: 1543, Generator Loss: 0.6528401970863342, Discriminator Loss: 1.659842610359192\n",
            "Epoch: 1544, Generator Loss: 0.6512211561203003, Discriminator Loss: 1.6718968152999878\n",
            "Epoch: 1545, Generator Loss: 0.6536005139350891, Discriminator Loss: 1.6328240633010864\n",
            "Epoch: 1546, Generator Loss: 0.6536248922348022, Discriminator Loss: 1.6496965885162354\n",
            "Epoch: 1547, Generator Loss: 0.6527608036994934, Discriminator Loss: 1.69568932056427\n",
            "Epoch: 1548, Generator Loss: 0.6587553024291992, Discriminator Loss: 1.6262110471725464\n",
            "Epoch: 1549, Generator Loss: 0.6521804928779602, Discriminator Loss: 1.6503639221191406\n",
            "Epoch: 1550, Generator Loss: 0.6535734534263611, Discriminator Loss: 1.64067804813385\n",
            "Epoch: 1551, Generator Loss: 0.6585046052932739, Discriminator Loss: 1.6370869874954224\n",
            "Epoch: 1552, Generator Loss: 0.6554802656173706, Discriminator Loss: 1.6100322008132935\n",
            "Epoch: 1553, Generator Loss: 0.6581031084060669, Discriminator Loss: 1.6425378322601318\n",
            "Epoch: 1554, Generator Loss: 0.6541128158569336, Discriminator Loss: 1.601530909538269\n",
            "Epoch: 1555, Generator Loss: 0.6592316031455994, Discriminator Loss: 1.6354291439056396\n",
            "Epoch: 1556, Generator Loss: 0.6561371684074402, Discriminator Loss: 1.65523362159729\n",
            "Epoch: 1557, Generator Loss: 0.6565679907798767, Discriminator Loss: 1.648837685585022\n",
            "Epoch: 1558, Generator Loss: 0.6602242588996887, Discriminator Loss: 1.6922686100006104\n",
            "Epoch: 1559, Generator Loss: 0.6551467180252075, Discriminator Loss: 1.6457716226577759\n",
            "Epoch: 1560, Generator Loss: 0.6567196249961853, Discriminator Loss: 1.6762927770614624\n",
            "Epoch: 1561, Generator Loss: 0.6556415557861328, Discriminator Loss: 1.6305326223373413\n",
            "Epoch: 1562, Generator Loss: 0.6597387194633484, Discriminator Loss: 1.6262365579605103\n",
            "Epoch: 1563, Generator Loss: 0.6589251756668091, Discriminator Loss: 1.6018799543380737\n",
            "Epoch: 1564, Generator Loss: 0.6615744829177856, Discriminator Loss: 1.645588994026184\n",
            "Epoch: 1565, Generator Loss: 0.6577230095863342, Discriminator Loss: 1.6258983612060547\n",
            "Epoch: 1566, Generator Loss: 0.6631428599357605, Discriminator Loss: 1.6628156900405884\n",
            "Epoch: 1567, Generator Loss: 0.6569553017616272, Discriminator Loss: 1.626931071281433\n",
            "Epoch: 1568, Generator Loss: 0.6596799492835999, Discriminator Loss: 1.633476972579956\n",
            "Epoch: 1569, Generator Loss: 0.6640573143959045, Discriminator Loss: 1.6366384029388428\n",
            "Epoch: 1570, Generator Loss: 0.6553875803947449, Discriminator Loss: 1.6534397602081299\n",
            "Epoch: 1571, Generator Loss: 0.654504656791687, Discriminator Loss: 1.6339447498321533\n",
            "Epoch: 1572, Generator Loss: 0.6633333563804626, Discriminator Loss: 1.6694252490997314\n",
            "Epoch: 1573, Generator Loss: 0.6614107489585876, Discriminator Loss: 1.659788727760315\n",
            "Epoch: 1574, Generator Loss: 0.6618319153785706, Discriminator Loss: 1.6225032806396484\n",
            "Epoch: 1575, Generator Loss: 0.6630333662033081, Discriminator Loss: 1.6547483205795288\n",
            "Epoch: 1576, Generator Loss: 0.6661492586135864, Discriminator Loss: 1.6441758871078491\n",
            "Epoch: 1577, Generator Loss: 0.6613043546676636, Discriminator Loss: 1.6375778913497925\n",
            "Epoch: 1578, Generator Loss: 0.6655408143997192, Discriminator Loss: 1.6356645822525024\n",
            "Epoch: 1579, Generator Loss: 0.6607957482337952, Discriminator Loss: 1.642498254776001\n",
            "Epoch: 1580, Generator Loss: 0.6578585505485535, Discriminator Loss: 1.6074007749557495\n",
            "Epoch: 1581, Generator Loss: 0.6647090911865234, Discriminator Loss: 1.6239099502563477\n",
            "Epoch: 1582, Generator Loss: 0.6576879620552063, Discriminator Loss: 1.6337969303131104\n",
            "Epoch: 1583, Generator Loss: 0.6564692854881287, Discriminator Loss: 1.6510039567947388\n",
            "Epoch: 1584, Generator Loss: 0.6543906331062317, Discriminator Loss: 1.6757138967514038\n",
            "Epoch: 1585, Generator Loss: 0.6648194193840027, Discriminator Loss: 1.6291993856430054\n",
            "Epoch: 1586, Generator Loss: 0.6534827351570129, Discriminator Loss: 1.6582914590835571\n",
            "Epoch: 1587, Generator Loss: 0.6595600247383118, Discriminator Loss: 1.6139475107192993\n",
            "Epoch: 1588, Generator Loss: 0.6544595956802368, Discriminator Loss: 1.6309829950332642\n",
            "Epoch: 1589, Generator Loss: 0.6597399711608887, Discriminator Loss: 1.6173051595687866\n",
            "Epoch: 1590, Generator Loss: 0.6542343497276306, Discriminator Loss: 1.6419252157211304\n",
            "Epoch: 1591, Generator Loss: 0.6495491862297058, Discriminator Loss: 1.6606786251068115\n",
            "Epoch: 1592, Generator Loss: 0.6549233198165894, Discriminator Loss: 1.6538456678390503\n",
            "Epoch: 1593, Generator Loss: 0.6512560248374939, Discriminator Loss: 1.6197515726089478\n",
            "Epoch: 1594, Generator Loss: 0.6505410075187683, Discriminator Loss: 1.6584292650222778\n",
            "Epoch: 1595, Generator Loss: 0.652539074420929, Discriminator Loss: 1.6527807712554932\n",
            "Epoch: 1596, Generator Loss: 0.6572249531745911, Discriminator Loss: 1.6638017892837524\n",
            "Epoch: 1597, Generator Loss: 0.6531451344490051, Discriminator Loss: 1.6515238285064697\n",
            "Epoch: 1598, Generator Loss: 0.6488203406333923, Discriminator Loss: 1.6320741176605225\n",
            "Epoch: 1599, Generator Loss: 0.6512960195541382, Discriminator Loss: 1.6424801349639893\n",
            "Epoch: 1600, Generator Loss: 0.6551752090454102, Discriminator Loss: 1.6235895156860352\n",
            "Epoch: 1601, Generator Loss: 0.650301992893219, Discriminator Loss: 1.6275379657745361\n",
            "Epoch: 1602, Generator Loss: 0.6568396687507629, Discriminator Loss: 1.645675778388977\n",
            "Epoch: 1603, Generator Loss: 0.6594691872596741, Discriminator Loss: 1.6334999799728394\n",
            "Epoch: 1604, Generator Loss: 0.649903416633606, Discriminator Loss: 1.6466197967529297\n",
            "Epoch: 1605, Generator Loss: 0.6540797352790833, Discriminator Loss: 1.6862975358963013\n",
            "Epoch: 1606, Generator Loss: 0.6566153764724731, Discriminator Loss: 1.6298929452896118\n",
            "Epoch: 1607, Generator Loss: 0.6585566401481628, Discriminator Loss: 1.648863673210144\n",
            "Epoch: 1608, Generator Loss: 0.6571311354637146, Discriminator Loss: 1.6051260232925415\n",
            "Epoch: 1609, Generator Loss: 0.6556621193885803, Discriminator Loss: 1.631734013557434\n",
            "Epoch: 1610, Generator Loss: 0.6566658020019531, Discriminator Loss: 1.632034420967102\n",
            "Epoch: 1611, Generator Loss: 0.6563991904258728, Discriminator Loss: 1.6380224227905273\n",
            "Epoch: 1612, Generator Loss: 0.6491378545761108, Discriminator Loss: 1.6248540878295898\n",
            "Epoch: 1613, Generator Loss: 0.6385233998298645, Discriminator Loss: 1.6345354318618774\n",
            "Epoch: 1614, Generator Loss: 0.6392912268638611, Discriminator Loss: 1.636953353881836\n",
            "Epoch: 1615, Generator Loss: 0.6547282934188843, Discriminator Loss: 1.6897753477096558\n",
            "Epoch: 1616, Generator Loss: 0.6483994722366333, Discriminator Loss: 1.6510124206542969\n",
            "Epoch: 1617, Generator Loss: 0.6496419906616211, Discriminator Loss: 1.6552388668060303\n",
            "Epoch: 1618, Generator Loss: 0.648278534412384, Discriminator Loss: 1.6201809644699097\n",
            "Epoch: 1619, Generator Loss: 0.6482684016227722, Discriminator Loss: 1.6990163326263428\n",
            "Epoch: 1620, Generator Loss: 0.6476268768310547, Discriminator Loss: 1.652233362197876\n",
            "Epoch: 1621, Generator Loss: 0.6475459337234497, Discriminator Loss: 1.6750566959381104\n",
            "Epoch: 1622, Generator Loss: 0.6483855247497559, Discriminator Loss: 1.677287220954895\n",
            "Epoch: 1623, Generator Loss: 0.6492304801940918, Discriminator Loss: 1.656685471534729\n",
            "Epoch: 1624, Generator Loss: 0.6556753516197205, Discriminator Loss: 1.6112209558486938\n",
            "Epoch: 1625, Generator Loss: 0.6491680145263672, Discriminator Loss: 1.6337462663650513\n",
            "Epoch: 1626, Generator Loss: 0.652688205242157, Discriminator Loss: 1.632559061050415\n",
            "Epoch: 1627, Generator Loss: 0.65782231092453, Discriminator Loss: 1.6580308675765991\n",
            "Epoch: 1628, Generator Loss: 0.6543638110160828, Discriminator Loss: 1.627069354057312\n",
            "Epoch: 1629, Generator Loss: 0.651861846446991, Discriminator Loss: 1.624916672706604\n",
            "Epoch: 1630, Generator Loss: 0.6469680070877075, Discriminator Loss: 1.6779931783676147\n",
            "Epoch: 1631, Generator Loss: 0.6531122326850891, Discriminator Loss: 1.655408263206482\n",
            "Epoch: 1632, Generator Loss: 0.6562142372131348, Discriminator Loss: 1.6313215494155884\n",
            "Epoch: 1633, Generator Loss: 0.6633647084236145, Discriminator Loss: 1.6102796792984009\n",
            "Epoch: 1634, Generator Loss: 0.6565575003623962, Discriminator Loss: 1.6616719961166382\n",
            "Epoch: 1635, Generator Loss: 0.6536487936973572, Discriminator Loss: 1.6246798038482666\n",
            "Epoch: 1636, Generator Loss: 0.6482735872268677, Discriminator Loss: 1.7231532335281372\n",
            "Epoch: 1637, Generator Loss: 0.6518999934196472, Discriminator Loss: 1.6053187847137451\n",
            "Epoch: 1638, Generator Loss: 0.6547825932502747, Discriminator Loss: 1.6303426027297974\n",
            "Epoch: 1639, Generator Loss: 0.6460922956466675, Discriminator Loss: 1.630059003829956\n",
            "Epoch: 1640, Generator Loss: 0.6519483923912048, Discriminator Loss: 1.6547437906265259\n",
            "Epoch: 1641, Generator Loss: 0.6406944394111633, Discriminator Loss: 1.6317037343978882\n",
            "Epoch: 1642, Generator Loss: 0.6568986773490906, Discriminator Loss: 1.6402969360351562\n",
            "Epoch: 1643, Generator Loss: 0.6567639112472534, Discriminator Loss: 1.6338691711425781\n",
            "Epoch: 1644, Generator Loss: 0.6537891030311584, Discriminator Loss: 1.625017762184143\n",
            "Epoch: 1645, Generator Loss: 0.6517496705055237, Discriminator Loss: 1.680055856704712\n",
            "Epoch: 1646, Generator Loss: 0.652094304561615, Discriminator Loss: 1.6255958080291748\n",
            "Epoch: 1647, Generator Loss: 0.6520093679428101, Discriminator Loss: 1.6542407274246216\n",
            "Epoch: 1648, Generator Loss: 0.6566704511642456, Discriminator Loss: 1.6243832111358643\n",
            "Epoch: 1649, Generator Loss: 0.653880774974823, Discriminator Loss: 1.5886460542678833\n",
            "Epoch: 1650, Generator Loss: 0.6531192660331726, Discriminator Loss: 1.6125257015228271\n",
            "Epoch: 1651, Generator Loss: 0.6473612189292908, Discriminator Loss: 1.6193948984146118\n",
            "Epoch: 1652, Generator Loss: 0.6489068269729614, Discriminator Loss: 1.6175490617752075\n",
            "Epoch: 1653, Generator Loss: 0.6449757218360901, Discriminator Loss: 1.6161364316940308\n",
            "Epoch: 1654, Generator Loss: 0.6519954800605774, Discriminator Loss: 1.6135355234146118\n",
            "Epoch: 1655, Generator Loss: 0.6522507071495056, Discriminator Loss: 1.6083323955535889\n",
            "Epoch: 1656, Generator Loss: 0.6448208689689636, Discriminator Loss: 1.6301555633544922\n",
            "Epoch: 1657, Generator Loss: 0.6583870649337769, Discriminator Loss: 1.632287859916687\n",
            "Epoch: 1658, Generator Loss: 0.6528702974319458, Discriminator Loss: 1.6627379655838013\n",
            "Epoch: 1659, Generator Loss: 0.655659556388855, Discriminator Loss: 1.644542932510376\n",
            "Epoch: 1660, Generator Loss: 0.6562634706497192, Discriminator Loss: 1.647443175315857\n",
            "Epoch: 1661, Generator Loss: 0.6457090377807617, Discriminator Loss: 1.6173120737075806\n",
            "Epoch: 1662, Generator Loss: 0.6528894305229187, Discriminator Loss: 1.6411277055740356\n",
            "Epoch: 1663, Generator Loss: 0.6546854376792908, Discriminator Loss: 1.6150790452957153\n",
            "Epoch: 1664, Generator Loss: 0.6495345234870911, Discriminator Loss: 1.6150048971176147\n",
            "Epoch: 1665, Generator Loss: 0.6485819220542908, Discriminator Loss: 1.666728138923645\n",
            "Epoch: 1666, Generator Loss: 0.6556211113929749, Discriminator Loss: 1.6540015935897827\n",
            "Epoch: 1667, Generator Loss: 0.652903139591217, Discriminator Loss: 1.58879554271698\n",
            "Epoch: 1668, Generator Loss: 0.653870701789856, Discriminator Loss: 1.6050292253494263\n",
            "Epoch: 1669, Generator Loss: 0.6512956619262695, Discriminator Loss: 1.690651535987854\n",
            "Epoch: 1670, Generator Loss: 0.6545925736427307, Discriminator Loss: 1.6028846502304077\n",
            "Epoch: 1671, Generator Loss: 0.6495651006698608, Discriminator Loss: 1.6545655727386475\n",
            "Epoch: 1672, Generator Loss: 0.6578125357627869, Discriminator Loss: 1.6682994365692139\n",
            "Epoch: 1673, Generator Loss: 0.6594480872154236, Discriminator Loss: 1.6559642553329468\n",
            "Epoch: 1674, Generator Loss: 0.6529659628868103, Discriminator Loss: 1.6137278079986572\n",
            "Epoch: 1675, Generator Loss: 0.658396303653717, Discriminator Loss: 1.6378377676010132\n",
            "Epoch: 1676, Generator Loss: 0.6596769690513611, Discriminator Loss: 1.6107999086380005\n",
            "Epoch: 1677, Generator Loss: 0.6518006324768066, Discriminator Loss: 1.657368779182434\n",
            "Epoch: 1678, Generator Loss: 0.6593114137649536, Discriminator Loss: 1.6460336446762085\n",
            "Epoch: 1679, Generator Loss: 0.6569777727127075, Discriminator Loss: 1.6522020101547241\n",
            "Epoch: 1680, Generator Loss: 0.6561955809593201, Discriminator Loss: 1.6836549043655396\n",
            "Epoch: 1681, Generator Loss: 0.6501918435096741, Discriminator Loss: 1.653076171875\n",
            "Epoch: 1682, Generator Loss: 0.6587163209915161, Discriminator Loss: 1.6605108976364136\n",
            "Epoch: 1683, Generator Loss: 0.6583786606788635, Discriminator Loss: 1.6120675802230835\n",
            "Epoch: 1684, Generator Loss: 0.6570128798484802, Discriminator Loss: 1.6566814184188843\n",
            "Epoch: 1685, Generator Loss: 0.6546218991279602, Discriminator Loss: 1.6407889127731323\n",
            "Epoch: 1686, Generator Loss: 0.65517657995224, Discriminator Loss: 1.6114180088043213\n",
            "Epoch: 1687, Generator Loss: 0.6530166268348694, Discriminator Loss: 1.6785327196121216\n",
            "Epoch: 1688, Generator Loss: 0.6521416902542114, Discriminator Loss: 1.6450636386871338\n",
            "Epoch: 1689, Generator Loss: 0.6528257727622986, Discriminator Loss: 1.6475402116775513\n",
            "Epoch: 1690, Generator Loss: 0.6546869277954102, Discriminator Loss: 1.647735357284546\n",
            "Epoch: 1691, Generator Loss: 0.655400276184082, Discriminator Loss: 1.6343152523040771\n",
            "Epoch: 1692, Generator Loss: 0.6579924821853638, Discriminator Loss: 1.5981581211090088\n",
            "Epoch: 1693, Generator Loss: 0.6548903584480286, Discriminator Loss: 1.619358777999878\n",
            "Epoch: 1694, Generator Loss: 0.6534680724143982, Discriminator Loss: 1.6251699924468994\n",
            "Epoch: 1695, Generator Loss: 0.6506252288818359, Discriminator Loss: 1.662994623184204\n",
            "Epoch: 1696, Generator Loss: 0.6467809081077576, Discriminator Loss: 1.6555105447769165\n",
            "Epoch: 1697, Generator Loss: 0.6532773375511169, Discriminator Loss: 1.6323574781417847\n",
            "Epoch: 1698, Generator Loss: 0.6453878283500671, Discriminator Loss: 1.6560518741607666\n",
            "Epoch: 1699, Generator Loss: 0.6442491412162781, Discriminator Loss: 1.6453348398208618\n",
            "Epoch: 1700, Generator Loss: 0.6382367014884949, Discriminator Loss: 1.6335813999176025\n",
            "Epoch: 1701, Generator Loss: 0.6288038492202759, Discriminator Loss: 1.6438560485839844\n",
            "Epoch: 1702, Generator Loss: 0.6304918527603149, Discriminator Loss: 1.6676357984542847\n",
            "Epoch: 1703, Generator Loss: 0.640824019908905, Discriminator Loss: 1.6363680362701416\n",
            "Epoch: 1704, Generator Loss: 0.6385825276374817, Discriminator Loss: 1.6457644701004028\n",
            "Epoch: 1705, Generator Loss: 0.6515824198722839, Discriminator Loss: 1.6390453577041626\n",
            "Epoch: 1706, Generator Loss: 0.6412593126296997, Discriminator Loss: 1.662535309791565\n",
            "Epoch: 1707, Generator Loss: 0.6527104377746582, Discriminator Loss: 1.6069953441619873\n",
            "Epoch: 1708, Generator Loss: 0.6484580636024475, Discriminator Loss: 1.623600959777832\n",
            "Epoch: 1709, Generator Loss: 0.6509181261062622, Discriminator Loss: 1.6448934078216553\n",
            "Epoch: 1710, Generator Loss: 0.6506490707397461, Discriminator Loss: 1.685516357421875\n",
            "Epoch: 1711, Generator Loss: 0.6506959795951843, Discriminator Loss: 1.6520928144454956\n",
            "Epoch: 1712, Generator Loss: 0.6495113372802734, Discriminator Loss: 1.6215476989746094\n",
            "Epoch: 1713, Generator Loss: 0.6492448449134827, Discriminator Loss: 1.6094659566879272\n",
            "Epoch: 1714, Generator Loss: 0.6505246758460999, Discriminator Loss: 1.6039296388626099\n",
            "Epoch: 1715, Generator Loss: 0.6519113183021545, Discriminator Loss: 1.6326382160186768\n",
            "Epoch: 1716, Generator Loss: 0.6532357335090637, Discriminator Loss: 1.6409968137741089\n",
            "Epoch: 1717, Generator Loss: 0.6546155214309692, Discriminator Loss: 1.6176389455795288\n",
            "Epoch: 1718, Generator Loss: 0.6479170918464661, Discriminator Loss: 1.657737135887146\n",
            "Epoch: 1719, Generator Loss: 0.6575778722763062, Discriminator Loss: 1.5835329294204712\n",
            "Epoch: 1720, Generator Loss: 0.6519566774368286, Discriminator Loss: 1.6423341035842896\n",
            "Epoch: 1721, Generator Loss: 0.6484892964363098, Discriminator Loss: 1.683053731918335\n",
            "Epoch: 1722, Generator Loss: 0.650642991065979, Discriminator Loss: 1.637835144996643\n",
            "Epoch: 1723, Generator Loss: 0.6558594703674316, Discriminator Loss: 1.6194809675216675\n",
            "Epoch: 1724, Generator Loss: 0.6518282294273376, Discriminator Loss: 1.6467530727386475\n",
            "Epoch: 1725, Generator Loss: 0.6478360295295715, Discriminator Loss: 1.6514146327972412\n",
            "Epoch: 1726, Generator Loss: 0.6444883346557617, Discriminator Loss: 1.6412168741226196\n",
            "Epoch: 1727, Generator Loss: 0.6534343361854553, Discriminator Loss: 1.6000248193740845\n",
            "Epoch: 1728, Generator Loss: 0.6465891599655151, Discriminator Loss: 1.6461135149002075\n",
            "Epoch: 1729, Generator Loss: 0.6560482978820801, Discriminator Loss: 1.6189697980880737\n",
            "Epoch: 1730, Generator Loss: 0.6539630889892578, Discriminator Loss: 1.638292908668518\n",
            "Epoch: 1731, Generator Loss: 0.6429851651191711, Discriminator Loss: 1.6228147745132446\n",
            "Epoch: 1732, Generator Loss: 0.6601763963699341, Discriminator Loss: 1.5845214128494263\n",
            "Epoch: 1733, Generator Loss: 0.6501480937004089, Discriminator Loss: 1.6750613451004028\n",
            "Epoch: 1734, Generator Loss: 0.6476469039916992, Discriminator Loss: 1.6229828596115112\n",
            "Epoch: 1735, Generator Loss: 0.6498419046401978, Discriminator Loss: 1.6099436283111572\n",
            "Epoch: 1736, Generator Loss: 0.6593090891838074, Discriminator Loss: 1.631111741065979\n",
            "Epoch: 1737, Generator Loss: 0.6623263955116272, Discriminator Loss: 1.6281987428665161\n",
            "Epoch: 1738, Generator Loss: 0.6511895060539246, Discriminator Loss: 1.6469980478286743\n",
            "Epoch: 1739, Generator Loss: 0.6631816029548645, Discriminator Loss: 1.6421808004379272\n",
            "Epoch: 1740, Generator Loss: 0.6571289896965027, Discriminator Loss: 1.645585060119629\n",
            "Epoch: 1741, Generator Loss: 0.6533675789833069, Discriminator Loss: 1.6209204196929932\n",
            "Epoch: 1742, Generator Loss: 0.6534132361412048, Discriminator Loss: 1.6463850736618042\n",
            "Epoch: 1743, Generator Loss: 0.6589935421943665, Discriminator Loss: 1.6381340026855469\n",
            "Epoch: 1744, Generator Loss: 0.6510239243507385, Discriminator Loss: 1.6832106113433838\n",
            "Epoch: 1745, Generator Loss: 0.6552651524543762, Discriminator Loss: 1.6216716766357422\n",
            "Epoch: 1746, Generator Loss: 0.6596789360046387, Discriminator Loss: 1.6453181505203247\n",
            "Epoch: 1747, Generator Loss: 0.6544429063796997, Discriminator Loss: 1.6794025897979736\n",
            "Epoch: 1748, Generator Loss: 0.6523520350456238, Discriminator Loss: 1.6090196371078491\n",
            "Epoch: 1749, Generator Loss: 0.6486700177192688, Discriminator Loss: 1.6707324981689453\n",
            "Epoch: 1750, Generator Loss: 0.6519966125488281, Discriminator Loss: 1.659147024154663\n",
            "Epoch: 1751, Generator Loss: 0.6537075638771057, Discriminator Loss: 1.620564341545105\n",
            "Epoch: 1752, Generator Loss: 0.650515615940094, Discriminator Loss: 1.6733399629592896\n",
            "Epoch: 1753, Generator Loss: 0.657791018486023, Discriminator Loss: 1.6267133951187134\n",
            "Epoch: 1754, Generator Loss: 0.6560988426208496, Discriminator Loss: 1.6001840829849243\n",
            "Epoch: 1755, Generator Loss: 0.6530471444129944, Discriminator Loss: 1.6165739297866821\n",
            "Epoch: 1756, Generator Loss: 0.6495729088783264, Discriminator Loss: 1.623319387435913\n",
            "Epoch: 1757, Generator Loss: 0.6449612379074097, Discriminator Loss: 1.6438552141189575\n",
            "Epoch: 1758, Generator Loss: 0.6442117691040039, Discriminator Loss: 1.6420222520828247\n",
            "Epoch: 1759, Generator Loss: 0.6356333494186401, Discriminator Loss: 1.6693509817123413\n",
            "Epoch: 1760, Generator Loss: 0.6450234651565552, Discriminator Loss: 1.6252601146697998\n",
            "Epoch: 1761, Generator Loss: 0.6510728001594543, Discriminator Loss: 1.629752278327942\n",
            "Epoch: 1762, Generator Loss: 0.6473866105079651, Discriminator Loss: 1.642048716545105\n",
            "Epoch: 1763, Generator Loss: 0.6473207473754883, Discriminator Loss: 1.660629391670227\n",
            "Epoch: 1764, Generator Loss: 0.6535657048225403, Discriminator Loss: 1.6599925756454468\n",
            "Epoch: 1765, Generator Loss: 0.6439605355262756, Discriminator Loss: 1.585633635520935\n",
            "Epoch: 1766, Generator Loss: 0.6353449821472168, Discriminator Loss: 1.6394360065460205\n",
            "Epoch: 1767, Generator Loss: 0.6435695290565491, Discriminator Loss: 1.6440099477767944\n",
            "Epoch: 1768, Generator Loss: 0.6461548209190369, Discriminator Loss: 1.6127984523773193\n",
            "Epoch: 1769, Generator Loss: 0.6378973722457886, Discriminator Loss: 1.6355202198028564\n",
            "Epoch: 1770, Generator Loss: 0.6420193314552307, Discriminator Loss: 1.6494786739349365\n",
            "Epoch: 1771, Generator Loss: 0.6480590105056763, Discriminator Loss: 1.622864007949829\n",
            "Epoch: 1772, Generator Loss: 0.6347000002861023, Discriminator Loss: 1.6678617000579834\n",
            "Epoch: 1773, Generator Loss: 0.6475251317024231, Discriminator Loss: 1.5885587930679321\n",
            "Epoch: 1774, Generator Loss: 0.646622359752655, Discriminator Loss: 1.6217676401138306\n",
            "Epoch: 1775, Generator Loss: 0.6399731636047363, Discriminator Loss: 1.6354771852493286\n",
            "Epoch: 1776, Generator Loss: 0.652175784111023, Discriminator Loss: 1.6406112909317017\n",
            "Epoch: 1777, Generator Loss: 0.646121621131897, Discriminator Loss: 1.6015946865081787\n",
            "Epoch: 1778, Generator Loss: 0.6517065763473511, Discriminator Loss: 1.618026852607727\n",
            "Epoch: 1779, Generator Loss: 0.6506118774414062, Discriminator Loss: 1.6058937311172485\n",
            "Epoch: 1780, Generator Loss: 0.648282527923584, Discriminator Loss: 1.6316713094711304\n",
            "Epoch: 1781, Generator Loss: 0.6548487544059753, Discriminator Loss: 1.627036213874817\n",
            "Epoch: 1782, Generator Loss: 0.6474102735519409, Discriminator Loss: 1.599818468093872\n",
            "Epoch: 1783, Generator Loss: 0.6473628878593445, Discriminator Loss: 1.6708959341049194\n",
            "Epoch: 1784, Generator Loss: 0.6462445855140686, Discriminator Loss: 1.6803582906723022\n",
            "Epoch: 1785, Generator Loss: 0.6420494914054871, Discriminator Loss: 1.6521869897842407\n",
            "Epoch: 1786, Generator Loss: 0.6467174887657166, Discriminator Loss: 1.6096153259277344\n",
            "Epoch: 1787, Generator Loss: 0.642593502998352, Discriminator Loss: 1.6848869323730469\n",
            "Epoch: 1788, Generator Loss: 0.6478058099746704, Discriminator Loss: 1.5978747606277466\n",
            "Epoch: 1789, Generator Loss: 0.6453962326049805, Discriminator Loss: 1.6229517459869385\n",
            "Epoch: 1790, Generator Loss: 0.6545789837837219, Discriminator Loss: 1.6232032775878906\n",
            "Epoch: 1791, Generator Loss: 0.6558545231819153, Discriminator Loss: 1.638946533203125\n",
            "Epoch: 1792, Generator Loss: 0.6539954543113708, Discriminator Loss: 1.6237809658050537\n",
            "Epoch: 1793, Generator Loss: 0.651610791683197, Discriminator Loss: 1.6391575336456299\n",
            "Epoch: 1794, Generator Loss: 0.6475487351417542, Discriminator Loss: 1.6100488901138306\n",
            "Epoch: 1795, Generator Loss: 0.650598406791687, Discriminator Loss: 1.6166504621505737\n",
            "Epoch: 1796, Generator Loss: 0.6640235781669617, Discriminator Loss: 1.5625602006912231\n",
            "Epoch: 1797, Generator Loss: 0.6480736136436462, Discriminator Loss: 1.6364033222198486\n",
            "Epoch: 1798, Generator Loss: 0.6580473184585571, Discriminator Loss: 1.6319435834884644\n",
            "Epoch: 1799, Generator Loss: 0.6473811268806458, Discriminator Loss: 1.644296407699585\n",
            "Epoch: 1800, Generator Loss: 0.6536617279052734, Discriminator Loss: 1.6131311655044556\n",
            "Epoch: 1801, Generator Loss: 0.6447882056236267, Discriminator Loss: 1.6277697086334229\n",
            "Epoch: 1802, Generator Loss: 0.6464903950691223, Discriminator Loss: 1.6483118534088135\n",
            "Epoch: 1803, Generator Loss: 0.6456980109214783, Discriminator Loss: 1.6363998651504517\n",
            "Epoch: 1804, Generator Loss: 0.6440253257751465, Discriminator Loss: 1.6428767442703247\n",
            "Epoch: 1805, Generator Loss: 0.6505350470542908, Discriminator Loss: 1.634218454360962\n",
            "Epoch: 1806, Generator Loss: 0.6456427574157715, Discriminator Loss: 1.623016595840454\n",
            "Epoch: 1807, Generator Loss: 0.6580362915992737, Discriminator Loss: 1.5910485982894897\n",
            "Epoch: 1808, Generator Loss: 0.6537281274795532, Discriminator Loss: 1.6165841817855835\n",
            "Epoch: 1809, Generator Loss: 0.6449630856513977, Discriminator Loss: 1.6295095682144165\n",
            "Epoch: 1810, Generator Loss: 0.6465985178947449, Discriminator Loss: 1.649148941040039\n",
            "Epoch: 1811, Generator Loss: 0.6567949652671814, Discriminator Loss: 1.601843237876892\n",
            "Epoch: 1812, Generator Loss: 0.6555870771408081, Discriminator Loss: 1.641560435295105\n",
            "Epoch: 1813, Generator Loss: 0.6489578485488892, Discriminator Loss: 1.6073665618896484\n",
            "Epoch: 1814, Generator Loss: 0.6519541144371033, Discriminator Loss: 1.606593370437622\n",
            "Epoch: 1815, Generator Loss: 0.6510949730873108, Discriminator Loss: 1.6355189085006714\n",
            "Epoch: 1816, Generator Loss: 0.6513667702674866, Discriminator Loss: 1.6360634565353394\n",
            "Epoch: 1817, Generator Loss: 0.6493940353393555, Discriminator Loss: 1.6274839639663696\n",
            "Epoch: 1818, Generator Loss: 0.6583282351493835, Discriminator Loss: 1.6399568319320679\n",
            "Epoch: 1819, Generator Loss: 0.6556312441825867, Discriminator Loss: 1.607146978378296\n",
            "Epoch: 1820, Generator Loss: 0.6606396436691284, Discriminator Loss: 1.622454285621643\n",
            "Epoch: 1821, Generator Loss: 0.6626428961753845, Discriminator Loss: 1.591143250465393\n",
            "Epoch: 1822, Generator Loss: 0.6645842790603638, Discriminator Loss: 1.587556004524231\n",
            "Epoch: 1823, Generator Loss: 0.6587685942649841, Discriminator Loss: 1.62772536277771\n",
            "Epoch: 1824, Generator Loss: 0.6618954539299011, Discriminator Loss: 1.6146926879882812\n",
            "Epoch: 1825, Generator Loss: 0.6626787185668945, Discriminator Loss: 1.6330426931381226\n",
            "Epoch: 1826, Generator Loss: 0.6592888236045837, Discriminator Loss: 1.6283233165740967\n",
            "Epoch: 1827, Generator Loss: 0.6594291925430298, Discriminator Loss: 1.6032493114471436\n",
            "Epoch: 1828, Generator Loss: 0.6590520143508911, Discriminator Loss: 1.614859938621521\n",
            "Epoch: 1829, Generator Loss: 0.6534847021102905, Discriminator Loss: 1.6124765872955322\n",
            "Epoch: 1830, Generator Loss: 0.6595659255981445, Discriminator Loss: 1.6144441366195679\n",
            "Epoch: 1831, Generator Loss: 0.6597766876220703, Discriminator Loss: 1.6219812631607056\n",
            "Epoch: 1832, Generator Loss: 0.6632370948791504, Discriminator Loss: 1.6229207515716553\n",
            "Epoch: 1833, Generator Loss: 0.6589912176132202, Discriminator Loss: 1.6011043787002563\n",
            "Epoch: 1834, Generator Loss: 0.6605494618415833, Discriminator Loss: 1.6083365678787231\n",
            "Epoch: 1835, Generator Loss: 0.6549267768859863, Discriminator Loss: 1.6734199523925781\n",
            "Epoch: 1836, Generator Loss: 0.6547761559486389, Discriminator Loss: 1.598307490348816\n",
            "Epoch: 1837, Generator Loss: 0.644949734210968, Discriminator Loss: 1.638305425643921\n",
            "Epoch: 1838, Generator Loss: 0.6587953567504883, Discriminator Loss: 1.6213887929916382\n",
            "Epoch: 1839, Generator Loss: 0.6537121534347534, Discriminator Loss: 1.5991630554199219\n",
            "Epoch: 1840, Generator Loss: 0.6595215201377869, Discriminator Loss: 1.6198999881744385\n",
            "Epoch: 1841, Generator Loss: 0.6453233361244202, Discriminator Loss: 1.6345939636230469\n",
            "Epoch: 1842, Generator Loss: 0.6586936116218567, Discriminator Loss: 1.6453279256820679\n",
            "Epoch: 1843, Generator Loss: 0.6536874771118164, Discriminator Loss: 1.6546516418457031\n",
            "Epoch: 1844, Generator Loss: 0.6547709107398987, Discriminator Loss: 1.6129769086837769\n",
            "Epoch: 1845, Generator Loss: 0.6535406112670898, Discriminator Loss: 1.6283196210861206\n",
            "Epoch: 1846, Generator Loss: 0.6561371684074402, Discriminator Loss: 1.623366117477417\n",
            "Epoch: 1847, Generator Loss: 0.6454161405563354, Discriminator Loss: 1.6632194519042969\n",
            "Epoch: 1848, Generator Loss: 0.6586037874221802, Discriminator Loss: 1.6040343046188354\n",
            "Epoch: 1849, Generator Loss: 0.6430248618125916, Discriminator Loss: 1.6539993286132812\n",
            "Epoch: 1850, Generator Loss: 0.6494547724723816, Discriminator Loss: 1.6520004272460938\n",
            "Epoch: 1851, Generator Loss: 0.6567553281784058, Discriminator Loss: 1.6289911270141602\n",
            "Epoch: 1852, Generator Loss: 0.6531633734703064, Discriminator Loss: 1.6291052103042603\n",
            "Epoch: 1853, Generator Loss: 0.6555538773536682, Discriminator Loss: 1.5790538787841797\n",
            "Epoch: 1854, Generator Loss: 0.6540935635566711, Discriminator Loss: 1.5922986268997192\n",
            "Epoch: 1855, Generator Loss: 0.6589505076408386, Discriminator Loss: 1.6439679861068726\n",
            "Epoch: 1856, Generator Loss: 0.643476665019989, Discriminator Loss: 1.6209920644760132\n",
            "Epoch: 1857, Generator Loss: 0.6509631276130676, Discriminator Loss: 1.588797926902771\n",
            "Epoch: 1858, Generator Loss: 0.6428138613700867, Discriminator Loss: 1.6049013137817383\n",
            "Epoch: 1859, Generator Loss: 0.6340370774269104, Discriminator Loss: 1.645251989364624\n",
            "Epoch: 1860, Generator Loss: 0.6426934003829956, Discriminator Loss: 1.6107276678085327\n",
            "Epoch: 1861, Generator Loss: 0.6409763693809509, Discriminator Loss: 1.5948251485824585\n",
            "Epoch: 1862, Generator Loss: 0.6342477202415466, Discriminator Loss: 1.569774866104126\n",
            "Epoch: 1863, Generator Loss: 0.6423574090003967, Discriminator Loss: 1.6162844896316528\n",
            "Epoch: 1864, Generator Loss: 0.6306461691856384, Discriminator Loss: 1.5837738513946533\n",
            "Epoch: 1865, Generator Loss: 0.6273293495178223, Discriminator Loss: 1.6446369886398315\n",
            "Epoch: 1866, Generator Loss: 0.6212558746337891, Discriminator Loss: 1.5877749919891357\n",
            "Epoch: 1867, Generator Loss: 0.5955389738082886, Discriminator Loss: 1.6383552551269531\n",
            "Epoch: 1868, Generator Loss: 0.6062150001525879, Discriminator Loss: 1.623372197151184\n",
            "Epoch: 1869, Generator Loss: 0.621296763420105, Discriminator Loss: 1.6153274774551392\n",
            "Epoch: 1870, Generator Loss: 0.6404319405555725, Discriminator Loss: 1.6389122009277344\n",
            "Epoch: 1871, Generator Loss: 0.6423466801643372, Discriminator Loss: 1.6409885883331299\n",
            "Epoch: 1872, Generator Loss: 0.6432111859321594, Discriminator Loss: 1.674855351448059\n",
            "Epoch: 1873, Generator Loss: 0.6328492760658264, Discriminator Loss: 1.6045619249343872\n",
            "Epoch: 1874, Generator Loss: 0.6317957639694214, Discriminator Loss: 1.6952440738677979\n",
            "Epoch: 1875, Generator Loss: 0.6436141133308411, Discriminator Loss: 1.6149686574935913\n",
            "Epoch: 1876, Generator Loss: 0.6411986947059631, Discriminator Loss: 1.59356689453125\n",
            "Epoch: 1877, Generator Loss: 0.6436057686805725, Discriminator Loss: 1.6040176153182983\n",
            "Epoch: 1878, Generator Loss: 0.6478557586669922, Discriminator Loss: 1.6413203477859497\n",
            "Epoch: 1879, Generator Loss: 0.6430660486221313, Discriminator Loss: 1.6245206594467163\n",
            "Epoch: 1880, Generator Loss: 0.6401670575141907, Discriminator Loss: 1.6436097621917725\n",
            "Epoch: 1881, Generator Loss: 0.6490224599838257, Discriminator Loss: 1.6252151727676392\n",
            "Epoch: 1882, Generator Loss: 0.6475142240524292, Discriminator Loss: 1.6147280931472778\n",
            "Epoch: 1883, Generator Loss: 0.6481371521949768, Discriminator Loss: 1.6107608079910278\n",
            "Epoch: 1884, Generator Loss: 0.6508674621582031, Discriminator Loss: 1.6175956726074219\n",
            "Epoch: 1885, Generator Loss: 0.6484428644180298, Discriminator Loss: 1.6458665132522583\n",
            "Epoch: 1886, Generator Loss: 0.6512245535850525, Discriminator Loss: 1.635262370109558\n",
            "Epoch: 1887, Generator Loss: 0.6494889855384827, Discriminator Loss: 1.6211844682693481\n",
            "Epoch: 1888, Generator Loss: 0.6458370089530945, Discriminator Loss: 1.6414741277694702\n",
            "Epoch: 1889, Generator Loss: 0.6512726545333862, Discriminator Loss: 1.617169976234436\n",
            "Epoch: 1890, Generator Loss: 0.6570948362350464, Discriminator Loss: 1.6256872415542603\n",
            "Epoch: 1891, Generator Loss: 0.6492359042167664, Discriminator Loss: 1.6153337955474854\n",
            "Epoch: 1892, Generator Loss: 0.6551958918571472, Discriminator Loss: 1.6149423122406006\n",
            "Epoch: 1893, Generator Loss: 0.6541203260421753, Discriminator Loss: 1.647684097290039\n",
            "Epoch: 1894, Generator Loss: 0.6483802795410156, Discriminator Loss: 1.6085736751556396\n",
            "Epoch: 1895, Generator Loss: 0.6581233143806458, Discriminator Loss: 1.624055027961731\n",
            "Epoch: 1896, Generator Loss: 0.6568870544433594, Discriminator Loss: 1.613381266593933\n",
            "Epoch: 1897, Generator Loss: 0.6529243588447571, Discriminator Loss: 1.590219497680664\n",
            "Epoch: 1898, Generator Loss: 0.6507320404052734, Discriminator Loss: 1.639015793800354\n",
            "Epoch: 1899, Generator Loss: 0.6507376432418823, Discriminator Loss: 1.6151853799819946\n",
            "Epoch: 1900, Generator Loss: 0.6434527635574341, Discriminator Loss: 1.5976791381835938\n",
            "Epoch: 1901, Generator Loss: 0.6396297216415405, Discriminator Loss: 1.6882750988006592\n",
            "Epoch: 1902, Generator Loss: 0.6587964296340942, Discriminator Loss: 1.6309877634048462\n",
            "Epoch: 1903, Generator Loss: 0.6576092839241028, Discriminator Loss: 1.6007518768310547\n",
            "Epoch: 1904, Generator Loss: 0.65821772813797, Discriminator Loss: 1.5716561079025269\n",
            "Epoch: 1905, Generator Loss: 0.6597491502761841, Discriminator Loss: 1.6187368631362915\n",
            "Epoch: 1906, Generator Loss: 0.6554492115974426, Discriminator Loss: 1.597124695777893\n",
            "Epoch: 1907, Generator Loss: 0.6582861542701721, Discriminator Loss: 1.6355226039886475\n",
            "Epoch: 1908, Generator Loss: 0.6541919708251953, Discriminator Loss: 1.6220967769622803\n",
            "Epoch: 1909, Generator Loss: 0.6550762057304382, Discriminator Loss: 1.6219959259033203\n",
            "Epoch: 1910, Generator Loss: 0.6518670916557312, Discriminator Loss: 1.6675831079483032\n",
            "Epoch: 1911, Generator Loss: 0.65174400806427, Discriminator Loss: 1.5686525106430054\n",
            "Epoch: 1912, Generator Loss: 0.6526713371276855, Discriminator Loss: 1.5829145908355713\n",
            "Epoch: 1913, Generator Loss: 0.6321597695350647, Discriminator Loss: 1.6327601671218872\n",
            "Epoch: 1914, Generator Loss: 0.6419686079025269, Discriminator Loss: 1.6363706588745117\n",
            "Epoch: 1915, Generator Loss: 0.6511275172233582, Discriminator Loss: 1.6339657306671143\n",
            "Epoch: 1916, Generator Loss: 0.6453272104263306, Discriminator Loss: 1.6324776411056519\n",
            "Epoch: 1917, Generator Loss: 0.6458774209022522, Discriminator Loss: 1.627583622932434\n",
            "Epoch: 1918, Generator Loss: 0.6396728754043579, Discriminator Loss: 1.6132242679595947\n",
            "Epoch: 1919, Generator Loss: 0.6428807377815247, Discriminator Loss: 1.6136847734451294\n",
            "Epoch: 1920, Generator Loss: 0.6474935412406921, Discriminator Loss: 1.590140700340271\n",
            "Epoch: 1921, Generator Loss: 0.6481093168258667, Discriminator Loss: 1.6420449018478394\n",
            "Epoch: 1922, Generator Loss: 0.6480216383934021, Discriminator Loss: 1.5951385498046875\n",
            "Epoch: 1923, Generator Loss: 0.6464029550552368, Discriminator Loss: 1.605172038078308\n",
            "Epoch: 1924, Generator Loss: 0.6449764966964722, Discriminator Loss: 1.6111136674880981\n",
            "Epoch: 1925, Generator Loss: 0.647483229637146, Discriminator Loss: 1.646208643913269\n",
            "Epoch: 1926, Generator Loss: 0.6510300636291504, Discriminator Loss: 1.6020969152450562\n",
            "Epoch: 1927, Generator Loss: 0.6443389058113098, Discriminator Loss: 1.5966521501541138\n",
            "Epoch: 1928, Generator Loss: 0.6492210030555725, Discriminator Loss: 1.616578459739685\n",
            "Epoch: 1929, Generator Loss: 0.6477026343345642, Discriminator Loss: 1.630889654159546\n",
            "Epoch: 1930, Generator Loss: 0.6396942138671875, Discriminator Loss: 1.6506996154785156\n",
            "Epoch: 1931, Generator Loss: 0.6294507384300232, Discriminator Loss: 1.6333032846450806\n",
            "Epoch: 1932, Generator Loss: 0.6442384719848633, Discriminator Loss: 1.6143856048583984\n",
            "Epoch: 1933, Generator Loss: 0.6458913683891296, Discriminator Loss: 1.5797702074050903\n",
            "Epoch: 1934, Generator Loss: 0.6406584978103638, Discriminator Loss: 1.6194536685943604\n",
            "Epoch: 1935, Generator Loss: 0.6419585347175598, Discriminator Loss: 1.6019827127456665\n",
            "Epoch: 1936, Generator Loss: 0.6411952972412109, Discriminator Loss: 1.6081655025482178\n",
            "Epoch: 1937, Generator Loss: 0.6455941200256348, Discriminator Loss: 1.6314626932144165\n",
            "Epoch: 1938, Generator Loss: 0.6381065249443054, Discriminator Loss: 1.6158369779586792\n",
            "Epoch: 1939, Generator Loss: 0.6456729173660278, Discriminator Loss: 1.6072579622268677\n",
            "Epoch: 1940, Generator Loss: 0.649987518787384, Discriminator Loss: 1.6689327955245972\n",
            "Epoch: 1941, Generator Loss: 0.6461527943611145, Discriminator Loss: 1.565750002861023\n",
            "Epoch: 1942, Generator Loss: 0.6483668684959412, Discriminator Loss: 1.6500054597854614\n",
            "Epoch: 1943, Generator Loss: 0.6485314965248108, Discriminator Loss: 1.6161988973617554\n",
            "Epoch: 1944, Generator Loss: 0.6473978757858276, Discriminator Loss: 1.6287362575531006\n",
            "Epoch: 1945, Generator Loss: 0.6488468050956726, Discriminator Loss: 1.6481634378433228\n",
            "Epoch: 1946, Generator Loss: 0.6471266746520996, Discriminator Loss: 1.6251885890960693\n",
            "Epoch: 1947, Generator Loss: 0.6436840891838074, Discriminator Loss: 1.5908581018447876\n",
            "Epoch: 1948, Generator Loss: 0.6492195725440979, Discriminator Loss: 1.6073366403579712\n",
            "Epoch: 1949, Generator Loss: 0.6482626795768738, Discriminator Loss: 1.5750154256820679\n",
            "Epoch: 1950, Generator Loss: 0.6371179819107056, Discriminator Loss: 1.6446936130523682\n",
            "Epoch: 1951, Generator Loss: 0.6544084548950195, Discriminator Loss: 1.6334102153778076\n",
            "Epoch: 1952, Generator Loss: 0.6520885229110718, Discriminator Loss: 1.619052767753601\n",
            "Epoch: 1953, Generator Loss: 0.6508389711380005, Discriminator Loss: 1.6321983337402344\n",
            "Epoch: 1954, Generator Loss: 0.6511875987052917, Discriminator Loss: 1.634764313697815\n",
            "Epoch: 1955, Generator Loss: 0.6540475487709045, Discriminator Loss: 1.6334043741226196\n",
            "Epoch: 1956, Generator Loss: 0.654155433177948, Discriminator Loss: 1.6145964860916138\n",
            "Epoch: 1957, Generator Loss: 0.6534972786903381, Discriminator Loss: 1.5732710361480713\n",
            "Epoch: 1958, Generator Loss: 0.6598106026649475, Discriminator Loss: 1.5832538604736328\n",
            "Epoch: 1959, Generator Loss: 0.6545242667198181, Discriminator Loss: 1.6213607788085938\n",
            "Epoch: 1960, Generator Loss: 0.6603838801383972, Discriminator Loss: 1.5815366506576538\n",
            "Epoch: 1961, Generator Loss: 0.6539797186851501, Discriminator Loss: 1.6255651712417603\n",
            "Epoch: 1962, Generator Loss: 0.6551851034164429, Discriminator Loss: 1.6681205034255981\n",
            "Epoch: 1963, Generator Loss: 0.65157550573349, Discriminator Loss: 1.602130651473999\n",
            "Epoch: 1964, Generator Loss: 0.6498141884803772, Discriminator Loss: 1.6046115159988403\n",
            "Epoch: 1965, Generator Loss: 0.6520034670829773, Discriminator Loss: 1.6020467281341553\n",
            "Epoch: 1966, Generator Loss: 0.6574442982673645, Discriminator Loss: 1.5798293352127075\n",
            "Epoch: 1967, Generator Loss: 0.6527938842773438, Discriminator Loss: 1.6517431735992432\n",
            "Epoch: 1968, Generator Loss: 0.6546083092689514, Discriminator Loss: 1.6255950927734375\n",
            "Epoch: 1969, Generator Loss: 0.6526427268981934, Discriminator Loss: 1.5957614183425903\n",
            "Epoch: 1970, Generator Loss: 0.6581749320030212, Discriminator Loss: 1.6084855794906616\n",
            "Epoch: 1971, Generator Loss: 0.6546540856361389, Discriminator Loss: 1.5959855318069458\n",
            "Epoch: 1972, Generator Loss: 0.6539710164070129, Discriminator Loss: 1.5776625871658325\n",
            "Epoch: 1973, Generator Loss: 0.6556568145751953, Discriminator Loss: 1.6415863037109375\n",
            "Epoch: 1974, Generator Loss: 0.6558828949928284, Discriminator Loss: 1.6174284219741821\n",
            "Epoch: 1975, Generator Loss: 0.6526843905448914, Discriminator Loss: 1.6208572387695312\n",
            "Epoch: 1976, Generator Loss: 0.6447165608406067, Discriminator Loss: 1.6345770359039307\n",
            "Epoch: 1977, Generator Loss: 0.6538957953453064, Discriminator Loss: 1.6503320932388306\n",
            "Epoch: 1978, Generator Loss: 0.6530653834342957, Discriminator Loss: 1.580855369567871\n",
            "Epoch: 1979, Generator Loss: 0.6594002842903137, Discriminator Loss: 1.6303014755249023\n",
            "Epoch: 1980, Generator Loss: 0.6576257944107056, Discriminator Loss: 1.6405789852142334\n",
            "Epoch: 1981, Generator Loss: 0.6554898023605347, Discriminator Loss: 1.6009682416915894\n",
            "Epoch: 1982, Generator Loss: 0.6556205153465271, Discriminator Loss: 1.655796766281128\n",
            "Epoch: 1983, Generator Loss: 0.6558471918106079, Discriminator Loss: 1.6093695163726807\n",
            "Epoch: 1984, Generator Loss: 0.6546164155006409, Discriminator Loss: 1.613748550415039\n",
            "Epoch: 1985, Generator Loss: 0.6542171835899353, Discriminator Loss: 1.6093980073928833\n",
            "Epoch: 1986, Generator Loss: 0.6668943762779236, Discriminator Loss: 1.608987808227539\n",
            "Epoch: 1987, Generator Loss: 0.6544974446296692, Discriminator Loss: 1.6112470626831055\n",
            "Epoch: 1988, Generator Loss: 0.6623495817184448, Discriminator Loss: 1.606998085975647\n",
            "Epoch: 1989, Generator Loss: 0.6614161133766174, Discriminator Loss: 1.5976201295852661\n",
            "Epoch: 1990, Generator Loss: 0.6475855708122253, Discriminator Loss: 1.629447102546692\n",
            "Epoch: 1991, Generator Loss: 0.6543802618980408, Discriminator Loss: 1.5875195264816284\n",
            "Epoch: 1992, Generator Loss: 0.6524730324745178, Discriminator Loss: 1.6025962829589844\n",
            "Epoch: 1993, Generator Loss: 0.6493362784385681, Discriminator Loss: 1.5991889238357544\n",
            "Epoch: 1994, Generator Loss: 0.645079493522644, Discriminator Loss: 1.599306344985962\n",
            "Epoch: 1995, Generator Loss: 0.6447820663452148, Discriminator Loss: 1.6743837594985962\n",
            "Epoch: 1996, Generator Loss: 0.6398566365242004, Discriminator Loss: 1.6614418029785156\n",
            "Epoch: 1997, Generator Loss: 0.6429937481880188, Discriminator Loss: 1.6341878175735474\n",
            "Epoch: 1998, Generator Loss: 0.6486613154411316, Discriminator Loss: 1.6576141119003296\n",
            "Epoch: 1999, Generator Loss: 0.6412960886955261, Discriminator Loss: 1.5965783596038818\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "ZjJsERcGUh7B",
        "outputId": "d898abe1-24f4-43e8-81d9-73839b41fff4"
      },
      "source": [
        "plt.title('Generator Loss')\n",
        "plt.plot(gen_losses, 'r-')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "EDgTQSF7UjXN",
        "outputId": "17206d8a-c8b3-4012-96ff-59539192169d"
      },
      "source": [
        "plt.title('Discriminator Loss')\n",
        "plt.plot(disc_losses, 'b-')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yp6xoYHic9FR"
      },
      "source": [
        "## Using the Discriminator for Classification\n",
        "We will now evaluate the performance of discriminator on the original training data as a classifier. We will check both the classification accuracy and Area Under ROC Curve as the metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEH1mO3eUm1C",
        "outputId": "86b31904-00ba-43b2-8f63-970447bc3e90"
      },
      "source": [
        "_, train_predictions = discriminator(tf.convert_to_tensor(x_train))\n",
        "train_predictions.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([100, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sunWHDnYUoGP",
        "outputId": "311efa9a-137e-4939-a5de-2333ed42e2f1"
      },
      "source": [
        "binary_accuracy = tf.keras.metrics.BinaryAccuracy()\n",
        "binary_accuracy.update_state(tf.one_hot(tf.squeeze(y_train), depth=2), train_predictions)\n",
        "print('Training Accuracy: %.4f %s' % (binary_accuracy.result().numpy()*100, '%'))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy: 88.0000 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "501qLgvRUpSl",
        "outputId": "881f06a8-a67f-4b7b-ebae-0e66cd9e7d4e"
      },
      "source": [
        "fpr, tpr, _ = roc_curve(y_train, tf.argmax(train_predictions,1).numpy())\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.figure()\n",
        "lw = 2\n",
        "plt.plot(fpr, tpr, color='darkorange',\n",
        "lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd3wUdfrA8c+TntATikgXkd4kAooCForA6Z2igIrlOBERG/6wgnJYzoKoSLWicpZTD+WUIqjYEAWkSBMREUInQEJIIeX5/TGTsMSUDWR3s8nzfr3yyu7M7Myzs7vzzHzbiKpijDHGFCYk0AEYY4wp2yxRGGOMKZIlCmOMMUWyRGGMMaZIliiMMcYUyRKFMcaYIlmiKOdEZL2I9Ax0HGWFiDwoIq8EaNuzROSxQGy7tInItSLy2Um+1r6TQcYShR+JyDYRSRORFBHZ4x44Kvtym6raWlWX+HIbuUQkUkT+JSLb3ff5q4iMERHxx/YLiKeniCR4TlPVJ1T1Hz7anojIHSKyTkSOikiCiLwvIm19sb2TJSLjRWT2qaxDVf+tqr292NafkuPJfidFJMKN/Vd3/24TkddEpHFJ12VKxhKF//1FVSsDHYCOwAMBjqfERCSskFnvAxcD/YAqwFBgOPCCD2IQESlr398XgDuBO4BY4CzgI6B/aW+oiM/A5wK47Q+Ay4BrgGpAe2AlzneuRAK5/4KSqtqfn/6AbcAlHs+fBj71eN4VWAocBtYAPT3mxQKvA7uAQ8BHHvMGAKvd1y0F2uXfJnA6kAbEeszrCBwAwt3nfwc2uutfCDTyWFaB24Bfgd8LeG8XA+lAg3zTuwDZwJnu8yXAv4AfgWTg43wxFbUPlgCPA9+57+VM4CY35iPAVuAWd9lK7jI5QIr7dzowHpjtLtPYfV83ANvdffGQx/aigTfc/bERuBdIKOSzbea+z85FfP6zgKnAp268PwBNPea/AOxw98tK4AKPeeNxDpSz3fn/ADoD37v7ajcwBYjweE1rYBFwENgLPAj0BY4Bme4+WeMuWw141V3PTuAxINSdd6O7z58DEt15NwLfuvPFnbfPje1noA3OSUKmu70U4H/5fwdAqBvXb+4+WUm+75C73CXu5/mneUX8vgr6rIe5n/XXwHxgVL51rAGucB+38Nh/vwBXB/oYEqi/gAdQkf7y/UDquz+oF9zn9dwfYT+cK71e7vNa7vxPgfeAGkA40MOd3tH9gXZxf3Q3uNuJLGCbXwA3e8TzDDDDfXw5sAVoCYQBY4GlHsuq+6OJBaILeG9PAl8V8r7/4PgBfIl7IGqDczD/0OPHXNw+WOL+yFu7MYbjnK03xTlY9QBSgbPd5XuS78BeyMHjZZyk0B7IAFp6vid3n9cH1uZfn8d6RwB/FPP5z3LfT2c3/n8D73rMvw6Ic+fdA+wBojzizgT+6u6baKATTmINc9/LRuAud/kqOAf9e4Ao93mX/PvAY9tzgJnuZ1IbJ5HnfmY3AlnA7e62ojkxUfTBOcBXdz+HlkBdj/f8WBG/gzE4v4Pm7mvbA3El+X4VtN4iPus33fcYDVwPfOexfCucpBvpLrMD50QkjOMnVa0CfRwJxF9Zu3SvCD4SkSM4X8J9wCPu9OuAeao6T1VzVHURsALoJyJ1gUuBEap6SFUzVfUr93XDgZmq+oOqZqvqGzgHu64FbPttYAg4RTfAYHcaOAe6f6nqRlXNAp4AOohII4/X/0tVD6pqWgHrrolzYCrIbnd+rrdUdZ2qHgXGAVeLSGhR+8DjtbNUdb2qZrn74VNV/U0dXwGfARcUEkdh/qmqaaq6BueMsr07/WrgCXefJwCTi1hHXBHv39McVf3R3cf/ximCBEBVZ6tqovvensU5YDX3eO33qvqRu2/SVHWlqi5zl9+Gc6Dv4S47ANijqs+qarqqHlHVHwoKSETq4Ozju1T1qKruw7lCGOyx2C5VfdHdVv7PPxMnEbUAxP0OebMvwLkyGquqv7if4RpVTSxgOW/3b3HGu+8xDSc5en7HrwX+q6oZOPtvm6q+7r7nVTgnNVeVQgxBxxKF//1VVavgnO224PgBtBFwlYgczv0DzgfqAg2Ag6p6qID1NQLuyfe6BjjFLPl9CJzrJp7uOMUy33is5wWPdRzEOcOr5/H6HUW8rwNurAWp684vaD1/4FwZ1KTofVBgDCJyqYgsE5GD7vL9ODEpeWOPx+NUILeBwen5tlfU+0+k8PfvzbYQkf8TkY0ikuS+l2qc+F7yv/ezROQTt2FEMk5yz12+AU5xjjca4XwGuz32+0ycK4sCt+1JVb/AKfaaCuwTkZdEpKqX2/Y2Tm/3b3Hy3oeqHsG5Us9NiENwkjc4+6RLvu/itcBppRBD0LFEESDu2e8sYKI7aQfOmXZ1j79KqvqkOy9WRKoXsKodwOP5Xhejqu8UsM1DOGfcg3AqBN9VVfVYzy351hOtqks9V1HEW1qM88Nq4DlRRLrgHAy+8JjsuUxDnDPSA8Xsgz/FICKROMlvIlBHVasD83ASXHHxemM3TpFTQXHn9zlQX0TiT2ZDInIBTh3I1UAN970kcfy9wJ/fz3RgE9BMVavilPXnLr8DOKOQzeVfzw6cq9CaHvu9qqq2LuI1J65QdbKqdsIpvjkLp0ip2Ne5225azDLgfL86i0j9IpY5CsR4PC/ooJ4/nneAISJyLk4R3ZcecX2V77tYWVVv9SLWcscSRWA9D/QSkfY4lZR/EZE+IhIqIlFu88767mX8fGCaiNQQkXAR6e6u42VghIh0cVsCVRKR/iJSpZBtvo1TNjuQ48VOADOAB0SkNYCIVBMRry+zVXUxzsHyQxFp7b6Hru77mq6qv3osfp2ItBKRGGAC8IGqZhe1DwrZbARO8cx+IEtELgU8m2zuBeJEpJq37yOf/+DskxoiUg8YVdiC7vubBrzjxhzhxj9YRO73YltVcOoB9gNhIvIwUNxZeRWcyuMUEWkBeB7EPgHqishd4jRbruImbXD2S+PcVmPu9+sz4FkRqSoiISLSVER64AUROcf9/oXjHKzTca5Wc7dVWMICeAV4VESaud/fdiISl38h9/u1CJgjIp1EJMx9TyNE5O/uYquBwe7vIx7nO16ceThXDxOA91Q1N+5PgLNEZKi7vnD3fbb0Yp3ljiWKAFLV/TiVaw+r6g6cCuUHcQ4WO3DOynI/o6E4Z96bcOo27nLXsQK4GefS/xBOhfSNRWx2Lk4LnT1umXxuLHOAp4B33WKMdTj1IiVxJc4Z2QKcVi6zcVrS3J5vubdwrqb24JzF3eHGUNw+OIFbdHAHzgH9EM5V0lyP+Ztwzhi3usUHBRXHFWUCkAD8jnNG+wHOmXdh7uB4EcxhnCKVvwH/82JbC3H222ac4rh0ii7qAvg/nPd8BOeE4b3cGe6+6QX8BWc//wpc6M5+3/2fKCI/uY+vx0m8G3D25Qd4X9RT1d3+ITf2RJyGEuB8/q3c/f9RAa+dhPP5fYaT9F7FqWguyECcA/t7OFdb64B4nM8GnPqupm4c/+TEE6ECufUR/8VpVfW2x/QjOCcdg3FaGu7B+X1EFrfO8kiOlzwY43sisgSnJUpAekefChG5FRisql6daRtTXtgVhTGFEJG6ItLNLYppjtPUdE6g4zLG36x3ojGFi8Bp/dMEpyjpXZx6CGMqFCt6MsYYUyQrejLGGFOkoCt6qlmzpjZu3DjQYRhjTFBZuXLlAVWtdTKvDbpE0bhxY1asWBHoMIwxJqiIyB8n+1orejLGGFMkSxTGGGOKZInCGGNMkSxRGGOMKZIlCmOMMUWyRGGMMaZIPksUIvKaiOwTkXWFzBcRmSwiW0RkrYic7atYjDHGnDxfXlHMwrmRe2EuxRnuuhnO7Tyn+zAWY4ypOFThWAok/wF7V3Hs10WntDqfdbhT1a9FpHERi1wOvOneYW2ZiFQXkboluNeuMcaUb6qQlQppByH9IKQnOv/T3P8nPM43LScTgDH/68WqXad2F9lA9syux4k3Zklwp/0pUYjIcJyrDho2bOiX4IwxplRlphV8QM9/4M+fDLKLuldWEcJiICqWNmdFMfm7xqcUelAM4aGqLwEvAcTHx9twt8aYwMnK8P7s3nNeVtrJbS8sCqLiICrW+YvOfVzAtOg4NmwTflqfznU3dALgelV6PJhEkyYTTvotBzJR7OTEm9XXd6cZY4zvZR/zOMAXdHAv5Mw/K/XkthcacfzgHu1xkC9ommcyCC/szrAnSk3N5LHHvuaZZ5YSGip07daEM8+MRURo3Lj6ycXsCmSimAuMEpF3gS5AktVPGGNKLCeriIO9Oy2tgAN/ZsrJbS8krICDe+7zgg78cc70sBgQKd337po//1duu20ev/9+GIBhwzoRF+ddgvGGzxKFiLwD9ARqikgC8AgQDqCqM3Bukt4P2AKkAjf5KhZjTBDIyYL0wwWfyecd7AtIBseST257Elr0mXxh88Ir++yAX1I7dyZz110L+eCDDQC0a1eHGTP6c+65DYp5Zcn4stXTkGLmK3Cbr7ZvjAmQnGzISCpZC530g5Bx+OS2JyEQWeOEcvqiyvDzpkVULTMH/JN1223z+PjjX4iJCWfChJ7ceWdXwsJKv9dDUFRmG2MCQHPcA34JWuikJzpXBZxMmxOBqBolL8OPrOokiwoiKysnLxk89dQlhIeH8uyzvWnYsJrPtmmJwpjyTtUpninJ2X1aImQccpLFyYisXrKz+6g4iKwGIaGl+97LkaSkdMaO/YLNmw+yYMG1iAjNm9fk/fev8vm2LVEYEyxUnQpYb8/uPedp9sltM6Jqyc7uo2IhqrpT4WtKhary/vsbuOuuBezenUJoqLB69R46djy1TnQlYZ+mMf6W19u2qErbguYdzOttW2Lhlb1voZM7L7IGhIaX7ns3JfLbbwcZNWo+CxZsAeDcc+szY8YA2rWr49c4LFEYcyoy0woupy9qyIX0RKcN/8kIr+Rdx6vcJplRcU65f2hE6b5v43MTJy5l3LgvSU/Ponr1KJ566hL+8Y+zCQnxfwW8JQpjALLSS1aGnzstK/3kthcW7UWlbe7B3uOAHxZVuu/blFmpqZmkp2cxdGg7Jk7sTe3alQIWiyUKU77k9rb1qoWOx+NT7W3rTft7zzN/L3vbmopj//6j/PJLIuef74xnd9993ejZszHduzcKcGSWKExZlZ3pcYD3ooVO7rzMoye3vZDwkrXQyZ0WFh30bfFNYOXkKK+9top7711EWFgImzaNIjY2msjIsDKRJMAShfG1nCxIP1SyFjrpiXDsyMltLySsZC108nrbVrIDvvG7dev2MWLEJ3z3nTOQdq9eZ5CamklsbNm64rREYbyTk+30nC1whMwiBlTLSDq57UmIFy108lfaxkJEFTvgmzLv6NFjTJjwFZMmLSMrK4c6dSrx/PN9GTSoNVIGv7+WKCqa3N62RQ2aVtCZ/6n2tvW6hY47r4L1tjUVy8CB77NgwRZEYOTIeB5//GKqVy+7DRUsUQQr1ROHVyiy/b3Hmf+p9LbNHV7BqxY67rzIanbANyaf++7rxt69KUyf3p8uXeoHOpxiWaIINFWnPN6b9vcnHPgPnXxv28hqJWuhEx3nDMlgwysYU2JZWTm8+OIPbNt2mBdeuBSAnj0bs2LF8ID0iTgZlij8Zd0sSPiq4GKdnKyTW2dElZK10ImKda4KbHgFY/zixx93csstn7B69R4Ahg/vROvWtQGCJkmAJQr/OLITFhZxu43wSiU7u899bMMrGFMmHT6czoMPfs6MGStQhUaNqjFlSr+8JBFsLFH4w44vnf+ndYbOD5x45h9ZA8IiAxufMabUvPvuOu66awF79x4lLCyEe+45l3HjulOpUvAOo2KJwh92LHH+nzUQmv01oKEYY3zrs89+Y+/eo3Tr1oDp0/vTtq1/B/DzBUsU/pCwxPnf4MKAhmGMKX0ZGVns3HmEM86oAcDTT/figgsacsMNHYKqHqIo1m7R15J3wOHfnHH9a3cIdDTGmFL0xRe/067dDPr3f5tjx5xWiDVrxnDTTR3LTZIASxS+l3s1Uf8Ca21kTDmxd28KQ4fO4eKL32Tz5kQAEhKSAxyV79iRy9e2uxXZVuxkTNDLyVFefnkl99//OYcPpxMVFcbYsRcwZkw3IiLKbz8jSxS+llc/0TOQURhjSsHf/vYec+f+AkCfPk2ZOrUfTZvGBjgq37OiJ19K/gOSfnd6Qtey+gljgt0VV7TgtNMq8957A5k//9oKkSTArih8K7dZbL3uNvyFMUFo7txfSEhIZuTIcwC4/vr2XHFFS6pUqVh9nyxR+FJuorBiJ2OCyvbtSdxxx3w+/vgXIiND6dv3TM44owYiUuGSBFii8K28RGEV2cYEg8zMbCZP/oFHHlnC0aOZVKkSwWOPXUSjRtUCHVpAWaLwlaRtkLzNGXW1VrtAR2OMKcayZQnccssnrF27F4CrrmrFc8/1oV69qgGOLPAsUfhK7tVEfaufMCYYjBv3JWvX7qVJk+pMmdKPfv2aBTqkMsMSha/ssP4TxpRlqsqRI8eoWtWpc5gy5VLefHMNDz3UnZgYG5nZkzWP9QVVq8g2pgz75ZcDXHLJW1xxxXuoOrf4bd68Jo8/frEliQLYFYUvJG+DI9udmwRZ/YQxZUZ6ehb/+tc3PPnkdxw7lk1cXDTbth2mSZMagQ6tTLNE4Qu5w3bU72H3izamjFi06DdGjpzHli0HAfj73zvw9NO9iIuLCXBkZZ9Pj2Ii0ldEfhGRLSJyfwHzG4rIlyKySkTWikg/X8bjNzZshzFlhqry979/TO/es9my5SCtWtXi669v5NVXL7ck4SWfXVGISCgwFegFJADLRWSuqm7wWGws8B9VnS4irYB5QGNfxeQXqjYQoDFliIjQuHF1oqPDePjhHowefW65HsDPF3xZ9NQZ2KKqWwFE5F3gcsAzUSiQ20i5GrDLh/H4R9JWSElw7nNds02gozGmQlq9eg+7dx/h0kudJq733deNoUPbWV3ESfJl0VM9YIfH8wR3mqfxwHUikoBzNXF7QSsSkeEiskJEVuzfv98XsZaevNZOVj9hjL8dOZLB6NEL6dTpJW644SMOHkwDIDIyzJLEKQj0kWwIMEtV6wP9gLdE/nx0VdWXVDVeVeNr1arl9yBLJLf/RP2eAQ3DmIpEVZkzZyOtWk3jueeWAXDNNW0JDw/0Ia588GXR006ggcfz+u40T8OAvgCq+r2IRAE1gX0+jMt3rP+EMX73xx+HGTVqPp98shmA+PjTmTlzAGefXTfAkZUfvky3y4FmItJERCKAwcDcfMtsBy4GEJGWQBRQxsuWinD4N0jZCdE1oWbrQEdjTLmnqlx55X/45JPNVK0ayZQpl7Js2TBLEqXMZ1cUqpolIqOAhUAo8JqqrheRCcAKVZ0L3AO8LCJ341Rs36i53SSD0Q7rP2GMP+TkKCEhgogwcWJvZsxYwXPP9aFu3SqBDq1c8mmHO1Wdh1NJ7TntYY/HG4BuvozBr6zYyRifSkxM5f77FwPw8suXAdCzZ2N69mwcwKjKPzvtLS2qNhCgMT6iqrzxxmpatJjKK6+s4s0315KQkBzosCoMG8KjtBz6FY7uhuhaENcq0NEYU25s3LifW2/9lK+++gNwriCmT+9P/fp2nwh/sURRWjyH7RAJZCTGlAuqysMPf8lTT31HZmYONWvG8OyzvRk6tB1ivzG/skRRWvKG7egZ0DCMKS9EhJ07j5CZmcPNN5/Nk09eQmxsdKDDqpAsUZQGVRsI0JhSsGvXEQ4cSKVduzoAPP10L4YN60i3bg0DHFnFZpXZpeHQZji6B2JqQ2zLQEdjTNDJzs5hypQfadlyKoMHf8CxY9kA1KwZY0miDLAritLgOWyHlZ0aUyI//bSbW275hBUrnDFBu3dvRHJyBjVr2hDgZYUlitKQ23+ioTWLNcZbyckZjBv3BVOmLCcnR6lfvyqTJ/flr39tYZXVZYzXiUJEYlQ11ZfBBCXP8Z1sIEBjvKKqdO/+OmvW7CU0VBg9uivjx/ekSpXIQIdmClBsHYWInCciG4BN7vP2IjLN55EFi4ObIHUvVDoNYpsHOhpjgoKIcPfdXencuR4rVgzn2Wf7WJIow7y5ongO6IM7oJ+qrhGR7j6NKph4Xk3Y5bIxBTp2LJtJk74nNFQYM8YZtef669tz3XXtCA21NjVlnVdFT6q6I1+ZYbZvwglCO6z/hDFF+eabPxgx4lM2bNhPZGQo11/fnjp1KiMihIbayVUw8CZR7BCR8wAVkXDgTmCjb8MKEnb/CWMKdeBAKvfeu4jXX18NQLNmsUyb1p86dSoHODJTUt4kihHACzi3Md0JfAaM9GVQQSNxA6Tth0p1ocZZgY7GmDJBVZk1azVjxiwiMTGNiIhQHnjgfO6//3yioqyhZTDy5lNrrqrXek4QkW7Ad74JKYh4Xk1Y/YQxeWbP/pnExDQuuqgJ06b1o3nzmoEOyZwCbxLFi8DZXkyrePKG7bD+E6ZiS03NJCkpnbp1qyAiTJvWj+XLd3HttW2tT0Q5UGiiEJFzgfOAWiIy2mNWVZw71lVsmmP1E8YA8+f/ym23zeOMM2qwaNFQRITmzWvaVUQ5UtQVRQRQ2V3G8/6CycBAXwYVFBI3QNoBqHw6VD8z0NEY43c7dyZz110L+eCDDQBUqRJJYmKaDb1RDhWaKFT1K+ArEZmlqn/4MabgkHc1caHVT5gKJTs7h6lTlzN27BccOXKMSpXCmTDhQu64owthYdYnojzypo4iVUSeAVoDUbkTVfUin0UVDDwHAjSmgsjJUXr0mMV33+0A4K9/bcELL/SlYcNqAY7M+JI36f/fOMN3NAH+CWwDlvswprJPc2DHV85jGwjQVCAhIULv3k1p0KAqH388mDlzBlmSqABEVYteQGSlqnYSkbWq2s6dtlxVz/FLhPnEx8frihUrArHp4/avhTfbQ+X6MHy7FT2ZcktV+c9/1hMWFsKVVzr3gs/IyCIzM4fKlSMCHJ0pCfdYHn8yr/Wm6CnT/b9bRPoDu4DYk9lYuWH9J0wF8NtvBxk5ch6fffYbtWrFcNFFTahRI5rIyDAibfy+CsWbRPGYiFQD7sHpP1EVuMunUZV1nhXZxpQzGRlZPPPMUh5//BvS07OoUSOKxx+/iGrVoop/sSmXik0UqvqJ+zAJuBDyemZXTJoDCW79hPWfMOXMkiXbuPXWT9m06QAAQ4e2Y+LE3tSuXSnAkZlAKqrDXShwNc4YTwtUdZ2IDAAeBKKBjv4JsYzZ/zOkH4QqDaFak0BHY0ypyc7OYeRIJ0k0bx7H9On9ufBC+46boq8oXgUaAD8Ck0VkFxAP3K+qH/kjuDIpb9iOnlY/YYJeTo6Snp5FTEw4oaEhTJ/en6+//oN77+1GZKQN4GccRX0T4oF2qpojIlHAHqCpqib6J7Qyarvdf8KUDz//vJcRIz6lRYs4Xn31cgB69GhMjx6NAxuYKXOKShTHVDUHQFXTRWRrhU8SmgM7v3YeW0W2CVJHjx5jwoSvmDRpGVlZOfz++yEOHUqjRo3oQIdmyqiiEkULEVnrPhagqftcAM3tU1Gh7FsD6YegaiOo1jjQ0RhTYv/73y+MGjWf7duTEIGRI+N5/PGLqV7dWjSZwhWVKFr6LYpg4Vk/YUwQycrKYdCgD/jvf52bU3bocBozZw6gc+d6AY7MBIOiBgW0gQDzs/4TJkiFhYVQrVoklStH8OijFzJqVGcbwM94zaffFBHpKyK/iMgWEbm/kGWuFpENIrJeRN72ZTynJCfb+k+YoPLDDwn88ENC3vNnnunFxo23cdddXS1JmBLxWfs3tx/GVKAXkAAsF5G5qrrBY5lmwANAN1U9JCK1fRXPKdu/BjKSnL4TVRsFOhpjCnX4cDoPPLCYmTNX0qJFTVavHkFERChxcXafCHNyvEoUIhINNFTVX0qw7s7AFlXd6q7jXeByYIPHMjcDU1X1EICq7ivB+v3LhhU3ZZyq8s476xg9eiF79x4lLCyEyy5rTnZ2DnZTSnMqik0UIvIXYCLOHe+aiEgHYIKqXlbMS+sBOzyeJwBd8i1zlruN73C+yeNVdYGXsfuX3fbUlGG//prIyJHzWLx4KwDdujVgxowBtGlTdi/STfDw5opiPM7VwRIAVV0tIqXVrz8MaAb0BOoDX4tIW1U97LmQiAwHhgM0bNiwlDZdAjnZkJDbf6Kn/7dvTBEyM7O56KI3SUhIJjY2mqefvoSbbupISIiNHGBKh1fDjKtqkpw4XEXRN7Fw7MQZAiRXfXeapwTgB1XNBH4Xkc04ieOEGyOp6kvAS+Dcj8KLbZeufavgWDJUOwOqBiBRGVMAVUVECA8P5fHHL+LLL7fx9NOXUKuWDeBnSpc3TR/Wi8g1QKiINBORF4GlXrxuOdBMRJqISAQwGJibb5mPcK4mEJGaOEVRW70N3m+sWawpQ/buTWHo0Dk89tjXedOuv749r79+uSUJ4xPeJIrbce6XnQG8jTPceLH3o1DVLGAUsBDYCPxHVdeLyAQRya3fWAgkisgG4EtgTJkcJsQ62pkyICdHmTlzBS1aTGX27LVMmrSMI0cyAh2WqQC8uRXq2ar6k5/iKZbfb4WakwVTY+HYERi+A6rU99+2jXGtWbOHESM+Zdkyp19E375nMnVqP844o0aAIzPBwte3Qn1WRE4DPgDeU9V1J7OhoLVvlZMkqp9pScL4XWZmNg888DnPP7+M7Gylbt3KvPBCXwYObIXYMPfGT4otelLVC3HubLcfmCkiP4vIWJ9HVlbYsOImgMLCQli1ag85Ocrtt3dm48bbuOqq1pYkjF951eFOVffg3LzoS+Be4GHgMV8GVmZY/YTxs+3bk8jOzqFJkxqICDNm9CcpKYP4+NMDHZqpoIq9ohCRliIyXkR+BnJbPFWMMpicLEj4xnlsPbKNj2VmZjNx4lJatpzKzTf/j9z6w2bN4ixJmIDy5oriNeA9oI+q7vJxPGXL3pWQmQI1mkEVG47Z+M733+9gxIhPWbt2LwCxsdGkpmZSqVJEgCMzxotEoarn+iOQMsn6TxgfO3QojfvvX8xLLzkNC5s0qc7Uqf249NJmAY7MmH+YcB4AACAASURBVOMKTRQi8h9VvdotcvJsQ1tx7nCXmyis2Mn4QEZGFh06zGT79iTCw0MYM+Y8HnqoOzEx4YEOzZgTFHVFcaf7f4A/AilzsjNhp1s/YRXZxgciI8MYNqwjn3/+O9On96dVq1qBDsmYAhVama2qu92HI1X1D88/YKR/wgugvSsh8yjUaA6V6wY6GlMOpKdn8cgjX/L22z/nTXvwwQtYsuQGSxKmTPNmCI9eBUy7tLQDKXN2WP8JU3oWLfqNtm2nM2HC19x990LS0jIBp5+E9YkwZV1RdRS34lw5nCEiaz1mVQG+83VgAWcV2aYU7NmTwujRC3nnHWdAg9atazFjxgCio60ewgSPouoo3gbmA/8CPO93fURVD/o0qkDLzoSd3zqPG/QIbCwmKGVn5zBz5koefPBzkpIyiI4O45FHenD33ecSEWF3mzPBpahEoaq6TURuyz9DRGLLdbLYsxyyUiG2BVQ6LdDRmCCUna28+OKPJCVl0K9fM6ZMuZQmTWwAPxOciruiGACsxGke61mQqsAZPowrsPKG7bBiJ+O9I0cyyM5WqlePIiIilJdf/gt796ZwxRUtrR7CBLVCE4WqDnD/l9ZtT4OH3R/blICqMmfOJu64Yz59+jTl1VcvB+D88+1uiKZ88Gasp24iUsl9fJ2ITBKR8vsLyD4GO926+vpWP2GKtm3bYS677F2uvPI/7Nx5hHXr9pOenhXosIwpVd40j50OpIpIe+Ae4DfgLZ9GFUi59RNxraBSnUBHY8qozMxsnnrqW1q1msonn2ymatVIpky5lKVL/05UlFeDMhsTNLz5RmepqorI5cAUVX1VRIb5OrCAye0/YcN2mEKkpmbStesr/PzzPgAGD27DpEm9qVu3SoAjM8Y3vEkUR0TkAWAocIGIhADltxF4bv1EQ6vINgWLiQknPv50UlMzmTatP717Nw10SMb4lDeJYhBwDfB3Vd3j1k8849uwAiQrA3YtdR7X7x7YWEyZoaq8+eYamjaNzaugfu65PkREhFrHOVMheHMr1D3Av4FqIjIASFfVN30eWSDs+RGy0iCuNcTUDnQ0pgzYuHE/F174Bjfe+DHDh/+PY8eyAahWLcqShKkwvGn1dDXwI3AVcDXwg4gM9HVgAWHDdhhXWlomY8d+Qfv2M/jqqz+oVSuGBx44n/Bwb9p/GFO+eFP09BBwjqruAxCRWsBi4ANfBhYQNhCgARYs2MJtt81j69ZDANx889k8+eQlxMZGBzgyYwLDm0QRkpskXIl416w2uGSlw+7vncfWf6LCSkk5xtChczhwIJU2bWozY0Z/unUrv92GjPGGN4ligYgsBN5xnw8C5vkupADZ86OTLGq2hZiagY7G+FF2dg45OUp4eCiVK0fwwgt9SUhI5u67uxIebgP4GePNPbPHiMgVwPnupJdUdY5vwwqA7VbsVBGtXLmLW275hMsvb864cc6V5DXXtA1wVMaULUXdj6IZMBFoCvwM/J+q7vRXYH5nAwFWKMnJGYwb9wVTpiwnJ0dJTs7g/vvPtysIYwpQVF3Da8AnwJU4I8i+6JeIAiErHXbl1k9Y/4nyTFV5//31tGgxhcmTf0QERo/uyk8/3WJJwphCFFX0VEVVX3Yf/yIiP/kjoIDYvQyyM6BWO4iOC3Q0xkeOHMlg0KAPmD9/CwBdutRjxowBdOhg9xwxpihFJYooEenI8ftQRHs+V9Xykzis/0SFULlyBBkZ2VSrFsmTT17C8OGdCAmx+0QYU5yiEsVuYJLH8z0ezxW4yFdB+Z0NBFhuff31H9StW5lmzeIQEV577TKiosKoU6dyoEMzJmgUdeOiinF6nZnmFD0hVj9Rjhw4kMq99y7i9ddXc/HFTVi0aCgiQqNG1QMdmjFBxwbO373MuVlRrQ4QHRvoaMwpyslRZs1azZgxizh4MI2IiFAuuKAh2dlKWJgVMxlzMnzaw1pE+orILyKyRUTuL2K5K0VERSTel/EUyIbtKDfWr99Hz56zGDZsLgcPpnHxxU34+edbeeSRnoSFlb/BBIzxF59dUYhIKDAV6AUkAMtFZK6qbsi3XBXgTuAHX8VSJKvILheSktLp2vVVUlKOUbt2JSZN6s0117RFxK4ijDlVxSYKcX5p1wJnqOoE934Up6nqj8W8tDOwRVW3uut5F7gc2JBvuUeBp4AxJQ3+lGWmwp4fcOonLvD75s2pU1VEhGrVorjvvm7s3JnME09cTI0aNoCfMaXFm+vxacC5wBD3+RGcK4Xi1AN2eDxPcKflEZGzgQaq+mlRKxKR4SKyQkRW7N+/34tNe2nX9079RO2OEFWj9NZrfG7nzmQGDvwPs2evzZv20EMXMH36AEsSxpQybxJFF1W9DUgHUNVDQMSpbti9peok4J7illXVl1Q1XlXja9WqdaqbPi5v2I6epbdO41NZWTm88MIyWrSYyocfbuSRR5aQnZ0DYMVMxviIN3UUmW59g0Le/ShyvHjdTqCBx/P67rRcVYA2wBL3B34aMFdELlPVFV6s/9TZQIBBZfnynYwY8Sk//bQbgL/+tQWTJ/clNNQqqo3xJW8SxWRgDlBbRB4HBgJjvXjdcqCZiDTBSRCDce69DYCqJgF543mLyBKcgQf9kyQyjzpDi0sI1LP6ibLs6NFj3HffYqZNW44qNGxYjRdfvJTLLmse6NCMqRC8GWb83yKyErgYZ/iOv6rqRi9elyUio4CFQCjwmqquF5EJwApVnXuKsZ+aXd9DTibU6QRR1gmrLAsLC2Hx4q2EhAijR5/LI4/0oFKlUy79NMZ4yZtWTw2BVOB/ntNUdXtxr1XVeeS7yZGqPlzIsj2LW1+psmE7yrTffjtI9epRxMXFEBkZxltv/Y2oqDDatq0T6NCMqXC8KXr6FKd+QoAooAnwC9Dah3H5Xm7/iYbWf6IsycjI4plnlvL4499w7bVteeWVywA455x6xbzSGOMr3hQ9nXC7L7dJ60ifReQPx1I86ifOL3554xdLlmzj1ls/ZdOmA4DTwik7O8cqq40JsBL3zFbVn0Skiy+C8ZtdSyEnC047ByKrBTqaCm/fvqOMGbOIN99cA0Dz5nFMn96fCy9sEuDIjDHgXR3FaI+nIcDZwC6fReQPucVOVj8RcAcOpNKy5VQOHkwjMjKUhx66gHvv7UZkpI1XaUxZ4c2vsYrH4yycOosPfROOn9hAgGVGzZoxXH55cxISkpk2rT9nnmkj+BpT1hSZKNyOdlVU9f/8FI/vHUuBPctBQq1+IgCOHj3GhAlf0b//WXTv3giAadP6ExkZaj2rjSmjCk0UIhLm9oXo5s+AfG7Xd6DZcFpniKwa6GgqlP/97xdGjZrP9u1JfPrpr6xdeyshIUJUlBUzGVOWFfUL/RGnPmK1iMwF3geO5s5U1f/6ODbfyBu2w5rF+suOHUnceecC5szZBEDHjqcxc+YAu1+1MUHCm1O5KCAR5x7Zuf0pFAjORGEDAfpNVlYOkyf/wMMPf8nRo5lUrhzBY49dyG23dbYbCRkTRIpKFLXdFk/rOJ4gcqlPo/KVY0dgzwq3fqJ8laiVRcnJGfzrX99y9GgmV17Zkuef70v9+lbcZ0ywKSpRhAKVOTFB5ArORLHzW6d+om5XiKhS/PKmxA4fTic6OozIyDBiY6OZOXMAkZGh9O9/VqBDM8acpKISxW5VneC3SPwh77anPQMZRbmkqrzzzjruvnsho0adw7hxPQC44oqWAY7MGHOqikoU5a+m0fpP+MTmzYmMHPkpn3/+OwBff7097xalxpjgV1SiuNhvUfhDRjLsXQkhYXC61U+UhvT0LJ566lueeOJbjh3LJjY2mmee6cWNN3awJGFMOVJoolDVg/4MxOd2fguaA6d1gYjKgY4m6O3Zk0L37q/z66/O1+TGGzvwzDO9qFkzJsCRGWNKW8Xp6ZRb7GTDipeKOnUq0aBBNcLCQpg+vT89ejQOdEjGGB+pQIliifPfBgI8KTk5yssvr+TCC5tw1llxiAhvv30FNWpEExERGujwjDE+VDF6PWUkwb6fICQc6p0X6GiCzpo1e+jW7TVGjPiUkSM/RdVpHV2nTmVLEsZUABXjiiLhG6d+ou65EF4p0NEEjZSUY4wfv4Tnn19GdrZy+ulVGDEiPtBhGWP8rGIkCus/UWIffbSJ22+fT0JCMiEhwu23d+axxy6iatXIQIdmjPGzCpIobCDAkti5M5nBgz8gIyObTp3qMmPGAOLjTw90WMaYACn/iSL9MOxb5dRPnH5uoKMpszIzswkLC0FEqFevKo8/fhEREaGMHHmO3bPamAqu/B8Bdn4DKNTtAuHWxr8gS5fuoFOnl5g9e23etHvuOY/bb+9iScIYUwEShRU7FergwTRuueV/dOv2Gj//vI9p01bktWgyxphc5b/oySqy/0RVmT17Lffc8xn796cSHh7Cvfd246GHLrChN4wxf1K+E0XaQdi3GkIjnKaxhr17Uxgy5EO+/HIbAD16NGL69P60bFkrsIEZY8qs8p0o8uonukJ4dKCjKROqV49i9+4UataMYeLEXlx/fXu7ijDGFKl8JwobtgOARYt+4+yz6xIXF0NkZBjvv38VdetWJi7OKveNMcUr35XZFXwgwN27jzBkyIf07j2b++5bnDe9TZvaliSMMV4rv1cUaQdh/1oIjXSKniqQ7OwcZs5cyQMPfE5ycgbR0WE0bx5nNxMyxpyU8psoEr4ir34iLCrQ0fjNTz/tZsSIT1i+fBcA/fs3Y8qUfjRuXD3AkRljglX5TRR5zWIrTrHTtm2H6dz5ZbKzlXr1qjB58qX87W8t7CrCGHNKfJooRKQv8AIQCryiqk/mmz8a+AeQBewH/q6qf5TKxhOWOP8rUP+Jxo2rc9NNHahSJZJ//rMnVarYAH7GmFPns8psEQkFpgKXAq2AISLSKt9iq4B4VW0HfAA8XSobTz3g1E+ERTlDd5RT27Yd5i9/eYevvtqWN+2ll/7CpEl9LEkYY0qNL68oOgNbVHUrgIi8C1wObMhdQFW/9Fh+GXBdqWx559fO/7rnlsv6iczMbCZN+p5//vMr0tKyOHAgle+/HwZgxUzGmFLny0RRD9jh8TwBKOr0fhgwv6AZIjIcGA7QsGHD4rdcjoft+Pbb7YwY8Qnr1+8HYPDgNkya1DvAURljyrMyUZktItcB8UCPguar6kvASwDx8fHFj1pXDgcCPHQojTFjFvHqq6sAaNq0BtOm9ad376YBjswYU975MlHsBBp4PK/vTjuBiFwCPAT0UNWMU95q6n44sM4pcjqt8ymvrqzIyVE+/vgXwsNDuP/+83nggfOJjg4PdFjGmArAl4liOdBMRJrgJIjBwDWeC4hIR2Am0FdV95XKVhO+cv6ffh6EBXeF7qZNB2jSpDqRkWHExcXw739fQcOG1WjRomagQzPGVCA+a/WkqlnAKGAhsBH4j6quF5EJInKZu9gzQGXgfRFZLSJzT3nD5aD/RGpqJg899Dnt2k3n6ae/y5veu3dTSxLGGL/zaR2Fqs4D5uWb9rDH40tKfaNBPhDgggVbGDnyU37//TAABw6kBjgiY0xFVyYqs0tN6j5IXA9h0VA3uOondu06wl13LeD9953Ww23b1mbGjAGcd16DYl5pjDG+Vb4SxY7c+oluzs2KgsTmzYnEx7/EkSPHiIkJZ/z4Htx1V1fCw0MDHZoxxpS3RLHE+R9k/SeaNYvlnHPqUalSOC++eCmNGtkAfsaYsqOcJYrg6D+RnJzBww9/yciR53DWWXGICHPnDqZSpeC5CjLGVBzlJ1Ec3QsHN0JYDJwWH+hoCqSqfPDBBu68cwG7d6ewadMBFixwRi2xJGGMKavKT6LILXaqd36ZrJ/YuvUQo0bNY/78LQB07Vqfp54q/UZfxhhT2spPoiijw4ofO5bNxIlLefTRr0lPz6J69SiefPJibr65EyEhNoCfMabsKz+JooxWZO/YkcSECV+RkZHNtde25dlne1OnTuVAh2WMMV4rH4kiZTcc3AThlaBO4OsnDh1Ko3r1KESEpk1jeeGFvpx5ZiwXX3xGoEMzxpgS89kQHn6VO75TvfMhNHAD5eXkKK+9toozz3yR2bPX5k2/5ZZ4SxLGmKBVPhJFbrPYAA7bsX79Pnr2nMWwYXM5eDAtr9LaGGOCXfkoesqtn2jo//4TqamZPProV0yc+D1ZWTnUrl2J557rw5AhbfweizHG+ELwJ4qUXXBoM4RXhtpn+3XTmzcn0qfPbLZtO4wIjBjRiSeeuJgaNaL9GocxxvhS8CeKvNFiL/B7/USjRtWIigqjffs6zJgxgK5d6/t1+6bsy8zMJCEhgfT09ECHYiqIqKgo6tevT3h46R0Py1Gi6OnzTWVl5TBjxgqGDGlDXFwMkZFhLFhwLfXqVSUsrHxU95jSlZCQQJUqVWjcuDEi1m/G+JaqkpiYSEJCAk2aNCm19Qb/0c1PHe1+/HEnnTu/zO23z+e++xbnTW/UqLolCVOo9PR04uLiLEkYvxAR4uLiSv0KNrivKI7shEO/QkQVqOOb+omkpHQeeugLpk1bjio0bFiNyy9v7pNtmfLJkoTxJ19834I7UeReTdS7AEJK962oKu+9t567717Inj0phIWFMHp0Vx5+uIcN4GeMqVCCu8xku++GFV+zZi9DhnzInj0pnHdeA376aThPPdXLkoQJOqGhoXTo0IE2bdrwl7/8hcOHD+fNW79+PRdddBHNmzenWbNmPProo6hq3vz58+cTHx9Pq1at6NixI/fcc08g3kKRVq1axbBhwwIdRqEyMjIYNGgQZ555Jl26dGHbtm0FLvfcc8/RunVr2rRpw5AhQ/KKjz7//HPOPvtsOnTowPnnn8+WLU4frSlTpvDaa6/5502oalD9derUSfO80lR1Iqq7l2tpyMrKPuH53Xcv0JdfXqnZ2Tmlsn5T8WzYsCHQIWilSpXyHl9//fX62GOPqapqamqqnnHGGbpw4UJVVT169Kj27dtXp0yZoqqqP//8s55xxhm6ceNGVVXNysrSadOmlWpsmZmZp7yOgQMH6urVq/26zZKYOnWq3nLLLaqq+s477+jVV1/9p2USEhK0cePGmpqaqqqqV111lb7++uuqqtqsWbO879HUqVP1hhtuUFXn8+rQoUOB2yzoewes0JM87gZv0VPyDjj8G0RUhdodTnl1X375OyNHzmPmzAF0794IgEmT+pzyeo3J86yP6iru0eKXcZ177rmsXesML/P222/TrVs3evfuDUBMTAxTpkyhZ8+e3HbbbTz99NM89NBDtGjRAnCuTG699dY/rTMlJYXbb7+dFStWICI88sgjXHnllVSuXJmUlBQAPvjgAz755BNmzZrFjTfeSFRUFKtWraJbt27897//ZfXq1VSv7tzZsVmzZnz77beEhIQwYsQItm/fDsDzzz9Pt27dTtj2kSNHWLt2Le3btwfgxx9/5M477yQ9PZ3o6Ghef/11mjdvzqxZs/jvf/9LSkoK2dnZzJs3j9tvv51169aRmZnJ+PHjufzyy9m2bRtDhw7l6NGjgHPWft5553m9fwvy8ccfM378eAAGDhzIqFGjUNU/1SVkZWWRlpZGeHg4qampnH766YBT55CcnAxAUlJS3vSYmBgaN27Mjz/+SOfOnU8pxuIEb6LIrZ+o3/2U6if27TvKmDGLePPNNQBMmvR9XqIwpjzJzs7m888/zyumWb9+PZ06dTphmaZNm5KSkkJycjLr1q3zqqjp0UcfpVq1avz8888AHDp0qNjXJCQksHTpUkJDQ8nOzmbOnDncdNNN/PDDDzRq1Ig6depwzTXXcPfdd3P++eezfft2+vTpw8aNG09Yz4oVK2jT5vgoCC1atOCbb74hLCyMxYsX8+CDD/Lhhx8C8NNPP7F27VpiY2N58MEHueiii3jttdc4fPgwnTt35pJLLqF27dosWrSIqKgofv31V4YMGcKKFSv+FP8FF1zAkSNH/jR94sSJXHLJifeZ2blzJw0aNAAgLCyMatWqkZiYSM2aNfOWqVevHv/3f/9Hw4YNiY6Opnfv3nkJ/JVXXqFfv35ER0dTtWpVli1blve6+Ph4vvnmG0sUhTrFYcVzcpRXX/2J++5bzKFD6URGhjJ2bHfGjDm1swdjClWCM//SlJaWRocOHdi5cyctW7akV69epbr+xYsX8+677+Y9r1GjRrGvueqqqwgNDQVg0KBBTJgwgZtuuol3332XQYMG5a13w4YNea9JTk4mJSWFypWPD9O/e/duatWqlfc8KSmJG264gV9//RURITMzM29er169iI2NBeCzzz5j7ty5TJw4EXCaMW/fvp3TTz+dUaNGsXr1akJDQ9m8eXOB8X/zzTfFvseSOHToEB9//DG///471atX56qrrmL27Nlcd911PPfcc8ybN48uXbrwzDPPMHr0aF555RUAateuzaZNm0o1loJUyETx+++HuO66OSxdugOA3r2bMnVqP848M7b04jOmjIiOjmb16tWkpqbSp08fpk6dyh133EGrVq34+uuvT1h269atVK5cmapVq9K6dWtWrlyZV6xTUp5FK/nb9VeqVCnv8bnnnsuWLVvYv38/H330EWPHjgUgJyeHZcuWERUVVeR781z3uHHjuPDCC5kzZw7btm2jZ8+eBW5TVfnwww9p3vzEpu7jx4+nTp06rFmzhpycnEK3XZIrinr16rFjxw7q169PVlYWSUlJxMXFnbDM4sWLadKkSV7Su+KKK1i6dCl9+vRhzZo1dOnSBXCSat++ffNel1vE5mvB2eopeTskbYXIalCr5PUTVatGsnlzIqedVpl3372SBQuutSRhyr2YmBgmT57Ms88+S1ZWFtdeey3ffvstixc7HUjT0tK44447uPfeewEYM2YMTzzxRN5ZdU5ODjNmzPjTenv16sXUqVPznucWPdWpU4eNGzeSk5PDnDlzCo1LRPjb3/7G6NGjadmyZd5BtHfv3rz44ot5y61evfpPr23ZsmVeKyBwrijq1asHwKxZswrdZp8+fXjxxRfzWnitWrUq7/V169YlJCSEt956i+zs7AJf/80337B69eo//eVPEgCXXXYZb7zxBuDU1Vx00UV/qp9o2LAhy5YtIzU1FVXl888/p2XLltSoUYOkpKS8z2DRokW0bNky73WbN28+oejNV4IzUeTdH7s7hIR69ZKFC7eQkZEFQFxcDHPnDmbTptsYNKiNdYgyFUbHjh1p164d77zzDtHR0Xz88cc89thjNG/enLZt23LOOecwatQoANq1a8fzzz/PkCFDaNmyJW3atGHr1q1/WufYsWM5dOgQbdq0oX379nz5pdNs/cknn2TAgAGcd9551K1bt8i4Bg0axOzZs/OKnQAmT57MihUraNeuHa1atSowSbVo0YKkpKS8s/t7772XBx54gI4dO5KVlVXo9saNG0dmZibt2rWjdevWjBs3DoCRI0fyxhtv0L59ezZt2nTCVcjJGjZsGImJiZx55plMmjSJJ598EoBdu3bRr18/ALp06cLAgQM5++yzadu2LTk5OQwfPpywsDBefvllrrzyStq3b89bb73FM888k7fu7777rtSLEgsiuRk1WMTHx+uKx9rC+lnQcxJ0urvI5XfsSOKOOxbw0UebePTRCxk7trt/AjUG2Lhx4wlngKb0Pffcc1SpUoV//OMfgQ7Fr1atWsWkSZN46623/jSvoO+diKxU1ZO6BWhwX1EUMRBgVlYOkyZ9T8uWU/noo01UrhxBbKwN/21MeXPrrbcSGRkZ6DD87sCBAzz66KN+2VbwVWZnH4PkbRBZHWq1K3CRZcsSGDHiE9as2QvAlVe25IUX+lKvXlU/BmqM8YeoqCiGDh0a6DD8zh9FTrmCL1Ecc1sa1O9RYP3EDz8kcN55r6IKjRtXZ8qUS+nf/yw/B2nMcQV1rjLGV3xRnRB8iSLTTRSFNIvt3LkeffqcSceOpzF2bHdiYvx7MyNjPEVFRZGYmGhDjRu/UPd+FEU1KT4ZwZcocq8o3IEAf/01kbvvXsikSX046yznx/jpp9cQEmI/ShN49evXJyEhgf379wc6FFNB5N7hrjQFX6LIPgZRNcio2pIn/7mEf/3rWzIysomKCuODD64GsCRhyozw8PBSvdOYMYHg01ZPItJXRH4RkS0icn8B8yNF5D13/g8i0tib9X6e2J927V9i/PivyMjI5qabOjBjxoDSDt8YYww+7EchIqHAZqAXkAAsB4ao6gaPZUYC7VR1hIgMBv6mqoMKXKErrlINPZh6FwAtW9ZkxowBNoifMcYUo6z2o+gMbFHVrap6DHgXuDzfMpcDb7iPPwAulmJq/A6lRhMVFcITT1zE6tUjLEkYY4yP+fKKYiDQV1X/4T4fCnRR1VEey6xzl0lwn//mLnMg37qGA8Pdp22AdT4JOvjUBA4Uu1TFYPviONsXx9m+OK65qlY5mRcGRWW2qr4EvAQgIitO9vKpvLF9cZzti+NsXxxn++I4EfnzjTW85Muip51AA4/n9d1pBS4jImFANSDRhzEZY4wpIV8miuVAMxFpIiIRwGBgbr5l5gI3uI8HAl9osI1SaIwx5ZzPip5UNUtERgELgVDgNVVdLyITcG7yPRd4FXhLRLYAB3GSSXFe8lXMQcj2xXG2L46zfXGc7YvjTnpfBN0w48YYY/wrOIcZN8YY4zeWKIwxxhSpzCYKXw3/EYy82BejRWSDiKwVkc9FpNz2QixuX3gsd6WIqIiU26aR3uwLEbna/W6sF5G3/R2jv3jxG2koIl+KyCr3d9IvEHH6moi8JiL73D5qBc0XEZns7qe1InK2VytW1TL3h1P5/RtwBhABrAFa5VtmJDDDfTwYeC/QcQdwX1wIxLiPb63I+8JdrgrwNbAMiA903AH8XjQDVgE13Oe1Ax13APfFS8Ct7uNWwLZAx+2jfdEdOBtYV8j8fsB8QICuDcYkIQAABmRJREFUwA/erLesXlH4ZPiPIFXsvlDVL1U11X26DKfPSnnkzfcC4FHgKSDdn8H5mTf74mZgqqoeAlDVfX6O0V+82RcK5N7ishqwy4/x+Y2qfo3TgrQwlwNvqmMZUF1E6ha33rKaKOoBOzyeJ7jTClxGVbOAJCDOL9H5lzf7wtMwnDOG8qjYfeFeSjdQ1U/9GVgAePO9OAs4S0S+E5FlItLXb9H5lzf7YjxwnYgkAPOA2/0TWplT0uMJECRDeBjviMh1QDzQI9CxBIKIhACTgBsDHEpZEYZT/NQT5yrzaxFpq6qHAxpVYAwBZqnqsyJyLk7/rTaqmhPowIJBWb2isOE/jvNmXyAilwAPAZepaoafYvO34vZFFZxBI5eIyDacMti55bRC25vvRQIwV1UzVfV3nGH/m/kpPn/yZl8MA/4DoKrfA1E4AwZWNF4dT/Irq4nChv84rth9ISIdgZk4SaK8lkNDMftCVZNUtaaqNlbVxjj1NZep6kkPhlaGefMb+QjnagIRqYlTFLXVn0H6iTf7YjtwMYCItMRJFBXx/rRzgevd1k9dgSRV3V3ci8pk0ZP6bviPoOPlvngGqAy879bnb1fVywIWtI94uS8qBC/3xUKgt4hsALKBMapa7q66vdwX9wAvi8jdOBXbN5bHE0sReQfn5KCmWx/zCBAOoKozcOpn+gFbgFTgJq/WWw73lTHGmFJUVouejDHGlBGWKIwxxhTJEoUxxpgiWaIwxhhTJEsUxhhjimSJwpRJIpItIqs9/hoXsWxKKWxvloj87m7rJ7f3bknX8YqI/H97ZxdiVRXF8d8fG3W84vhgRD0EQVkIyYTSS9gHxSQJkTgxRBEDQRGpRBZBSYGYfViB0VOKzEBSoWVQUWbhMENaY8yMM2MfvvTax0MPTU0gtHpY69BpPJ65MZIztX6wOfucu/bea98LZ5+99z3/tSLyT0z57OhMfYx6iu9lXNJ7kpZOY9/+X1VKTf498u+xyaxE0oSZLT7XtjV19ADvm9kBSR3Ai2a2cgb1zdin6eqV1AucMrNnauy7cQXdjefal+T/Q84okjmBpMURa2NI0pikM1RjJV0sqb/0xL0mrndIOhZl90ua7gbeD1weZR+JusYlPRzXGpI+kHQirnfF9T5JqyU9B7SGH/vis4k4vilpXcnnHkmdkuZJ2inpeMQJeKCJr+UYIegm6dro47Cko5KujLeUtwFd4UtX+L5X0mDYVqnvJsnfOd/66ZkyVSX8TeKRSAdxFYEl8dky/M3SYkY8EcctwJORn4drPy3Db/yNuP448FRFez1AZ+TvBL4AVgFjQAN/8/0kcA2wAdhdKtsWxz4i/kXhU8mm8HE90Bv5+biSZytwP7A1ri8AvgQuq/BzotS//cDaOF8CXBD5W4C3I98NvFoqvwO4J/JLcf2nxvn+vTPN7jQrJTySBJg0s/biRFILsEPS9cAf+JP0RcD3pTLHgb1h+66ZjUi6AQ9U81nIm8zHn8Sr2ClpK64BdB+uDXTQzH4NH94B1gAfAS9Jeh5frhr4B/36ENglaQGwFug3s8lY7lopqTPs2nABv++mlG+VNBL9/xo4XLLvlXQFLlHRcpb2O4DbJT0a5wuBS6OuJKkkB4pkrnA3cCGwysxOy9VhF5YNzKw/BpJ1QI+kl4GfgcNmdlcTbTxmZgeKE0k3VxmZ2Sl53IvbgO2SPjWzbc10wsx+l9QH3Ap04UF2wCOObTKzQ9NUMWlm7ZIW4dpGDwGv4MGajpjZ+tj47ztLeQEbzOzbZvxNEsg9imTu0Ab8GIPETcAZccHlscJ/MLPdwB48JOTnwHWSij2HhqTlTbY5ANwhaZGkBr5sNCDpEuA3M3sdF2Ssijt8OmY2VbyFi7EVsxPwm/6DRRlJy6PNSswjGm4Gtugvmf1CLrq7ZPoLvgRXcAjYpJheyZWHk6SWHCiSucI+YLWkMeBe4JsKmxuBE5KG8af1XWb2E37jfEPSKL7sdFUzDZrZEL53MYjvWewxs2HgamAwloCeBrZXFH8NGC02s6fwMR5c6hPz0J3gA9tXwJCkcVw2vnbGH76M4kF5XgCejb6Xyx0BVhSb2fjMoyV8OxnnSVJL/j02SZIkqSVnFEmSJEktOVAkSZIkteRAkSRJktSSA0WSJElSSw4USZIkSS05UCRJkiS15ECRJEmS1PInABfJVZO0WokAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Lzy8TupUqz7",
        "outputId": "b93f8bc8-c920-45e7-f1c7-2c9979bea3da"
      },
      "source": [
        "_, test_predictions = discriminator(tf.convert_to_tensor(x_test))\n",
        "test_predictions.shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([100, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQpcUnI3Ur-d",
        "outputId": "c6e6444e-d234-44aa-97b8-501a6d53ca40"
      },
      "source": [
        "binary_accuracy = tf.keras.metrics.BinaryAccuracy()\n",
        "binary_accuracy.update_state(tf.one_hot(tf.squeeze(y_test), depth=2), test_predictions)\n",
        "print('Test Accuracy: %.4f %s' % (binary_accuracy.result().numpy()*100, '%'))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 69.0000 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "bJVaUUMAUtLE",
        "outputId": "b95f9d10-6f78-42ec-ea45-3e30ed8d1e9d"
      },
      "source": [
        "fpr, tpr, _ = roc_curve(y_test, tf.argmax(test_predictions,1).numpy())\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.figure()\n",
        "lw = 2\n",
        "plt.plot(fpr, tpr, color='darkorange',\n",
        "lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZjIRnfYmZJG"
      },
      "source": [
        "We will now look at the predictions on the generated synthetic data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wx3BZCj0Uv3M",
        "outputId": "b271c250-eeba-4ed1-cf6e-1a56142c74e1"
      },
      "source": [
        "generator_outputs = generator(random_data)\n",
        "generator_outputs.shape"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([200, 5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTHmzXj_UxKe",
        "outputId": "37c72767-3967-4d5b-f57a-359d31e7e306"
      },
      "source": [
        "_, predictions_synthetic = discriminator(generator_outputs)\n",
        "predictions_synthetic.shape"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([200, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-Q30zS7UySO",
        "outputId": "de99e2fd-5f87-4ff0-908c-6f1a64e99f77"
      },
      "source": [
        "predicted_labels_synthetic = tf.argmax(predictions_synthetic, 1)\n",
        "predicted_labels_synthetic[:20]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(20,), dtype=int64, numpy=array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2gWuTtR5WLD"
      },
      "source": [
        "## Improving the Performance\n",
        "It can be seen from the loss vs iterations plots that the generator has more or less converged, but the discriminator hasn't. The AUC scores suggest that the model is actually learning. This can be improved in the fillowing ways:\n",
        "1. Use slightly higher learning rates while training the discriminator.  \n",
        "2. As the generator converged, we can take the synthetic data generated by it and add to our original training set. We can again start training the GAN so that the discriminator becomes more robust. \n",
        "3. Training for a larger number of epochs.\n",
        "4. Using adaptive learning rates and learning rate scheduling."
      ]
    }
  ]
}